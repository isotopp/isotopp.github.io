---
author: isotopp
title: "UTF8MB4"
date: 2022-01-10T16:28:00+01:00
feature-img: assets/img/background/mysql.jpg
tags:
- lang_en
- mysql
- mysqldev
---

On Twitter, Jan Wildeboer [linked](https://twitter.com/jwildeboer/status/1481308177727729668) an [article by Adam Hooper](https://adamhooper.medium.com/in-mysql-never-use-utf8-use-utf8mb4-11761243e434) on MySQL and the weird `utf8mb4` character set.

The recommendation is correct:
In MySQL, use `utf8mb4` when you mean to work with `utf8` in your programming language.
The background and reasoning for this, and why it is wrong and way more complicated.

So let's walk through this:

## MySQL `utf8` means Unicode 1.0 BMP

UTF-8 in MySQL encodes the [Unicode BMP](https://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane).
The Unicode Basic Multilingual Plane, or BMP, is the original 65536 character plane of Unicode 1.0.
It was thought to be enough for all scripts in the world.
It wasn't.
Unicode was extended, as early as with the 2.0 release, to have more characters than that, in more code planes.
There can be up to 17 Unicode planes now.

There are various ways to encode codepoints of Unicode.

Some of them are fixed with.
For example, Windows uses internally a 16 bit character encoding, also from the time when people thought that Unicode 1.0 would solve the worlds writing problems. 
This is not only limited, but also wasteful:
When writing western text, every other byte in Windows Unicode text is a null byte.

In Unix, specifically originally in Plan 9 from AT&T, a variable length encoding for Unicode was developed.
Basically, the number of consecutive high bits of the first byte determines how long the byte sequence is.

- So a thing starting with `0x8?` denotes a single character sequence, 
- a thing starting with `0xc?` identifies a two character sequence and 
- a `0xe?` a three byte sequence.
- Four byte codepoints start with `0xf?`

MySQL also is from the time when people still believed in 65536 glyphs, and offers a number of encodings.
You can list the interesting ones with `SHOW CHARSET WHERE MAXLEN > 1`.

```sql
kris@localhost [kris]> show charset where maxlen > 1;
+---------+---------------------------------+---------------------+--------+
| Charset | Description                     | Default collation   | Maxlen |
+---------+---------------------------------+---------------------+--------+
| big5    | Big5 Traditional Chinese        | big5_chinese_ci     |      2 |
| cp932   | SJIS for Windows Japanese       | cp932_japanese_ci   |      2 |
| eucjpms | UJIS for Windows Japanese       | eucjpms_japanese_ci |      3 |
| euckr   | EUC-KR Korean                   | euckr_korean_ci     |      2 |
| gb18030 | China National Standard GB18030 | gb18030_chinese_ci  |      4 |
| gb2312  | GB2312 Simplified Chinese       | gb2312_chinese_ci   |      2 |
| gbk     | GBK Simplified Chinese          | gbk_chinese_ci      |      2 |
| sjis    | Shift-JIS Japanese              | sjis_japanese_ci    |      2 |
| ucs2    | UCS-2 Unicode                   | ucs2_general_ci     |      2 |
| ujis    | EUC-JP Japanese                 | ujis_japanese_ci    |      3 |
| utf16   | UTF-16 Unicode                  | utf16_general_ci    |      4 |
| utf16le | UTF-16LE Unicode                | utf16le_general_ci  |      4 |
| utf32   | UTF-32 Unicode                  | utf32_general_ci    |      4 |
| utf8    | UTF-8 Unicode                   | utf8_general_ci     |      3 |
| utf8mb4 | UTF-8 Unicode                   | utf8mb4_0900_ai_ci  |      4 |
+---------+---------------------------------+---------------------+--------+
15 rows in set (0.00 sec)
```

Specifically, there are the `utf16`, `utf16le` and `utf32` encodings.
And we can check what they do. In my terminal, which incidentally uses `utf8` to talk to the database, I can `select hex("ö") as ch` to check the representation of an o-Umlaut as bytes.

I can also `select hex(convert("ö" using <somecharset>))) as ch` to check what these bytes would be in other encodings.

Let's do that:

```sql
kris@localhost [kris]> select hex("ö") as ch;
+------+
| ch   |
+------+
| C3B6 |
+------+
1 row in set (0.00 sec)

kris@localhost [kris]> select hex(convert("ö" using latin1)) as ch;
+------+
| ch   |
+------+
| F6   |
+------+
1 row in set (0.00 sec)

kris@localhost [kris]> select hex(convert("ö" using utf16)) as ch;
+------+
| ch   |
+------+
| 00F6 |
+------+
1 row in set (0.00 sec)

kris@localhost [kris]> select hex(convert("ö" using utf16le)) as ch;
+------+
| ch   |
+------+
| F600 |
+------+
1 row in set (0.00 sec)

kris@localhost [kris]> select hex(convert("ö" using utf32)) as ch;
+----------+
| ch       |
+----------+
| 000000F6 |
+----------+
1 row in set (0.00 sec)
```

So we learn: 

- The `UTF8` o-Umlaut is `0xc3b6`.
- In the fixed 8-bit charset `latin1` that becomes `0xf6`.
- In `UTF16`, a superset of that, it becomes `0x00f6`.
- Unsurprisingly, in `UTF16LE` that is now the same, backwards, `0xf600`.
- And in the staggering wasteful `UTF32` we actually get three null bytes per glyph when storing western script, `0x000000f6`.

In fact, to encode all possible 17 unicode planes we require only 21 bits, so `UTF32` is hopelessly wasteful even for non-western scripts.
That is at least one null byte for each glyph in any script.

## Later Unicode revisions add more planes

But: The singular Basic Multilingual Plane was not enough for all of earths scripts, so it needed to be extended.
So we get characters that no longer fit a Windows `wchar`, and also not the 3-byte `utf8` encoding.

MySQL extends the character representation to 4 bytes, which covers all possible 17 Unicode planes, and names that character set `utf8mb4`.
Case closed.

When working with modern Unicode in MySQL, always define the character set as `utf8mb4`.
It is that simple.

## But why is the 4-byte `utf8` named differently?

So why did MySQL name the 4-byte `utf8` weirdly `utf8mb4` and did not update the definition of `utf8` to 4 bytes?

That is, because in databases you cannot ever change the definition of a character set or character set sort order ("collation") without great pains.
You can easily add more, but you can never change what you have delivered to customers, ever.

### Indexes physically materialize columns in sort order for fast lookups

So, databases have this thing called indexes. 
Suppose you have a table with a character column (a column of any datatype that has a charset and a collation), it may be a `char`, `varchar` or any of the four `text` types.
And you define an index on it:

```sql
kris@localhost [kris]> create table kris ( id serial, c varchar(20), index(c), d integer);
Query OK, 0 rows affected (0.28 sec)

kris@localhost [kris]> show create table kris\G
       Table: kris
       Table: kris
Create Table: CREATE TABLE `kris` (
  `id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `c` varchar(20) DEFAULT NULL,
  `d` int DEFAULT NULL,
  UNIQUE KEY `id` (`id`),
  KEY `c` (`c`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci
1 row in set (0.00 sec)

```

What happens here?
We store rows in the table `kris` by inserting data.
The database assigns a primary key `id`, and stores the tuple `(id, c, d)` in the table.
It also needs to maintain the index on `c`, so it will take the `c`-value of each row and store it in the index **in sorted order** with a pointer to the full row.

In InnoDB, the row pointer used by secondary indexes is actually the primary, `id`, so we do get the index `c` as tuples of `(c, id)`.
That means each secondary has the primary key as a tail, so the primary key better be short.
In our case, it is a `BIGINT`, so 8 bytes are added as a row pointer to the index, for each row.

Notice the fat **in sorted order**?

The index `c` is sorted, in the order defined by the collation for the character set.
That is going to be critically important.
So let's have a look at character ordering.

### Collations are comparison rules for characters

Our collation is given as `utf8mb4_0900_ai_ci`, and that are the Unicode Collation Algorithm (UCA) 9.0 sorting rules for 4-byte UTF8, with accent insensitivity (`ai`) and case insensitivity (`ci`).

We need to learn even more:

- Collations are rules for handling comparisons between glyphs in a specific encoding.
There are encodings for `latin1`, other ones specific to `utf8` and again different ones for `utf8mb4`.
- Collations define *sort orders* and *equality rules*.
`ci` means the *sort order* is case insensitive, so "Köhntopp", "KÖHNTOPP" and "köhntopp" sort the same. With `cs` they would not.
`ai` means accent insentitive, so `WHERE c = "Kohntopp"` matches "Köhntopp". With `as` it would not.

We can show the collations for a given character set.
There are over 250 of them, and 75 alone for `utf8mb4`.
Let's check just the `utf8mb4%_ai_ci` ones.

```sql
kris@localhost [kris]> show collation where collation like "utf8mb4%_ai_ci";
+----------------------------+---------+-----+---------+----------+---------+---------------+
| Collation                  | Charset | Id  | Default | Compiled | Sortlen | Pad_attribute |
+----------------------------+---------+-----+---------+----------+---------+---------------+
| utf8mb4_0900_ai_ci         | utf8mb4 | 255 | Yes     | Yes      |       0 | NO PAD        |
| utf8mb4_cs_0900_ai_ci      | utf8mb4 | 266 |         | Yes      |       0 | NO PAD        |
...
| utf8mb4_vi_0900_ai_ci      | utf8mb4 | 277 |         | Yes      |       0 | NO PAD        |
+----------------------------+---------+-----+---------+----------+---------+---------------+
22 rows in set (0.00 sec)
```

So many nation specific sort orders.
And if you define an index on a character column, it has a collation attached and the data will be physically sorted in that nations sort order.

These things are in fact even versioned:
If there is an UCA 9.0, are there other, earlier ones?
Sure enough:

```sql
kris@localhost [information_schema]> select * from collations where collation_name like "%general_mysql%";
+--------------------------+--------------------+-----+------------+-------------+---------+---------------+
| COLLATION_NAME           | CHARACTER_SET_NAME | ID  | IS_DEFAULT | IS_COMPILED | SORTLEN | PAD_ATTRIBUTE |
+--------------------------+--------------------+-----+------------+-------------+---------+---------------+
| ucs2_general_mysql500_ci | ucs2               | 159 |            | Yes         |       1 | PAD SPACE     |
| utf8_general_mysql500_ci | utf8               | 223 |            | Yes         |       1 | PAD SPACE     |
+--------------------------+--------------------+-----+------------+-------------+---------+---------------+
2 rows in set (0.00 sec)
```

The fine manual notes, at the bottom of [Unicode Character Sets](https://dev.mysql.com/doc/refman/8.0/en/charset-unicode-sets.html#charset-unicode-sets-uca):

> #### Miscellaneous Information
> 
> The xxx_general_mysql500_ci collations preserve the pre-5.1.24 ordering of the original xxx_general_ci collations and permit upgrades for tables created before MySQL 5.1.24 (Bug #27877).

MySQL 5.1.24 was released on 2008-04-08, 14 years ago.
We have *buggy* sort orders used back then maintained in current MySQL for the sake of upgradability.

### TL;DR, so far

- Databases use indexes to access data quickly.
- Indexes physically materialize a column (and row pointers) in sort order.
- Sort order for character sets are character set dependent, and national language dependent.
- Sort order also got major upgrades during the evolution of Unicode: Not only did we move from "only BMP" to 17 planes, we also went through at least 9 revisions of the Unicode Collation Algorithm.

So:

- If you change the collation (the sort order) of an index, the index needs to be rebuilt.
- Sort orders are a property of character sets.
- UTF-8 sorted according to UCA 4.x and UCA 5.0, UTF8MB4 sorts according to UCA 9.0. Specifically, the concept of segregating sort order comparison from equality comparison, and introducing accent insensitivity is new in UCA 9.0 and not present in previous UCA versions.

That means, while `utf8` is a strict subset of `utf8mb4` in terms of character repertoire and encoding (modulo bug fixes), it sorts differently.
An `ALTER TABLE kris MODIFY COLUMN c varchar(20) charset utf8` in our example table implies a drop on the index `c`, and it's recreation with a different sort order, from scratch.
Not fast on a large table.

## So what would happen if `utf8` suddently meant `utf8mb4`?

Let's imagine a MySQL 9.0 comes out, and in that release `utf8` suddenly means `utf8mb4`.
If you greenfield a new installation, no problem.

If you apply the update to an existing installation, all your character columns that are part of an index need to be checked, and if required, rebuilt.
At the time when you apply the database update.
Which can take a very long time, during which the database is not available.
That can be quite inconvenient.

## What happens instead if we don't do that?

In that case, `utf8` means `utf8`, nothing changes, and `utf8mb4` is added and not used at all.
Unless you do.

So you could create new columns using the new character set and collations, even in existing tables.
That is possible, because unlike in the other database, in MySQL a charset and collation is a column attribute.
Tables, databases and servers just provide defaults that are inherited at the time of subordinate object creation.

So if you wanted to upgrade your database, you'd install a new version.
Then you would check all columns and note which one need changing.
And then you would change them, table by table, at your own leisure.
Possibly using tooling such as Percona Online Table Change ("OSC"), making the change in the background without service interruption.

That is what MySQL does.
It never changes collations any more (it did, in the dark, evil past, creating a lot of suffering).
It is not even correcting bugs in existing collations any more (except by creating newly renamed collations).
Instead you run your collation migrations as it fits into your operational schedule.

For Unicode that means, 4-byte Unicode is `utf8mb4` in MySQL, because the name `utf8` was already taken by a previous version of the Unicode character set.

## Not using libc.

MySQL does not use `libc` and the character set comparison functions in there for itself.
Instead it brings its own collations.

That is also an important lesson.
It means that, unlike the other database, MySQL does not break when you upgrade the operating system `libc` on your machine.
If MySQL used `libc` fort sorting and comparing, the database could break without any changes to the database whatsoever - an innocent security update to the operation systems `libc` that for some reason also touched `libc` sorting and comparison would break all character indexes in your database.

MySQL does not do that.
It lets you migrate these things in a way that is operationally convenient.

It also means that MySQL versions sort the same between Linux, FreeBSD, Solaris and even Windows.
Just like the on disk format is compatible across all platforms, so is character handling and time zone handling.
MySQL is MySQL on any platform, and copying files and replication always work across these boundaries.

## Dude, that's complicated

Yes.
Databases are weird, complicated and large.

That is because they keep the state for your program, so that your program can be stateless and easy to manage.
Guess where all that complexity and technical debt management went?

Where I work there is a replication hierarchy where the database instances are 120 TB in size, with the largest table being 35 TB in size.
Cloning a new instance at 400 MB/s takes around a week, plus replication catchup, so you wait around two weeks for a new instance, if nothing goes wrong.

Moving this to `utf8mb4` is a 6 month project, at least, and most of it is waiting.

This database was created around 16 years ago.
It has an unbroken chain of state changes ("inserts, updates and deletes") from back then to the present.

It was born as MySQL 4.x, at some point became a MariaDB and is now an Oracle MySQL 5.7.
None of the code that was running when it was created still exists.
But the bugs and faulty updates that code did, back then, still exist somewhere in the data, unless they have been found and corrected.

Persistence systems - databases and their NoSQL brethren that actually manage to write data to disk properly - are stateful systems.
The data they store usually outlives the code that created the records.
Changes to this data are forever. 
Mistakes in handling the data or the updates are forever, too.

DBA are a very special folk.
They are very paranoid.
They have tested restores of their backups, and their backups have backups.
That is, because if they make a mistake and data is gone, it is gone forever, and no rollout to fix things is possible.

## Sorting stuff

Not only can databases, tables and individual indexes be very large, they also need to be kept sorted, and sometimes data needs to be re-sorted.
Databases ususally have a large number of strategies for that.

Small things are sorted in memory.
That is, the database creates the full rows of the result table in memory, using all columns. 
It then sorts these rows in memory by shuffling around the full rows.
That uses up a lot of memory bandwidth, because we copy around full rows, which can be large.

Sometimes we do not want to move around full rows.
So we only sort the columns specified in the `ORDER BY CLAUSE`, which is a tiny subset of the full columns set of each row of the result table, and attach a row pointer to each of these sort rows.

In the end we have a tiny in-memory table in sort order, which only holds columns from the `ORDER BY`, and each row in there points to the unsorted full rows somewhere else - in memory or on disk.
We then output the full rows in sort order, but that means we generate a lot of seeks - possibly on disk.
That is, because the full rows have not been sorted, so the row pointers from the sorted data to the full rows are a mess.

Sometimes we have very many rows.
So we do as before, but for a subset of all rows - as many as fit into our sort buffer.
When the sort buffer is full, we write it out to disk.
Then we continue with the next batch.
Across all partial results we perform a merge sort.

If the things sorted were full rows, we can emit full result rows rather quickly.
If the things sorted were only `ORDER BY` columns or other partial result set rows with row pointers attached, we may have to perform one or multiple seeks in order to get the missing columns from all tables involved.

The optimizer has to decide which strategy to use here, based on estimates of the row width, size of the result set (which has not yet been produced, so it's a guess) and the available amount of resources.

### And then, implementation issues

Older versions of MySQL have different storage engines: `MYISAM`, `INNODB`, `MEMORY` and others.
Modern MySQL has only `INNODB`, but for temporary tables has a few tricks ready to make things faster (for example, temporary tables can be recreated and do not need to be crash safe, so no redo and undo logging is required).

Other storage engines than `INNODB` had restrictions.

For example, `MEMORY` tables could not have variable length columns, there was no `VARCHAR`, only `CHAR`.
So when sorting with temporary tables in memory, MySQL created these tables as `MEMORY` tables, promoting every `varchar(255)` to `CHAR(255)`, and so a row with a varchar value of `hello` suddenly used 255 bytes instead of 6 bytes to represent the string `hello` (plus a lot of padding).

Of course, with `utf8`, a `varchar(255) charset utf8` is promoted to a `char(255) charset utf8`, which instead allocates 765 bytes of memory, out of which the first five bytes contain the string `hello`, and the rest is wasteful padding.

`MEMORY` tables had a configurable size limit, and if your table exceeded that (which happened rather quickly, given the above restrictions), it was converted to an on-disk temporary table in `MYISAM` format.
That, of course had `varchar(255) charset utf8` as a native type and your `hello` would again shrink to six bytes.
Looking at the on disk temporary file, which was rather small, you might wander what caused the spill.

MySQL 8 introduces unlogged `INNODB` temporary tables (and a failed experiment involving mmap that you may want to disable).
So this problem is gone, once and for all: `varchar` is `varchar` at all times, and representing Unicode variants in temporary tables is no longer a problem.

This is in fact one of the major reasons to graduate from 5.7 for many people.

### Sorting TLDR and Unicode

Sorting Unicode columns was expensive in versions before MySQL 8.
That is because the `MEMORY` engine used did not understand variable length data types at all.
That led to all kinds of recommendations restricting the use of Unicode columns:
Don't use them, if you do not need them.
Keep them short.
Exclude `BLOB` and `TEXT` columns from sorting.
And so on.

MySQL 8 fixes all that.
Use Unicode columns as you see fit.
They work well, and as expected.
`BLOB` and `TEXT` are no longer a problem with sorting (well, they are, but for other reasons, but they no longer trigger an immediate spill, no matter how tiny they are).

## TLDR

We learned:

- Your programming languages `utf8` is called `utf8mb4` in MySQL.
- `utf8` and `utf8mb4` sort differently due to changes in the Unicode Collation Algorithm (UCA).
- Indexes are physically materialized sort orders of column sets. They can become pretty large. 
- MySQL never changes sort orders of character sets (collations), even if they are buggy. Instead a new collation with a new name is created.
- That is, because changing a sort order may require dropping and recreating an index, which is expensive if the index is large.
- Sorting things can be done in many different ways, depending on data set size, column size and other considerations.
- Implementation details can make sorting even more complicated.
- MySQL 8 is a worthwhile upgrade.
- Databases manage state so you don't have to. If you think databases are complicated, consider what you would have to do if the database and your DBA would not be there for you.
