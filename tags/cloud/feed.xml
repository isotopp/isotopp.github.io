<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cloud on Die wunderbare Welt von Isotopp</title>
    <link>https://blog.koehntopp.info/tags/cloud.html</link>
    <description>Recent content in Cloud on Die wunderbare Welt von Isotopp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>kris-blog@koehntopp.de (Kristian Köhntopp)</managingEditor>

    
    <webMaster>kris-blog@koehntopp.de (Kristian Köhntopp)</webMaster>

    
    <lastBuildDate>Fri, 28 Feb 2025 13:40:30 +0000</lastBuildDate><atom:link href="https://blog.koehntopp.info/tags/cloud/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cloud Cost vs. On-Premises Cost</title>
      <link>https://blog.koehntopp.info/2024/09/30/cloud-cost-vs-on-premises-cost.html</link>
      <pubDate>Mon, 30 Sep 2024 04:05:06 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2024/09/30/cloud-cost-vs-on-premises-cost.html</guid>
      <description>&lt;p&gt;Ich wurde gefragt:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hast Du eine Idee, wie ich seriös die Kosten für Cloud, reguläres und selbst Hosting vergleichen kann?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Seriös nicht, weil das ein Problem mit mehreren Schichten ist und keine dieser Schichten irgendwie schön ist.&lt;/p&gt;
&lt;h1 id=&#34;rahmenbedingungen&#34;&gt;
    &lt;a href=&#34;#rahmenbedingungen&#34;&gt;
	Rahmenbedingungen
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Der Haupttreiber bei der ganzen Sache ist die simple Tatsache, dass wir immer mehr Transistoren auf einen Chip bekommen.
Der Chip wird zwar nicht mehr schneller, weil bei circa 3 GHz Takt Dauerleistung Schluss ist,
aber weil die Strukturen immer noch kleiner werden bekommen wir immer mehr Cores und Cache.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/09/01-cloud-on-prem.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fortschritte bei der Chipentwicklung über Zeit.
Die y-Ache ist logarithmisch. Wir sehen, wie die Anzahl der Transistoren pro Chip ungebrochen hoch geht.
Circa 2005 haben wir jedoch ein Taktlimit erreicht – bei 3 GHz Dauertakt ist bei Silizium Schluss.
Die Performance pro Core geht noch durch Verlagerung gewisser Funktionen in Hardware
und effizientere Prozessor-Architekturen langsam weiter hoch.
Stattdessen steigt seit 2005 die Anzahl der Cores pro Chip an – die
Gewinne an Transistoren gehen also an mehr Cores, nicht in schnellere Cores.
Zugleich sinkt der Energieverbrauch pro Core, während der Energieverbrauch pro Chip bei 100-200W stagniert.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Und oft auch Features, die die meiste Zeit gar nicht gebraucht werden,
sodass Teile des Chips die meiste Zeit ausgeschaltet sind, um Energie zu sparen.
Denn Energieverbrauch erzeugt Abwärme, und die Abwärme setzt dem Chip ein (weiches) Leistungslimit.
Wenn wir also Dinge, die viel Energie verbrauchen (die FPU und die Vektoreinheiten) nicht verwenden,
dann können wir den Takt bei dem Teil des Chips, der benutzt wird hochdrehen,
und bekommen so ein wenig mehr Leistung für viel mehr Energieverbrauch und Abwärme.&lt;/p&gt;
&lt;p&gt;Prozessoren von 2024 haben auf einem Chip bis zu 128 oder gar 192 Cores und die doppelte Anzahl von vCores (Threads).&lt;/p&gt;
&lt;h1 id=&#34;hardwarekosten&#34;&gt;
    &lt;a href=&#34;#hardwarekosten&#34;&gt;
	Hardwarekosten
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Gehen wir 5 Jahre zurück, kommen wir zu konkreten Kosten.&lt;/p&gt;
&lt;p&gt;Die kleinsten sinnvoll kaufbaren Intel Xeon CPUs von 2019 wären die
&lt;a href=&#34;https://www.intel.com/content/www/us/en/products/sku/123547/intel-xeon-silver-4110-processor-11m-cache-2-10-ghz/specifications.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Xeon Silver 4110&lt;/a&gt;


gewesen.
Auf einer typischen Blade von Dell oder HP gibt uns das 2 Sockets, 16 Cores mit 32 Hyperthreads,
128 GB RAM (8 GB RAM pro Core), eine lokale Micron 1.92 TB NVME mit 800k IOPS,
und einen dedizierten 10 GBit/s Port direkt zum Top-of-Rack Switch (luxuriöse 625 MBit/s pro Core).&lt;/p&gt;
&lt;p&gt;Was ist das für eine Rechnung?&lt;/p&gt;
&lt;p&gt;Ein System muss, damit es funktioniert, balanciert sein.
Manche Workloads sind selbst unbalanciert und brauchen exzessiv CPU oder exzessiv Speicher oder Netz.
Aber die meisten Enterprise-Anwendungen brauchen pro Core mit 3 GHz 4-8 GB RAM und 100-400 MBit/s Netz.
Wenn wir unsere Hardware so proportionieren, dann können wir recht sicher sein, dass sie vielseitig einsetzbar ist.&lt;/p&gt;
&lt;p&gt;Preislich kommen wir (die Umgebung, in der ich gearbeitet habe – Eure Preise können variieren)
mit solchen schwachbrüstigen Klein-Blades auf Kosten von etwa 120-150 Euro im Monat,
für eine geplante Lebensdauer von fünf Jahren. Das ist der Kaufpreis für die Blade,
Platz im Rechenzentrum und anteilig Strom, Netzwerk und Blade-Chassis.&lt;/p&gt;
&lt;p&gt;Die Blade ist dann noch nicht inventarisiert, noch nicht an,
nichts installiert ein Betriebssystem oder die Anwendung – das ist Arbeit, und die rechnen wir hier nicht mit rein.&lt;/p&gt;
&lt;h1 id=&#34;aws-kosten-im-vergleich&#34;&gt;
    &lt;a href=&#34;#aws-kosten-im-vergleich&#34;&gt;
	AWS Kosten im Vergleich
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Wir können das mit einer AWS &lt;code&gt;m5.8xl&lt;/code&gt; vergleichen: 32 vCPU, 128 GB, 10 GBit/s und kein lokaler Speicher (EBS kostet extra).
Die kostet 1232 USD/Monat, also schmale 10x mehr, plus Netzwerk, plus EBS.&lt;/p&gt;
&lt;p&gt;Alternativ schauen wir auf eine AWS &lt;code&gt;i3.4xl&lt;/code&gt;: 16 vCPU, 122 GB Speicher, und zwei lokale 1.92 TB NVME,
sowie ein 10 GBit/s Link.
Die kostet 990 USD/Monat, plus Netz, aber wir haben zwei lokale Volumes.&lt;/p&gt;
&lt;p&gt;Anders als unsere Blade im Rechenzentrum ist diese VM bereits Teil einer gemanagten Umgebung,
wir können sie also provisionieren, und haben sie im Inventory.&lt;/p&gt;
&lt;p&gt;Anders als unsere Blade im Rechenzentrum ist diese VM auch eine VM, also ein Teil einer anderen, größeren CPU.
Die Silver 4110 ist eine weiche CPU: &lt;a href=&#34;https://en.wikichip.org/wiki/intel/xeon_silver/4110#Frequencies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unter Last geht ihr Takt runter&lt;/a&gt;

.
Größere CPUs sind je nach Auswahl auch oft härter,
sie kommen (zumindest bei &lt;code&gt;Normal&lt;/code&gt; Workloads) oft mit viel weniger Taktverlust.
Einige Hyperscaler, speziell AWS, lassen bei Intel besondere Typen nach Maß fertigen,
die nicht im normalen Portfolio für gewöhnliche Endkunden enthalten sind.
Sie haben einen festen Clock, also weder boosten sie, noch gehen sie unter Last runter,
und sie brauchen eine speziell gestaltete und genau spezifizierte Kühlung um das leisten.
Wenn man als einziger Kunde etwa die Hälfte der gesamten Jahresproduktion Xeons abnimmt bekommt man das.&lt;/p&gt;
&lt;h1 id=&#34;cloud-vs-baremetal-on-premises&#34;&gt;
    &lt;a href=&#34;#cloud-vs-baremetal-on-premises&#34;&gt;
	Cloud vs. Baremetal On-Premises
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Ein früherer Arbeitgeber von mir hatte circa 50.000 von solchen Blades.
Die meisten davon waren die kleinen 4110, und einige größere Einheiten mit &lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/products/123541/intel-xeon-gold-6132-processor-19-25m-cache-2-60-ghz.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dual Gold 6132&lt;/a&gt;

 an drei Standorten.
In Summe mag das auf etwa 1.000.000 Cores kommen, und dazu passend ausbalanciert Speicher.&lt;/p&gt;
&lt;p&gt;Bei diesem Arbeitgeber hat man um 2010 herum angefangen, Software zu schreiben, um diese Hardware zu verwalten,
automatisch zu provisionieren und Betriebssystem-Images und Anwendungen automatisch zu verteilen.
Ich kann mir also ein Script schreiben, das 200 Blades vom Typ x in RZ y anfordert.
Ich bekomme diese dann binnen 20 Minuten automatisch provisioniert, meiner Team-Abrechnung belastet,
und mit Partitionierung, Betriebssystem und Puppet-Klassen laufend zugewiesen.
Die ssh-Keys meines Teams erlauben mir einen Zugriff.&lt;/p&gt;
&lt;p&gt;In 2019 waren wir soweit, dass kein Rechner eine Uptime von mehr als 90 Tagen hatte.
Das heißt, alle Teams hatten ihre Software so verpackt,
dass sie vollkommen ohne manuelle Eingriffe installiert werden konnte,
keine Single Points of Failures existierten und alle produktiven Maschinen reihum reinstalliert werden konnten.
Dadurch haben wir Betriebssystem-Images und Anwendungen laufend neu deployed und aktuell gehalten.&lt;/p&gt;
&lt;p&gt;Automatisierung hat natürlich auch vorher schon funktioniert,
aber für die Vollautomatisierung war Arbeit und Druck notwendig.
Ich habe in
&lt;a href=&#34;https://blog.koehntopp.info/2023/02/18/this-is-not-a-drill.html&#34;&gt;This is not a Drill, this is just Tuesday&lt;/a&gt;


ausführlicher dazu geschreiben.&lt;/p&gt;
&lt;p&gt;Jedenfalls war viel Arbeit und es waren viele Leute notwendig,
denn ein Produkt, das so etwas für die lokale Firma passend auf Bare Metal Hardware macht, kann man nicht kaufen,
und schon gar nicht herstellerunabhängig.
Man muss also auf die Kosten für Hardware noch die Kosten für die Entwicklung dieser Stücke Software drauf rechnen.&lt;/p&gt;
&lt;p&gt;Am Ende hat sich dieser Arbeitgeber dafür entscheiden, seine Installation in die Cloud zu verschieben,
weil Operations dann jemand anderes Problem sind.
Das geht natürlich nicht von heute auf morgen,
Und soweit ich weiß, betreibt man auch jetzt, 5 Jahre später,
noch on-premises und Cloud parallel, und bezahlt beides.&lt;/p&gt;
&lt;p&gt;Würde das Projekt abgeschlossen sein, wäre man theoretisch nicht nur das Core.Infrastructure Team los,
sondern auch das Staffing für dieses Team.
Denn Menschen zu finden,
die mit Hardware umgehen können und einen sinnvollen Operations Mindset haben wird zunehmend schwieriger.
Entsprechend war das für das Management ein totaler No-Brainer, selbst bei einem Kostenfaktor von 10x.&lt;/p&gt;
&lt;p&gt;Der Aufwand, solche Personen zu finden, zu halten und so zu steuern,
dass sie dann sinnvolle Automation bauen, ist groß.
Das sind nicht nur Geldkosten, sondern schlimmer, das ist auch kognitive Load auf der Organisation.&lt;/p&gt;
&lt;h1 id=&#34;es-wird-nicht-einfacher&#34;&gt;
    &lt;a href=&#34;#es-wird-nicht-einfacher&#34;&gt;
	Es wird nicht einfacher
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Allgemeiner gesagt:
Damit Du in 2024 eine Firma haben kannst, die eigene Hardware am Start hat,
muss Deine Firma erst einmal eine Mindestgröße haben.&lt;/p&gt;
&lt;p&gt;Idealerweise kauft man aus Kostengründen immer die zweitgrößte Sorte Rechner, die zu haben ist.
Man kann das genau ausrechnen und dann ist es manchmal die zweitgrößte und manchmal die drittgrößte Sorte CPU,
die hergestellt wird, bei der der Bumms pro Dollar am besten ist.&lt;/p&gt;
&lt;p&gt;Das heißt, man hat dann etwas mit – sagen wir – 128 Cores pro Socket und Single Socket.
Bei 8 GB pro Core und 100-300 MBit/s pro Core landen wir bei
128 * 8 GB = 1024 GB = 1 TB RAM und bei 128 * 200 Mbit/s = 25600 MBit/s = 25 GBit/s.
Bei einer Dual Socket entsprechend das Doppelte,
also 2 TB RAM und 2x 25 GBit/s oder schlicht ein 100 GBit/s Interface (überprovisioniert, aber eventuell einfacher).&lt;/p&gt;
&lt;p&gt;Die typische Java-Anwendung kann man in der Regel so bis 8 Cores und 16 GB RAM skalieren,
danach haucht die JVM ihr Leben aus.
Wir haben also 16-32 Instanzen (Anwendungen) pro Socket.
Die meisten werden nicht am oberen Ende der Skala sein, also rechnen wir eher 50-100 Anwendungen pro Socket.&lt;/p&gt;
&lt;p&gt;Für viele Firmen ist also eine einzige Maschine schon zu groß.
Wir wollen aber aus Gründen der Verfügbarkeit nicht eine Maschine haben, sondern mindestens drei.&lt;/p&gt;
&lt;p&gt;Wenn eine solche Maschine aus welchen Gründen umfällt oder neu starten muss,
dann sind mal eben 100 Anwendungen offline – wir haben also ein Blast Radius Problem.&lt;/p&gt;
&lt;p&gt;Wir können dagegen in der Public Cloud VMs in der passenden Größe kaufen,
und weil der Cloud-Anbieter viele solcher Maschinen hat, wird jede unserer VMs &amp;ldquo;anti-affin&amp;rdquo; scheduled.
Das heißt, der Cloud-Anbieter versucht unsere Anwendungen über so viele Stücke Hardware als möglich zu verteilen,
und uns mit so vielen anderen Kunden als möglich auf einer Kiste zu hosten.
Wenn dann ein Stück Hardware umfällt, verliert jeder Kunde eine Instanz, aber nicht ein Kunde alle.&lt;/p&gt;
&lt;h1 id=&#34;on-premises-ist-auch-nicht-unabhängig&#34;&gt;
    &lt;a href=&#34;#on-premises-ist-auch-nicht-unabh%c3%a4ngig&#34;&gt;
	On-Premises ist auch nicht unabhängig
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Zurück On-Premises: Wir müssen jetzt aus unserer Hardware Stücke in passender Größe schneiden,
um sie in Pods oder virtuelle Maschinen zu verwandeln.
Für virtuelle Maschinen haben wir die Wahl zwischen VMware und Openstack.
Es gibt noch andere Lösungen, aber für die finden wir in der Regel keine Menschen mit Kenntnissen zur Anstellung.
Wir haben also die Wahl zwischen lebenslanger Schuldknechtschaft zu Broadcom,
oder wir müssen 3 oder mehr Spezialisten einstellen, die dem Openstack hinterher wischen,
wenn es mal wieder die Windel voll gemacht hat.&lt;/p&gt;
&lt;p&gt;Nun können wir VMs provisionieren, aber wir müssen aus den VMs noch immer Dienste machen.
Dort wiederholt sich das Spiel von der VM-Ebene.
Am Ende haben wir eine unserer Firmengröße angemesene &amp;ldquo;Core Infra&amp;rdquo; Abteilung, die unsere On-Prem Cloud betreibt.
Das sind dann vielleicht keine 50.000 Maschinen mit zusammen 1.000.000 Cores mehr,
sondern nur noch 8000 Maschinen mit 1.000.000 Cores, aber es sind trotzdem 10-30 Leute, die dafür da sind,
darauf Services zu implementieren.&lt;/p&gt;
&lt;p&gt;Wir haben also die Wahl, erschreckend kostengünstige Hardware, die viel zu groß für alles ist, zu kaufen.
Dann müssen wir Leute finden und anzustellen.
Diese sind dann vielleicht in der Lage (oder vielleicht auch nicht), daraus eine brauchbare Plattform zu bauen.&lt;/p&gt;
&lt;p&gt;Alternativ können wir dem Bezos Kohle in den Rachen schaufeln.&lt;/p&gt;
&lt;p&gt;Die Antwort ist für wirklich jede Firma mit weniger als einer dreistelligen Anzahl von Rechnern absolut offensichtlich,
und besteht nicht darin, Hardware zu kaufen.
Das Staffing, die lokale Betriebs-Entwicklung und alles, was damit einhergeht, will sich niemand ans Bein binden.
Sobald man dann größer ist, ist es teuer, aber zu spät.&lt;/p&gt;
&lt;p&gt;Folgerichtig findet man auch immer weniger Leute, die sich mit solchem lokalen Betrieb auskennen,
und Dir so etwas bauen können.
Dagegen kannst Du Dich mit Leuten tot werfen, die Dir Instanzen bei AWS klicken und für 60k-75k im Jahr zu Dir kommen.&lt;/p&gt;
&lt;h1 id=&#34;die-ganze-logik-kaskade&#34;&gt;
    &lt;a href=&#34;#die-ganze-logik-kaskade&#34;&gt;
	Die ganze Logik-Kaskade
    &lt;/a&gt;
&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Wir haben zu viele Transistoren pro Chip.
Die Hersteller von Chips wissen nicht, was die damit tun sollen.
Du bekommst absurde Features, die nicht auf allen Chips gebraucht werden (KI-Inferenz,
dutzende FPUs und Vektoreinheiten) in den Performance-Cores, oder riesige Mengen Efficiency Cores pro Socket.
Im Consumer Bereich bekommst Du statt eines Computers ein System-on-a-Chip, bei dem Grafikbeschleuniger,
RAM, RAM-Controller und NVME-Controller mit auf der CPU sitzen, zusammen mit 4E+8P Cores oder ähnlich.&lt;/li&gt;
&lt;li&gt;Wir haben daher im Enterprise-Bereich viel zu viele Cores pro Socket (128+ Cores).&lt;/li&gt;
&lt;li&gt;Als Kunde habe ich dadurch das Problem, dass ich kein Bare Metal mehr fahren kann.
Ich brauche also K8s oder einen Hypervisor zum Kleinschneiden der Systeme in brauchbare Stücke.&lt;/li&gt;
&lt;li&gt;Als Kunde habe ich dann auch ein Blast-Radius Problem.
Wenn so ein großer Host umfällt, dann nimmt er die ganze Firma offline.&lt;/li&gt;
&lt;li&gt;Als Kunde will ich also eine VM auf vielen Hosts haben und auf einem Host mit vielen anderen Kunden cohosted werden.
Ich gehe also lieber zu einer Public Cloud und miete mir da die passenden VMs.&lt;/li&gt;
&lt;li&gt;Ich brauche dann auch keine Hardware-Kundigen mehr.&lt;/li&gt;
&lt;li&gt;Als Hoster von so etwas kann ich die Hardware veredeln und Operations übernehmen,
indem ich statt VMs Dienste &amp;ldquo;as-a-Service&amp;rdquo; verkaufe.&lt;/li&gt;
&lt;li&gt;Als Kunde kann ich so nicht nur meine Hardware-Kundigen los werden,
sondern für den Dienst x (jetzt xaaS) auch meine x-Kundigen.&lt;/li&gt;
&lt;li&gt;Als Kunde von so einem Anbieter habe ich nun reine Entwickler,
die in komplett abstrakten Layers Dinge zusammenstecken,
low-code und nahe an Business Problemen.&lt;/li&gt;
&lt;li&gt;Dadurch habe ich niemanden nirgendwo mehr, der auch nur einen Hauch einer Ahnung hat, was das alles bedeutet,
lastmäßig, und ob der getriebene Aufwand dem Problem angemessen ist.&lt;/li&gt;
&lt;li&gt;Ich kann solche Einschätzungen aber als Beratung einkaufen,
entweder von lokalen Drittanbieter-Consultants oder vom Hoster, als &amp;ldquo;Well-Architected&amp;rdquo; Beratung nach Schema F.&lt;/li&gt;
&lt;li&gt;Das Bruttosozialprodukt steigt, weil das ja jetzt alles Transaktionen mit Echtgeld zwischen Firmen sind,
statt Verrechnung mit Spielgeld innerhalb einer Firma.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Und am Ende spielen die Kosten im Vergleich keine Rolle mehr, weil andere, größere Vektoren auf die Firmen einwirken.&lt;/p&gt;
&lt;p&gt;Einer dieser Vektoren sind auch immer größere Regulierungsverpflichtungen,
die den Betrieb eigener Rechner On-Premises unmöglich machen,
weil eine ISO 27k, eine NIS-2 und eine PCI-Zertifizierung ja auch Aufwand und Geld kosten.
Kauft man dagegen eine vorab zertifizierte Cloudlösung hat man mindestens drei Viertel des Schmerzes nicht,
wird einem versprochen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AMD und 128 cores</title>
      <link>https://blog.koehntopp.info/2021/11/08/amd-und-128-cores.html</link>
      <pubDate>Mon, 08 Nov 2021 13:28:27 +0100</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2021/11/08/amd-und-128-cores.html</guid>
      <description>&lt;p&gt;Wir sprachen in
&lt;a href=&#34;https://blog.koehntopp.info/2021/09/30/software-defined-silicon.html&#34;&gt;Software Defined Silicon&lt;/a&gt;


darüber, wie die CPU-Bedürfnisse von Hyperscalern und normalen Kunden divergieren.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hyperscaler haben Interesse an immer größeren CPUs mit immer mehr Kernen, und immer höherer Dichte in ihren Rechenzentren. [&amp;hellip;]&lt;/p&gt;
&lt;p&gt;Normale Kunden sehen das nicht so: man kann in einer 64C/128T-Core-Single-Socket-Konfiguration mit 2-4 TB RAM unter Umständen den gesamten Serverbedarf einer kleineren Firma in einer einzelnen physikalischen Maschine in VMs unterbringen.
Das Problem dabei: Explosionsradius, wenn mal etwas ausfällt.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Und das passiert:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.phoronix.com/scan.php?page=news_item&amp;amp;px=AMD-Zen-4-Genoa-Bergamo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AMD Shares Early Details Of Zen 4 Genoa, Bergamo&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;Heute hat AMD einen Ausblick auf die kommenden Zen 4 CPUs geliefert:
Die normale &amp;ldquo;Genoa&amp;rdquo; wird 96 Cores pro Socket liefern, also 192C/384T in einem 2P-Board.&lt;/p&gt;
&lt;p&gt;Es wird jedoch von dieser CPU auch eine &amp;ldquo;Bergamo&amp;rdquo;-Variante geben, und das ist eine
&amp;ldquo;high-core count compute engine designed for cloud-native workloads&amp;rdquo;.
Das sind dann 256C/512T in einem 2P-Board.&lt;/p&gt;
&lt;p&gt;Das ist nicht nur zu viel Maschinerie in einer einzelnen Kiste, sondern auch in einem normalen Rechenzentrums-Rack vermutlich nicht mehr so einfach zu kühlen.
Wenn man sich als Hyperscaler jedoch seine Rechenzentren nach Maß bauen lässt, sollte das alles nicht weiter weh tun.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Wozu der 4K-Monitor, fragst Du?
Na, damit die &amp;lsquo;htop&amp;rsquo; Anzeige auf den Bildschirm passt.&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Software Defined Silicon</title>
      <link>https://blog.koehntopp.info/2021/09/30/software-defined-silicon.html</link>
      <pubDate>Thu, 30 Sep 2021 20:01:06 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2021/09/30/software-defined-silicon.html</guid>
      <description>&lt;p&gt;Golem titelt &lt;a href=&#34;https://www.golem.de/news/software-defined-silicon-intel-will-xeon-funktionen-als-lizenz-update-verkaufen-2109-159912.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel will Xeon-Funktionen als Lizenz-Update verkaufen&lt;/a&gt;

:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intel will Xeon-Funktionen als Lizenz-Update verkaufen.&lt;/p&gt;
&lt;p&gt;Mit dem Software Defined Silicon will Intel in Xeon-Hardware zunächst abgeschaltete Funktionen künftig als Lizenz-Upgrade bereitstellen.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/HonkHase/status/1442760700112343044&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Manuel Atug ranted&lt;/a&gt;

 darüber auf Twitter:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2021/09/intel-software-silicon.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Wenn dir die eigene Hardware nicht mehr gehört&amp;hellip;
Intel will Xeon-Funktionen als Lizenz-Update verkaufen
&amp;ldquo;Mit dem Software Defined Silicon will Intel in Xeon-Hardware zunächst abgeschaltete Funktionen künftig als Lizenz-Upgrade bereitstellen.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ich &lt;a href=&#34;https://twitter.com/isotopp/status/1442896442926895104&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;antwortete&lt;/a&gt;

:&lt;/p&gt;
&lt;p&gt;Das braucht wesentlich mehr Kontext.&lt;/p&gt;
&lt;h1 id=&#34;xeons-sind-selten-und-haben-nicht-viele-kunden&#34;&gt;
    &lt;a href=&#34;#xeons-sind-selten-und-haben-nicht-viele-kunden&#34;&gt;
	Xeons sind selten und haben nicht viele Kunden
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Intel stellt zurzeit eine sehr kleine einstellige Anzahl Millionen Xeon CPUs her.
Davon gehen 85 % an weniger als 10 Kunden - Hyperscaler in den USA und China.
12 % oder so gehen an ca. 150 Kunden, die messbare Stückzahlen der Produktion aufkaufen.
Der Rest sind Kleinkunden.&lt;/p&gt;
&lt;p&gt;Das ist eine sehr ungesunde Marktstruktur, und Intel weiß um die Probleme, die das mit sich bringt.
Dies ist keine neue Situation, sondern war schon ein Problem, bevor Apple mit ARM losgelegt hat und AMD mit dem EPYC auf den Markt kam.
Diese beiden Ereignisse haben die Situation aber noch verkompliziert.&lt;/p&gt;
&lt;p&gt;Dazu kommt noch, dass die obigen Prozentangaben nach Stückzahlen sind, nicht nach Core-Count.
Denn ansonsten sähe die Situation noch extremer aus:
Hyperscaler haben Interesse an immer größeren CPUs mit immer mehr Kernen, und immer höherer Dichte in ihren Rechenzentren.
Hyperscaler haben CPU Utilization und Return-per-Core als KPI und wollen Custom Silicon zum Auslagern der Virtualisierung, für das Netz, oder spezielle Tensorflow CPUs.&lt;/p&gt;
&lt;p&gt;Normale Kunden sehen das nicht so:
man kann in einer 64C/128T-Core-Single-Socket-Konfiguration mit 2-4 TB RAM unter Umständen den gesamten Serverbedarf einer kleineren Firma in einer einzelnen physikalischen Maschine in VMs unterbringen.
Das Problem dabei: Explosionsradius, wenn mal etwas ausfällt.&lt;/p&gt;
&lt;h1 id=&#34;cloud-und-on-premise-driften-auseinander&#34;&gt;
    &lt;a href=&#34;#cloud-und-on-premise-driften-auseinander&#34;&gt;
	Cloud und On-Premise driften auseinander
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Effektiv bewirkt dies, dass die Bauweise der Rechner, die in der Public Cloud Dienst tun, und die Bauweise der Rechner in den Rechenzentren normaler Leute stehen auseinander driften.
Das fängt heute an, aber der Bruch wird sich in den kommenden 5-10 Jahren noch vertiefen.&lt;/p&gt;
&lt;p&gt;Ein Xeon-Markt für Hyperscaler ist groß, aber der Xeon-Markt &amp;ldquo;Server CPU für Cloud-Feinde&amp;rdquo; wird eher klein sein und schrumpfen.
Und wird sich dann irgendwann nicht mehr lohnen.&lt;/p&gt;
&lt;p&gt;Davon mal abgesehen wird durch die laufend verbesserte Integration Chipfläche frei, und Intel muss sehen, was damit anzufangen ist, wenn es nicht &amp;ldquo;immer mehr Cores&amp;rdquo; sein können.
Software Defined Silicon ist ein Testballon, mit dem man das herausfinden kann.
Für die Firma wäre es schon wichtig, das herauszufinden:
von oben drücken die Hyperscaler mit ihren Forderungen nach Mengenrabatten, von der Seite drängt AMD mit ihrer derzeit überlegenen EPYC-Architektur in den Markt und von unten drückt ARM.&lt;/p&gt;
&lt;p&gt;Vielen Kunden können diese Pay-to-Play Features egal sein:
Eine Webshop-Workload sieht in einem Intel vTune aus wie ein REP MOVSB Benchmark, memcpy() at scale, Variablen werden in HTML Templates eingesetzt.&lt;/p&gt;
&lt;p&gt;Sogar die FPU und die ganzen AVX-Instruktionen - das ist alles Verschwendung von Chipfläche für die meisten Kunden.
Nur wenige Anwendungen brauchen und nutzen diese Funktionalität in der CPU wirklich.&lt;/p&gt;
&lt;p&gt;Man stelle sich vor, man muss zum Beispiel für einen Inference Offloader (&amp;ldquo;bestehende Machine Learning Netze ausführen, nicht lernen&amp;rdquo;) bezahlen, und dann wird der auf den meisten Maschinen nie angewendet.
Aber eine Serie CPUs herstellen, die diesen Inference Offloader hat, ihnen eine eigene Seriennummer zu geben und sie einzeln zu verkaufen will auch keiner.
Die Stückzahlen und die Preise geben das nicht her.&lt;/p&gt;
&lt;h1 id=&#34;der-marktdruck-zeigt-zur-cloud&#34;&gt;
    &lt;a href=&#34;#der-marktdruck-zeigt-zur-cloud&#34;&gt;
	Der Marktdruck zeigt zur Cloud
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Wenn aber absehbar ist, dass in 5-10 Jahren die Trennung von Cloud-Hardware und On-Premise Hardware vollzogen sein wird, dann ist es auch klar, dass es nach Ablauf dieser Zeit nicht mehr möglich sein wird, ein Rechenzentrum mit Private Cloud kompetitiv zu betreiben.
Das wird so kommen, weil die Cloud-Technik schlicht proprietär und für normalsterbliche Kunden nicht zu haben sein wird.&lt;/p&gt;
&lt;p&gt;Bei der Technik und der Wirtschaftlichkeit zeigen alle Push- und Pull-Faktoren derzeit massiv in Richtung Cloud.
Wer sein Unternehmen und seine IT darin nicht mit Gewalt und jetzt in die Public Cloud bringt, der wird nach dem Vollzug dieser Trennung in die Röhre schauen.&lt;/p&gt;
&lt;p&gt;Eigene Server vor Ort sind - bei vorhersagbarem Bedarf - auf viele Weisen billiger als in der Cloud zu mieten.
Aber man muss dann auch die eigenen Operations professionalisieren und man muss sich überlegen, wo in 5-10 Jahren die Technik herkommen soll.&lt;/p&gt;
&lt;p&gt;Und wenn wir uns noch einmal die eingangs diskutierte Kundenstruktur von Intel ansehen, dann gibt es weniger als 200 Unternehmen weltweit, die dazu die erforderliche Masse haben.
Das ist sehr frustrierend.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rechenzentren und ihren Stromverbrauch regulieren</title>
      <link>https://blog.koehntopp.info/2020/11/01/rechenzentren-und-ihren-stromverbrauch-regulieren.html</link>
      <pubDate>Sun, 01 Nov 2020 13:04:46 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2020/11/01/rechenzentren-und-ihren-stromverbrauch-regulieren.html</guid>
      <description>&lt;p&gt;Es gibt ein Interview mit Stefan Ramesohl vom Umweltministerium (des Bundes) in Netzpolitik.org: &amp;ldquo;&lt;a href=&#34;https://netzpolitik.org/2020/interview-zur-umweltpolitischen-digitalagenda-warum-niemand-weiss-wie-viele-rechenzentren-es-in-europa-gibt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warum niemand weiß, wie viele Rechenzentren es in Europa gibt&lt;/a&gt;

&amp;rdquo;. Im Wesentlichen hat das Umweltministerium angesagt, daß es auf europäischer Ebene Rechenzentren erfassen und katalogisieren will, um in einem zweiten Schritt den Energieverbrauch von Rechenzentren zu regulieren.&lt;/p&gt;
&lt;p&gt;Das ist sehr spannend, denn derzeit gibt es keine Übersicht über Rechenzentren in Europa, und tatsächlich sind einige Rechenzentrumsbetreiber sehr paranoid, was den genauen Standort ihrer Hardware angeht und wieviel und welche Hardware darin ist oder was diese tut. Das ist zwar lächerlich - es ist sehr schwierig eine Energiesenke wie ein Rechenzentrum und ihre Abwärme zu verstecken - aber auch ein sehr sensitives Thema.&lt;/p&gt;
&lt;h2 id=&#34;eine-leseliste&#34;&gt;
    &lt;a href=&#34;#eine-leseliste&#34;&gt;
	Eine Leseliste
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;In dem Interview gibt es ein paar Dinge, die Anmerkungen verdienen, aber bevor es los geht noch die anderen Artikel in diesem Blog als Links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.koehntopp.info/2017/07/19/threads-vs-watts.html&#34;&gt;Threads vs. Watts&lt;/a&gt;

: Ich habe einen Dell R630 mit zwei Xeon 6132 CPUs getestet, und deren Energieverbrauch unter Last gemessen. Die Resultate sind repräsentativ für die ganze Klasse von Rechnern, die eine Art Arbeitspferd im modernen Rechenzentrum sind. Der Hauptpunkt: 50% der maximalen Energieaufnahme werden bereits bei 20% Auslastung aufgenommen.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.koehntopp.info/2018/02/21/a-journey-to-open-compute.html&#34;&gt;A Journey to Open Compute&lt;/a&gt;

: Mit Open Compute hat Facebook die Energieaufnahme eines Rechners in Idle auf 50% eines herkömmlichen Rechners senken können, und unter Volllast auf 80%. Das wird ermöglicht, indem man Rechner, Rack und Raum nach einer gemeinsamen Spezifikation baut und optimiert. Der Open Compute Standard ist jetzt eine offene Spezifikation, aber wegen der Abhängigkeiten zwischen Raum, Rack und Rechner lohnt sich das alles nur, wenn man ein &lt;a href=&#34;https://en.wikipedia.org/wiki/Big_Tech#GAFAM_or_FAAMG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAFAM&lt;/a&gt;

-type Hyperscaler ist.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.koehntopp.info/2017/11/07/power-budgets-for-computing-resources-portable-and-stationary.html&#34;&gt;Power budgets for computing resources - portable and stationary&lt;/a&gt;

 listet generell die Zusammenhänge zwischen Rechnen, Batterieverbrauch und Abwärme auf.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.koehntopp.info/2019/10/05/data-centers-and-energy.html&#34;&gt;Data Centers and Energy&lt;/a&gt;

: Wenn man Netflix schaut, wird Energie verbraucht. Wo und wieviel? Wir reden über Endgeräte (die nur wenige Watt brauchen), über Open Compute in Rechenzentren und über Energieverbrauch im Netzwerk, speziell auf der letzten Meile. Letzterer variiert enorm: 5G braucht sehr viel Energie, (V)DSL ist ebenfalls sehr aufwendig, und Glasfaser nicht - sie ist leicht eine Zehnerpotenz günstiger.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.koehntopp.info/2019/12/28/streaming-and-energy.html&#34;&gt;Streaming and Energy&lt;/a&gt;

 und &lt;a href=&#34;https://blog.koehntopp.info/2020/03/19/netflix-does-not-bring-down-the-internet.html&#34;&gt;Netflix does not bring down the Internet&lt;/a&gt;

: Speziell Videostreaming funktioniert schon sehr optimiert: Videos werden in Edge Data Centers gespeichert und nicht neu codiert, sie werden in der niedrigsten sinnvollen Auflösung geliefert und die Decodierung erfolgt mit spezieller Hardware, damit die Batterie im Endgerät länger hält. All das braucht weniger Energie als angenommen.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.koehntopp.info/2020/06/08/cloud-and-energy.html&#34;&gt;Cloud and Energy&lt;/a&gt;

: Das Uptime Institut sagt: &amp;ldquo;Data center energy efficiency gains have flattened out&amp;rdquo; (und sieht einen durchschnittlichen PUE von 1.58). Uptime sagt im selben Text aber auch, daß neuere und größere Facilities mit Open Compute &lt;em&gt;signifikant&lt;/em&gt; bessere PUE haben. Der einfachste Weg zur Verbesserung von PUE für die meisten Firmen ist, ihre Workloads in die Cloud zu verlagern (und dynamisch zu skalieren). Das hat bei korrekter Durchführung neben Energieffizienz auch noch jede Menge andere Vorteile, die sich aus einem gelungenen Outsourcing ergeben.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hyperscaler-rechenzentren-sind-viel-energieeffizienter&#34;&gt;
    &lt;a href=&#34;#hyperscaler-rechenzentren-sind-viel-energieeffizienter&#34;&gt;
	Hyperscaler-Rechenzentren sind viel energieeffizienter
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Auf Twitter ging ich auf &lt;a href=&#34;https://twitter.com/isotopp/status/1322857383929012224&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;das Interview ein&lt;/a&gt;

:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Netzpolitik.org:&lt;/em&gt; Diese großen Player können ja kein Interesse an staatlicher Regulierung haben, sondern werden versuchen, eine branchenweite Selbstverpflichtung herbeizuführen.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Eher nicht. Als &lt;a href=&#34;https://en.wikipedia.org/wiki/Big_Tech#GAFAM_or_FAAMG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAFAM&lt;/a&gt;

 wäre man sinnvollerweise für mehr Regulierung, denn das käme effektiv einem Cloud-Zwang gleich.&lt;/p&gt;
&lt;p&gt;Wie oben bereits dargestellt, hat GAFAM bereits Rechenzentren, die sehr viel effizienter mit der Energie umgehen als normale Rechenzentren es tun. Das ist so, weil diese Rechenzentren Raum, Rack und Rechner als ein System designed haben und weil die Betreiber als Hyperscaler es sich leisten können, solche Rechenzentren nach Maß zu designen, bauen zu lassen und zu optimieren.&lt;/p&gt;
&lt;p&gt;Während also ein traditionelles Rechenzentrum Rechner für eine Million Watt betreibt und dafür um die 600.000W an Kühlung und anderer Sekundärenergie aufbringen muß (Power Utilization Efficiency, PUE 1.6), können die am Besten optimierten Google-Rechenzentren eine Million Watt an Rechnern mit 60.000W Sekundärenergie betreiben (PUE 1.06).&lt;/p&gt;
&lt;p&gt;Ein effektiver PUE von &amp;lt;1.2 ist par für die Hyperscaler-Cloud.&lt;/p&gt;
&lt;p&gt;Ein kleinerer Rechenzentrums-Nutzer füllt nicht ein ganzes RZ mit Rechnern, läßt also nicht nach Maß bauen, sondern mietet existierenden RZ-Space, der prinzipbedingt nicht gut geeignet ist für Open Compute (OCP). Existierende RZ-Space ist generisch, er muß jede Art von IT-Equiment aufnehmen können und ist daher oft Überkühlt, der Airflow ist nicht optimiert und hat auf diese Weise mindestens dreimal mehr Overhead als RZ-Space, den Hyperscaler nach Maß bauen (PUE &amp;lt;1.2 vs. PUE ~ 1.6). Noch kleinere Benutzer füllen nur einzelne Räume oder haben Raumabschnitte (&amp;ldquo;Cages&amp;rdquo;), teilen also die Kühlung mit anderen Nutzern.&lt;/p&gt;
&lt;p&gt;Hyperscaler bauen nicht nur ein RZ, sondern tun das in Serie, und iterieren dabei das Design. Sie lassen auch Rechner und Rechnerkonzepte wie OCP entwickeln, und stimmen dabei das Design des RZ auf das Design von Rack und Rechner ab - daher kommt die energetische Überlegenheit von OCP.&lt;/p&gt;
&lt;p&gt;Dazu kommt, wie oben auch dargestellt, daß ein ausgelasteter Rechner energieeffizienter ist als einer, der teilweise vor sich hin idled. Ein Dell R630 verbraucht bereits 50% seiner maximalen Energie bei 20% Auslastung.&lt;/p&gt;
&lt;p&gt;Maschinen auszulasten und Workloads dynamisch zu skalieren ist etwas, für das &amp;ldquo;die Cloud&amp;rdquo;, also die API-gesteuerten Rechenzentren der Hyperscaler, gebaut worden sind. Hyperscale Clouds haben &amp;ldquo;sellable cores per provisioned cores&amp;rdquo; als eine zentrale Optimierungsmetrik, sie wollen ausgelastete Rechner, weil das die Einnahmen definiert.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2017/07/watt-thread.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Watts per Thread (Dell R630, Dual Xeon 6132)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Wie dem auch sei: Aus energetischer Sicht ist Auslastung wichtig, weil die Watts zur Aktivierung des n-ten Rechenkerns asymptotisch günstiger sind. Das ist so, weil die Idle-Energieaufnahme der Maschine und des ganzen Rechenzentrums drumherum sich so amortisiert.&lt;/p&gt;
&lt;p&gt;Außerdem: Wenn man es sich leisten kann, Hyperthreading zu aktivieren (Wegen der diversen Intel-Caching-Bugs der letzten Jahre ist das oft ein Sicherheitsrisiko), dann sind die Hyperthreads energetisch betrachtet nahezu kostenfrei. Integer- und Stringprocessing-Workloads können Hypterthreads als nahezu vollwertige zweite CPU betrachten, Fließkomma-intensive Workloads nicht.&lt;/p&gt;
&lt;p&gt;Webshops sind, wenn sie richtig gebaut worden sind, String- und Integer-Anwendungen und sehr wenig Fließkomma-intensiv, könnten also von Hyperthreading voll profitieren. Die Anzahl der nutzbaren Cores verdoppelt sich rechnerisch und ist bei Webshop fast vollständig realisierbar - ein Webshop mit einer fast reinen Integer/String Workload kann von den 56 rechnerischen Kernen einer Dual-6132 eine Load von deutlich über 40 stabil verarbeiten.&lt;/p&gt;
&lt;h2 id=&#34;kosten-und-energie&#34;&gt;
    &lt;a href=&#34;#kosten-und-energie&#34;&gt;
	Kosten und Energie
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Über eine Nutzungsdauer von fünf Jahren gerechnet machen Energiekosten in etwa die Hälfte der Gesamtkosten eines Rechners aus - für einen Hyperscaler ist das ein so intensiver Kostenfaktor, daß sie aus wirtschaftlichen Gründen seit mehr als 15 Jahren intensiv die Energieaufnahme ihrer Rechenzentren in allen Punkten optimieren.&lt;/p&gt;
&lt;p&gt;Das ist der primäre Grund für das Open Compute Projekt (OCP) und die Bauweise der Hyperscaler-Rechenzentren. Für Hyperscaler lohnt dies, weil das der Kernbereich ihres wirtschaftlichen Handelns ist.&lt;/p&gt;
&lt;p&gt;Dazu kommt, wie in der Leseliste dargestellt, daß alle Hyperscaler (bis auf Amazon) bereits 100% graugrün sind, also zu großen Teilen bereits auf tatsächlich regenerativer Energie laufen und den Rest mit Zertifikaten kompensieren, und obendrein sehr nah in der Zukunft liegende Ziele haben, was komplett grünen Betrieb &lt;em&gt;und&lt;/em&gt; Überkompensation angeht. Speziell Google ist ein sehr großer Investor in Wind- und Solarkraftanlagen, Netflix überkompensiert bereits jetzt, ist also Carbon-Negative.&lt;/p&gt;
&lt;p&gt;Für ein Firma, für die der Betrieb und die Skalierung von Rechenzentren und ihrer Hardware nicht der Kern ihres wirtschaftlichen Handelns ist, ist es ausgeschlossen hier mitzuhalten.&lt;/p&gt;
&lt;p&gt;Oder wie Ramesohl es formuliert:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Es ist einfach so, dass eine gewisse Skalierung zu großen Vorteilen führt, die dann wiederum über eine Wettbewerbsfähigkeit im Markt die Marktposition stärkt und damit den Marktanteil erhöht. Das ist ein selbstverstärkender Effekt. Hinzu kommt, dass diese Akteure in der Lage waren, zu investieren und sich damit ein technologisches Know-how aufzubauen, was wiederum im Umkehrschluss ihre Marktposition stärkt.&lt;/p&gt;
&lt;p&gt;Das ist richtig, dass gerade bei den großen Playern entsprechende selbstdefinierte Nachhaltigkeitsziele vorliegen. Das sind teilweise sehr ambitionierte Pläne, die versuchen, die Emissionen, die im Laufe der Unternehmensgeschichte bisher aufgelaufen sind, rückwirkend zu kompensieren.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;bandbreite-und-energie&#34;&gt;
    &lt;a href=&#34;#bandbreite-und-energie&#34;&gt;
	Bandbreite und Energie
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Ramesohl sagt auch:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Da geht es um die Frage, ob jede Internetwerbung eine Autoplay-Funktion braucht, also abspielt, wenn ich nur über die Seite scrolle. Das erzeugt nämlich ein enormes Datenvolumen. Oder die Frage, ob alles standardmäßig in höchster Auflösung gestreamt werden muss, wenn es auf einem kleinen Bildschirm angeschaut wird.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Dahinter steht die Fehlmeinung, daß eine Netzwerkinfrastruktur mehr Energie benötigt, wenn Daten übertragen werden. Das ist nicht der Fall.&lt;/p&gt;
&lt;p&gt;So wie eine Festplatte oder RAM nicht schwerer werden oder mehr Energie verbrauchen, wenn Daten darin gespeichert werden, so braucht ein kabelgebundenener Netzwerklink nicht (wesentlich) mehr Energie, wenn Daten übertragen werden.&lt;/p&gt;
&lt;p&gt;RAM und Festplatten brauchen (mehr) Energie, wenn Daten &lt;em&gt;geändert&lt;/em&gt; werden, also Bits gekippt werden.&lt;/p&gt;
&lt;p&gt;Und kabelgebundenene Netzwerkinfrastruktur überträgt immer Daten (Trägersignale), auf die die Nutzlast dann aufmoduliert wird. Das wiederum braucht im Vergleich zur Grundlast des Netzes (das Senden des Trägers) kaum Energie. Anders sieht es bei Funkverbindungen, also Datenübertragung via Mobilfunknetz aus, dort wird der Sender komplett abgeschaltet, wenn er nicht gebraucht wird, um das Spektrum frei zu halten.&lt;/p&gt;
&lt;p&gt;Speziell bei Glasfaser ist es so, daß man die Bandbreite zudem mit nur wenig mehr Energieaufwand um Größenordnungen hochdrehen kann, wenn man will - durch den Austausch der Laser kann dasselbe Medium in der Kapazität verzehnfacht oder verhundertfacht werden ohne signifikat mehr Energie zu verbrauchen.&lt;/p&gt;
&lt;h2 id=&#34;was-tun-wir-mit-all-dem-compute&#34;&gt;
    &lt;a href=&#34;#was-tun-wir-mit-all-dem-compute&#34;&gt;
	Was tun wir mit all dem Compute
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Am Ende ist es eventuell eine gute Idee, nicht nur genauer hin zu schauen, wo Rechenzentren stehen und wie sie designed sind, sondern was mit den Megawatts gemacht wird, die dort verwendet werden. Auf diese Weise bekommen wir eventuell Bitcoin weg gebombt. Das wäre schon einmal sehr wichtig, denn hier wird Energie in der Größenordnung ganzer Staaten in sinnlosen Berechnungen (&amp;ldquo;Proof of Work&amp;rdquo;) verheizt. Und nein, das kann man nicht ändern, PoW kann nicht sinnvolles berechnen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IT modernisieren und konsolidieren</title>
      <link>https://blog.koehntopp.info/2020/10/05/it-modernisieren-und-konsolidieren.html</link>
      <pubDate>Mon, 05 Oct 2020 21:52:04 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2020/10/05/it-modernisieren-und-konsolidieren.html</guid>
      <description>&lt;p&gt;Ich schrieb in einem &lt;a href=&#34;https://twitter.com/isotopp/status/1313084134168944640&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter Thread&lt;/a&gt;

 über &lt;a href=&#34;https://blog.koehntopp.info/2020/10/05/what-are-the-problems-with-posix.html&#34;&gt;Posix Dateisysteme vs. Object Stores&lt;/a&gt;

:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;UNIX FS ist 1974.
BSD FFS ist 1984.
XFS ist 1994.
ZFS (und Btrfs und Wafl) sind LFS, also 2004.
Object Storages, LSM, &amp;ldquo;RocksDB&amp;rdquo; ist ca. 2014, um den Takt zu halten.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;und wurde gefragt: &amp;ldquo;Was kommt 2024&amp;rdquo;. Meine halb spöttische, halb ernst gemeinte Antwort war:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Irrelevant.&lt;/p&gt;
&lt;p&gt;2024 läuft Dein Code serverless bei einem professionellen Betreiber und vom lokalen System und dem lokalen Dateisystem kriegst Du nix mehr zu sehen außer einer monatlichen Rechnung.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;rein-in-die-cloud&#34;&gt;
    &lt;a href=&#34;#rein-in-die-cloud&#34;&gt;
	Rein in die Cloud
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Selbst wenn es kein &amp;ldquo;serverless&amp;rdquo; ist, sondern eine VM oder ein Container: Wir haben dazu gelernt, und sind besser geworden. Deswegen sind aber auch unsere Standards und Anforderungen gestiegen und wir brauchen bessere Umgebungen.&lt;/p&gt;
&lt;p&gt;Die Prozesse und die Technik lokal so aufzusetzen, daß man weiterhin compliant bleibt mit SOX, PCI und PII ist eine ganze Menge Arbeit, die nicht zur Kern-Mission der meisten Unternehmen gehört. Es ist auch ein Investment, das besser in geschäftsrelevante Bereiche ginge. Techniken und Infrastruktur bereit zu stellen, die in einem Amazon-like Environment &amp;ldquo;so da&amp;rdquo; ist wird immer schwieriger und teurer werden.&lt;/p&gt;
&lt;h3 id=&#34;ein-paar-selbstverständlichkeiten&#34;&gt;
    &lt;a href=&#34;#ein-paar-selbstverst%c3%a4ndlichkeiten&#34;&gt;
	Ein paar Selbstverständlichkeiten
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Auf der Minimum-Liste für alle stehen inzwischen Dinge wie&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single Sign On und Identity Management (SSO und IAM)&lt;/li&gt;
&lt;li&gt;Rollenbasierte Access Controls (RBAC)&lt;/li&gt;
&lt;li&gt;eine PKI&lt;/li&gt;
&lt;li&gt;Encryption at Rest und Encryption in Flight&lt;/li&gt;
&lt;li&gt;Administrator Roles mit Segregation of Duties&lt;/li&gt;
&lt;li&gt;Mandatory Access Controls and Privilege Limits&lt;/li&gt;
&lt;li&gt;Audit Trails&lt;/li&gt;
&lt;li&gt;Complance und Audit von all dem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Und das ist schon mehr Arbeit als man als Firma mit einem eigenen RZ-Betrieb selbst leisten und erfinden kann. Es ist auch etwas, das so nicht fertig kaufbar und als Produkt installierbar ist, weil es eben nicht nur Technik ist, sondern auch Prozeß und Kultur.&lt;/p&gt;
&lt;p&gt;Es sind aber alles Dinge, die man als Technik in einem AWS Environment oder einer anderen großen Cloud per Default bekommt und zu der es dann auch Unterweisungen gibt, die einem Best Practices und funktionierende Prozesse nahe bringen.&lt;/p&gt;
&lt;p&gt;Dazu kommen dann Dienste, die in einem eigenen Rechenzentrum mühevoll oder teuer zu schaffen sind, die man aber in einer großen Cloud testweise oder produktiv dazu nehmen kann, ohne eigenen Aufwand zu treiben.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Geocoder&lt;/li&gt;
&lt;li&gt;Spracherkennung&lt;/li&gt;
&lt;li&gt;Bilderkennung&lt;/li&gt;
&lt;li&gt;Integration in eine skalierbare Big Data Umgebung&lt;/li&gt;
&lt;li&gt;Integration von Eventprozessoren&lt;/li&gt;
&lt;li&gt;Event Driven Execution (&amp;ldquo;Step Functions&amp;rdquo;), die es auch Nicht-Codern erlauben, Anwendungen durch Zusammenklebeben von -aaS Diensten zu schaffen.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Aber das sind nur Beispiele aus hunderten von Diensten&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;nicht-die-mission-der-meisten-betriebe&#34;&gt;
    &lt;a href=&#34;#nicht-die-mission-der-meisten-betriebe&#34;&gt;
	Nicht die Mission der meisten Betriebe
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Das ist etwas, das einer traditionellen RZ-IT nicht zugänglich ist, und das auch unverhältnismäßig viel Kapital, Investment und Personal binden würde, würde man es selbst lokal entwickeln. Schon so etwas simples wie RDS funktioniert besser und automatischer als alle selbstgestrickten DevOps Managementscripte für Datenbanken, die man selbst haben kann.&lt;/p&gt;
&lt;p&gt;Druck kommt nicht nur aus dem Management und aus der Compliance, sondern auch von unten: Die aktuelle Generation von Entwicklern hat nie mit einem lokalen RZ gearbeitet, sondern ist in AWS groß geworden. Sie setzt die Qualität der Implementierung und die Prozesse von Amazon als gegeben und als Maßstab voraus. Sie setzt die Leichtigkeit von Operations und Observability voraus, die für eine Cloud-Umgebung typisch sind.&lt;/p&gt;
&lt;p&gt;Deswegen ist die Zukunft von 2024 mindestens hybrid. Aber effektiv wird kein Betrieb mit einem lokalen Rechenzentrum und reinen IaaS-Angeboten noch groß Stiche holen.&lt;/p&gt;
&lt;p&gt;Das ist einer der Gründe, warum Europa als Technologieraum zunehmend abgehängt ist. Nicht nur viele Politiker und Entscheider verstehen nicht mehr, was in den letzten 10 Jahren passiert ist, sondern auch viele Sysadmins &amp;ldquo;on the ground&amp;rdquo; haben die Veränderung des Entwicklerbetriebes in den letzten 10 Jahren grundlegend verpaßt und können nicht erkennen, welche Gestaltungsdrücke gerade auf ihrer Umgebung lasten.&lt;/p&gt;
&lt;p&gt;Das sehen wir nicht nur in der &amp;ldquo;deutsche Schulen in COVID&amp;rdquo; Videokonferenzdiskussion, sondern auch in der ganzen Klasse von BOFH-Bemerkungen, die aus dem Sysadmin Lager oft kommt. Ich habe dazu vor über fünf Jahren schon einmal was gemacht: &lt;a href=&#34;https://www.slideshare.net/isotopp/go-away-or-i-will-replace-you-with-a-little-shell-script#2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides: Go away or I will replace you with a very small Shell script - ein paar Gedanken zum Thema Devops&lt;/a&gt;

 (&lt;a href=&#34;https://media.ccc.de/v/froscon2015-1500-go_away_or_i_will_replace_you_with_a_very_little_shell_script&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video von der Froscon Version&lt;/a&gt;

).&lt;/p&gt;
&lt;p&gt;Die Frage ist nicht, wie ein lokales Rechenzentrum Rechner betreibt und bereitstellt - das ist ein gelöstes Problem. Sondern welche Dienste sie darauf fertig im Angebot haben, ob diese Dienste mit aktuellen Standard und Anforderungen in Compliance sind, ob sie eine API haben, und sie mit lokalem Tooling integrierbar sind.&lt;/p&gt;
&lt;h3 id=&#34;integration-und-bildung-von-untereinander-abhängigen-systemen&#34;&gt;
    &lt;a href=&#34;#integration-und-bildung-von-untereinander-abh%c3%a4ngigen-systemen&#34;&gt;
	Integration und Bildung von untereinander abhängigen Systemen
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Wir sind dabei, von individuellen, frei stehenden und trennbaren Teilen zu integrierten Systemen zu gehen. Das hat Vorteile, weil so Prozesse entstehen und eingeübt werden, die meßbar Qualität verbessern und - wichtiger noch - automatisieren.&lt;/p&gt;
&lt;p&gt;Das ist nicht nur im Bereich Betrieb mit der Cloud so, sondern ist ein größerer Trend, der auch in der Software-Entwicklung sichtbar wird: Entwickler arbeiten nicht mehr mit vi im leeren Editor, sondern haben in der Regel Sprachen mit umfangreichen Bibliotheken und Integrationen im Betrieb. Das hat Folgen.&lt;/p&gt;
&lt;p&gt;In der Entwicklung: Wenn es keinen JetBrains-Editor für die Sprache gibt, wenn sie von gitlab und github nicht erkannt und ausgewertet wird, wenn sie keinen Dependency-Manager und keine CI/CD Integration hat, dann ist eine moderne Programmiersprache - Entschuldigung - Plattform - Entschuldigung - Ökosystem nicht vollständig und nicht mehr konkurrenzfähig.&lt;/p&gt;
&lt;p&gt;Wenn eine Sprache nicht von einschlägigen Security- und Audit-Werkzeugen unterstützt wird, wenn sie nicht &lt;a href=&#34;https://www.sonarqube.org/features/multi-languages/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bei SonarQube&lt;/a&gt;

 oder ähnlichen auf der Liste steht, wenn sie nicht von Valgrind, Fuzzern und anderen Werkzeugen unterstützt wird, dann hat eine moderne Programmierumgebung in einem Enterprise-Umfeld zunehmend Schwierigkeiten, die steigenden Compliance-Anforderungen im Bereich Software-Entwicklung und Qualitätssicherung zu erfüllen.&lt;/p&gt;
&lt;p&gt;Im Betrieb: Ohne Identity, PKI, SSO, RBAC, Segregation of Duties, Encryption überall, Auditing und enforceable maximum permissions hast Du Compliance Probleme oder wirst sie bald haben. Und ohne eine API für alles, Tooling für diese API, Infrastructure als Code, Codified Practices wie CI/CD hast Du keine attraktive und effektive Entwicklungsumgebung für Deine eigenen Leute.&lt;/p&gt;
&lt;p&gt;Das sind aber Gedanken und Entwicklungen, die in Deutschland an Politik und an Teilen der &amp;ldquo;Informatikschaffenden&amp;rdquo; vorbei gegangen sind, oder belächelt worden sind.&lt;/p&gt;
&lt;h2 id=&#34;raus-aus-der-cloud&#34;&gt;
    &lt;a href=&#34;#raus-aus-der-cloud&#34;&gt;
	Raus aus der Cloud
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Offensichtlich sind Server Wurst. Die Frage ist mehr, was da drauf läuft, welche Dienste und Integrationen bestehen. Das ist mit einem IT-Team von 4-5 Leuten vor Ort nicht sinnvoll zu schaffen, sondern im günstigsten Fall ein blankes IaaS, nicht unbedingt von der guten und vertrauenswürdigen Sorte.&lt;/p&gt;
&lt;p&gt;Schau Dir an, was zum Beispiel Scaleway oder OVH macht, oder 1und1 in Deutschland. Das sind fitte Teams, die eine Menge weg schaffen. Aber deren Clouds sind relativ dienstfreie IaaS-Clouds mit ein wenig Object Storage, und alle größeren Dinge mußt Du Dir selbst auf deren IaaS ansibilisieren.&lt;/p&gt;
&lt;p&gt;Das ist aber der Stand von 2010, nicht 2020. In 2020 ist ein Cloudkunde hinter Diensten her, damit er die nicht selbst betreiben muß.&lt;/p&gt;
&lt;p&gt;Es ist ja nicht nur unwürdig, HPE deren defekte iLOs und kaputte Festplatten-Firmware zu debuggen, sondern man will sich auch einfach ein MySQL oder Postgres klicken können. Oder gar einfach &amp;ldquo;einen KV Storage benutzen&amp;rdquo; und Dein Zeugs in ein Dynamo kippen und Dich gar nicht mehr um Instanzgrößen kümmern müssen, sondern nur noch für Storage und Zugriff nach Verbrauch bezahlen.&lt;/p&gt;
&lt;p&gt;Und an dieser Stelle kommt wieder die Politik durch.&lt;/p&gt;
&lt;h3 id=&#34;arbeitsteilung-beruht-auf-vertrauen-vertrauen-kommt-aus-transparenz-und-verläßlichkeit&#34;&gt;
    &lt;a href=&#34;#arbeitsteilung-beruht-auf-vertrauen-vertrauen-kommt-aus-transparenz-und-verl%c3%a4%c3%9flichkeit&#34;&gt;
	Arbeitsteilung beruht auf Vertrauen. Vertrauen kommt aus Transparenz und Verläßlichkeit
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Das funktioniert mit den Diensten nämlich ganz wunderbar in der &amp;ldquo;Arbeitsteiligen Gesellschaft™&amp;rdquo; - wir nennen so etwas Zivilisation. Nur, wenn wir zivilisiert, verantwortungsvoll und vertrauensvoll miteinander umgehen, dann kann man sich selbst die Arbeit mit den ganzen Diensten schenken und jemand anders beauftragen.&lt;/p&gt;
&lt;p&gt;Dazu sind bestimmte gesellschaftlichen Bedingungen notwendig, die zu schaffen sind. Wir brauchen ein System von Checks und Balances, internationalen Verträgen und Business Practices, und dann ein System von Kontrollen und &lt;em&gt;Vertrauen in die Wirksamkeit dieser Kontrollen&lt;/em&gt;, damit wir zivilisiert cloudcomputen können.&lt;/p&gt;
&lt;p&gt;Wenn Du aber einzelne Nation States hast, die technisch gesehen als Attacker da stehen (&amp;ldquo;Nation State Attacker&amp;rdquo;, &amp;ldquo;NSA&amp;rdquo;) und sich das auch so vorbehalten, dann ruiniert das die Geschäftsgrundlage für die arbeitsteilige Gesellschaft, mithin die Zivilisation selbst.&lt;/p&gt;
&lt;p&gt;Wenn Du einzelne Anbieter hast, die über die Verarbeitung und Nutzung der Daten nicht transparent sind, oder man den von ihnen vorgezeigten Audits kein Vertrauen schenken kann, dann &amp;hellip; genau dasselbe.&lt;/p&gt;
&lt;p&gt;Und wenn Du eine Politik hast, die ein Klima schafft, in der das Verständnis dieser Tatsachen geleugnet oder ignoriert wird, dann ruiniert das dieses Konzept schon.&lt;/p&gt;
&lt;p&gt;Das ist genau das, was Gestalten wie Trump, BoJo aber auch von Storch zerstören, wenn sie Isolationismus, Staatswilkür und Ignoranz gegenüber internationaler Ordnung demonstrieren. Das ist aber auch das, was Gestalten wie Audi Scheuer und sein Meister, Imperator Seehofer vernichten, denn solche vorgelebte Unfähigkeit und Korruption saugen der Zivilisation das Rückenmark aus. Das ist dann das, was zu Dingen wie Wirecard führt, zum Dieselskandal und zu Situationen, bei denen ein Volkswagen-Compliancemanager bei der Dokumentation illegaler Geschäftspraktiken auf eine Weise zu Tode kommt, die in jede Netflix-Mafiaserie gepaßt hätte.&lt;/p&gt;
&lt;p&gt;In so einem Umfeld ist eine Auslagerung der IT an Dritte für Unternehmen ein bedenkenswertes Risiko statt eine Erleichterung.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cloud and Energy</title>
      <link>https://blog.koehntopp.info/2020/06/08/cloud-and-energy.html</link>
      <pubDate>Mon, 08 Jun 2020 11:15:48 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2020/06/08/cloud-and-energy.html</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;https://blog.koehntopp.info/2019/10/05/data-centers-and-energy.html&#34;&gt;Data Centers and Energy&lt;/a&gt;

 I wrote about Hyperscaler Data Centers and Open Compute, and how they bring down the PUE of data centers, making them more efficient, and in &lt;a href=&#34;https://blog.koehntopp.info/2019/12/28/streaming-and-energy.html&#34;&gt;Streaming and Energy&lt;/a&gt;

 I followed up on this, explaining how Netflix energy usage fits into this. Now the Uptime Institute has released &lt;a href=&#34;https://journal.uptimeinstitute.com/data-center-pues-flat-since-2013/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a study&lt;/a&gt;

 that claims &amp;ldquo;Data center energy efficiency gains have flattened out&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;It is summarized as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The average power usage effectiveness (PUE) ratio for a data center in 2020 is 1.58, only marginally better than 7 years ago, according to the latest annual Uptime Institute survey (findings to be published shortly).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;But with a lot of caveats:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As ever, the data does not tell a complete story. This data is based on the average PUE per site, regardless of size or age. Newer data centers, usually built by hyperscale or colocation companies, tend to be much more efficient, and larger. A growing amount of work is therefore done in larger, more efficient data centers (Uptime Institute data in 2019 shows data centers above 20 MW to have lower PUEs). Data released by Google shows almost exactly the same curve shape — but at much lower values.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Which basically says the opposite: actually newer, larger hyperscaler cloud datacenters are way more energy efficient, and continued to do so after 2013. But a lot of IT is stagnating in old build, and is not being upgraded nor moved anywhere, so the numbers seem to stagnate when they are not.&lt;/p&gt;
&lt;p&gt;It suggests the best way to improve the PUE of your data center is to move to get cloud and get rid of it, but Uptime being Uptime they cannot phrase it that way.&lt;/p&gt;
&lt;p&gt;(via &lt;a href=&#34;https://twitter.com/SpeicherStief/status/1269906522122960896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@speicherstief&lt;/a&gt;

)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Streaming and Energy</title>
      <link>https://blog.koehntopp.info/2019/12/28/streaming-and-energy.html</link>
      <pubDate>Sat, 28 Dec 2019 19:42:16 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2019/12/28/streaming-and-energy.html</guid>
      <description>&lt;p&gt;A bunch of boomers in Germany is running a distraction campaign on the energy use of data centers and streaming. Example articles in german language can be found in
&lt;a href=&#34;https://www.zeit.de/2020/01/digitalpolitik-digitalisierung-klimaschutz-co2-stromverbrauch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zeit&lt;/a&gt;


and
&lt;a href=&#34;https://www.bento.de/politik/klickscham-wie-viel-co2-streaming-und-googlen-verursacht-und-welche-loesungen-es-gibt-a-c6e5ff54-71e9-46da-80cf-6ee1547d8b3a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bento&lt;/a&gt;

,
but there is a larger series of articles acrooss multiple newspapers.&lt;/p&gt;
&lt;p&gt;A better structured reasoning can be found in &lt;a href=&#34;https://www.srf.ch/news/panorama/energieverbrauch-im-internet-warum-streaming-viel-strom-braucht&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SRF&lt;/a&gt;

 (German), and it highlights how arbitrary and wrong the energy numbers in the former articles are. But even this article ignores the facts that the energy consumption in a typical cloud data center is most likely carbon neutral, because the power used is likely to be completely green. How green exactly is depending on the cloud operator and the location of the data center - I have written a &lt;a href=&#34;https://blog.koehntopp.info/2019/10/05/data-centers-and-energy.html&#34;&gt;much more detailed overview&lt;/a&gt;

 elsewhere in this blog.&lt;/p&gt;
&lt;p&gt;For some reason, the majority of these articles focus on video streaming, ignoring outright waste of energy at a much larger scale, for example blockchain use. Furthermore, most of the calculations on the energy use of specifically video streaming are making flawed assumptions. They are designed to create vastly oversized energy footprints, and even more oversized carbon footprints.&lt;/p&gt;
&lt;p&gt;Some things to remember:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A video is not being encoded for every view.&lt;/strong&gt; A raw video is encoded or transcoded once for each target format, of which there is a small integer number. So 4K raw source material is being transcoded into 1080p, 720p, 480p and maybe a few even smaller formats. Some streamers support specific classes of end user devices with a limited, non-extensible set of legacy codecs, such as gaming consoles or similar. They typically add one or two more formats to the set, but in the end the storage cost (and energy) dominates the transcoding cost.&lt;/p&gt;
&lt;p&gt;The encoded file is stored and played on demand for each viewer. In effect this turns watching a video on a streaming service basically into a normal file download with a &amp;ldquo;fancy&amp;rdquo; download protocol.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2019/12/google-chunked.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;via &lt;a href=&#34;https://www.netmanias.com/en/post/blog/5923/google-http-adaptive-streaming-iptv-video-streaming-youtube/youtube-changing-the-way-of-delivering-videos-chunking-and-adaptive-streaming-are-in-progressive-download-is-out&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Netmanias: Youtube Chunking, 2013&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;Define &amp;ldquo;fancy&amp;rdquo;: The file is being chopped into chunks of around a few seconds playback time, and a number of chunks ahead of the current playback position is being downloaded in advance and buffered on the end user device. How many of these chunks are preloaded depends on a lot of parameters, such as device type, internet connection speed and user behavior (&amp;ldquo;are they jumping around a lot?&amp;rdquo;). Using chunked downloads allows for switching video resolution depending on changes in line speed or display window size, and for jumping around in the video without downloading unnecessary parts of the video.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A video is not shipped across the Atlantic Ocean for each viewer.&lt;/strong&gt; Most likely (and especially for popular videos), an edge cache is holding a copy of the file locally and serving it to the customer. Google documents this in &lt;a href=&#34;https://peering.google.com/#/infrastructure&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;their peering and caching documentation&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2019/12/google-edge.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Google provides a map of their Google Global Cache nodes. There is at least one in every metropolitan area. These nodes also cache popular Youtube videos. The cache is quite effective, an older 2012 paper (&lt;a href=&#34;https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/braun_noms2012_youtube_caching.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;

) discusses that.&lt;/p&gt;
&lt;p&gt;Netflix does similar things with their own infrastructure.&lt;/p&gt;
&lt;p&gt;This turns watching a video on a streaming service into downloading a file with a fancy download protocol from a pretty local server, with local being defined as &amp;ldquo;very close to your location in terms of network, not physical topology&amp;rdquo;. Where network topology matches physical topology, this may be &amp;ldquo;in your city&amp;rdquo;, in any case it is the minimum number of network hops, the copy &amp;ldquo;closest to you&amp;rdquo; that is being served.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A video is not being decoded in software on a regular CPU&lt;/strong&gt; The effect of a software vs. hardware video encoder is being shown in &lt;a href=&#34;https://www.youtube.com/watch?v=2YpOZV8elqA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this 2014 Youtube Video&lt;/a&gt;

, in which a Samsung S4 cellphone is being used to encode a live video stream from a specific brand of surveillance camera. Software encoding produces 8 fps, hardware encoding with the special hardware in the cellphone produces line rate at 25 fps.&lt;/p&gt;
&lt;p&gt;Unfortunately we do not get to see the changed energy profile, but purpose built video encoding and decoding hardware as is present on any average graphics hardware in 2019, down to the cellphone level, does things not only faster, but also with a fraction of the energy need. Such hardware is used for streaming video encoding in data centers, and for playback on end user devices.&lt;/p&gt;
&lt;p&gt;This is especially important on battery powered devices, where less energy usage translates into longer device runtime directly.&lt;/p&gt;
&lt;p&gt;For encoding, the approaches differ. Netflix is running on AWS, and used to use &lt;a href=&#34;https://medium.com/netflix-techblog/high-quality-video-encoding-at-scale-d159db052746&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;software encoding on regular EC2 instances&lt;/a&gt;

. A relatively recent Netflix talk about their &lt;a href=&#34;https://www.youtube.com/watch?v=JouA10QJiNc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;encoding pipeline&lt;/a&gt;

 has all the details.&lt;/p&gt;
&lt;p&gt;Google offers their own dedicated &lt;a href=&#34;https://cloud.google.com/solutions/media-entertainment/use-cases/video-encoding-transcoding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video encoding cloud service&lt;/a&gt;

, but does not document what they are using. It is likely that a hybrid of software and hardware encoding is being used: Google is known to have &lt;a href=&#34;https://www.quora.com/What-does-YouTube-use-for-encoding-video/answer/Ciro-Santilli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fuzzed ffmpeg&lt;/a&gt;

 a lot and to use it extensively. Also, ffmpeg can &lt;a href=&#34;https://www.tal.org/tutorials/ffmpeg_nvidia_encode&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;make use of GPU hardware for encoding&lt;/a&gt;

, when such hardware is available, and GCP has such hardware.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Running a cloud service uses energy, but it is way less energy than anybody else would use for the same job. Also, if you choose your cloud provider wisely, you may use energy, but won&amp;rsquo;t produce CO2&lt;/strong&gt; I have explained that in &lt;a href=&#34;https://blog.koehntopp.info/2019/10/05/data-centers-and-energy.html&#34;&gt;much more detail&lt;/a&gt;

 elsewhere.&lt;/p&gt;
&lt;p&gt;The TL;DR is: Cloud providers run data centers on renewable energy, Google is one of the worlds largest investors into solar and wind farms. So even if energy is being used, no CO2 is being produced to run the data center.&lt;/p&gt;
&lt;p&gt;Cloud providers use purpose built data centers that match their purpose built computers, &amp;ldquo;Open Compute Technology&amp;rdquo;. These machines use less than half the power of normal servers when idle, and around 33% less energy when fully loaded. They also run in data centers that do not require compression cooling, but can consume ambient air temperatures, greatly reducing the cooling energy spent.&lt;/p&gt;
&lt;p&gt;Cloud technology can size applications on demand, allowing way better utilisation of servers. While a typical enterprise data center has around 5%-10% utilisation, cloud data centers are 3x to 7x better utilised.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Netflix specifically is overcompensating. In
&lt;a href=&#34;https://media.netflix.com/en/company-blog/a-renewable-energy-update-from-us&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A renewable energy update from us&lt;/a&gt;


they explain how much energy they use, primary and secondary, and how they
compensate for this. This is assuming that their cloud provider, Amazon, was
running on 100% coal, which they aren&amp;rsquo;t, and that means they are actually
carbon negative.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; The BBC has an &lt;a href=&#34;https://www.bbc.co.uk/sounds/play/p0819sc4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;article&lt;/a&gt;

 on this, and they come to the same conclusions (via &lt;a href=&#34;https://twitter.com/elizab0t/status/1223570360555188224&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter&lt;/a&gt;

).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Centers and Energy</title>
      <link>https://blog.koehntopp.info/2019/10/05/data-centers-and-energy.html</link>
      <pubDate>Sat, 05 Oct 2019 11:44:55 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2019/10/05/data-centers-and-energy.html</guid>
      <description>&lt;p&gt;Deutsche Welle is shocked:
&lt;a href=&#34;https://www.dw.com/de/co2-aussto%C3%9F-von-online-video-streaming-als-klima-killer/a-49469109?maca=de-Twitter-sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generation Greta is watching Netflix&lt;/a&gt;


(Article in German Language), Netflix runs on computers, and apparently
computers are using power.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Second part &lt;a href=&#34;https://blog.koehntopp.info/2019/12/28/streaming-and-energy.html&#34;&gt;Streaming and Energy&lt;/a&gt;

 now available.&lt;/p&gt;
&lt;h2 id=&#34;end-user-device&#34;&gt;
    &lt;a href=&#34;#end-user-device&#34;&gt;
	End-User Device.
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s 2019. End user devices are using Wi-Fi, and are running on
Batteries. They are Cellphones, Tablets or Laptops.&lt;/p&gt;
&lt;p&gt;Devices that are not connected to grid are more usable if they
are trying to save energy, and so all modern devices are full of
special purpose hardware that allows them to fulfill their main
functions more energy efficient. In particular for the use-case
of watching video modern hardware, even cellphones, have special
&lt;a href=&#34;https://en.wikipedia.org/wiki/Application-specific_integrated_circuit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ASICs&lt;/a&gt;


that can do video and audio decompression and compression on the
fly faster and with much less energy usage than a CPU could do.&lt;/p&gt;
&lt;p&gt;Also, devices without a fan are limited to a sustained power
usage of 5W or less, because that is typically the heat that a
device can dissipate without a fan and without looking strange.&lt;/p&gt;
&lt;p&gt;All of this is an enormous improvement: An old-style desktop
computer has a TDP (Thermal Design Power) of 150W and actually
uses a large two digit number of Watts to run, and an old-school
17&amp;quot; monitor instead of a modern LCD uses as much energy. A
modern tablet or cellphone does both, loading an email and
displaying it, within a power budget of 1.5W to 5.0W, so easily
20-40x more energy efficient.&lt;/p&gt;
&lt;p&gt;The article mentions&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Eine geringere Video-Auflösung spart immer Daten und damit
Strom. Ein Smartphone etwa kann eine HD-Auflösung gar nicht
darstellen.&amp;rdquo; Außerdem gelte: je größer der Bildschirm, etwa
bei einem Smart-TV im Wohnzimmer, desto höher der
Stromverbrauch. Fazit: HD-Filme auf dem Smartphone über das
mobile Netz zu schauen, ist am stromintensivsten und damit am
klimaschädlichsten.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;»&amp;ldquo;A smaller resolution saves data and power. A smartphone for
example cannot show HD resolution&amp;rdquo;. Also, the larger the screen,
the more power use. In total: Watching HD movies on a smartphone
is the most energy intense and most climate endangering use.«&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s an amazing mix of truth and nonsense:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The smartphone will still use less than 5W, because it can&amp;rsquo;t
fry itself.&lt;/li&gt;
&lt;li&gt;And doing this at home via Wi-Fi will not use mobile
infrastructure but whatever access point is in the house, so
relatively low power (10W total for AP and phone?).&lt;/li&gt;
&lt;li&gt;The smartphone can indeed show Full HD, because modern smartphone
screens are in fact 1920x1280 or larger in resolution,&lt;/li&gt;
&lt;li&gt;but you won&amp;rsquo;t be able to notice that, because the pixels at 450dpi or
higher are too small for human eyes.&lt;/li&gt;
&lt;li&gt;And all this is so obvious that Netflix knows this and won&amp;rsquo;t
actually give you full HD for a smartphone endpoint. Unless of
course you are streaming from the smartphone to a TV, in which
case it does. You can prove this by downloading, and observing
that a full episode of an 1h show comes down to &amp;lt;250 MB of
total storage.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-center&#34;&gt;
    &lt;a href=&#34;#data-center&#34;&gt;
	Data Center
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Modern Cloud Data Centers are containing Megawatts and Megawatts
of compute capacity
(What&amp;rsquo;s a &lt;a href=&#34;https://www.google.com/search?q=1000000&amp;#43;Watt&amp;#43;in&amp;#43;Horse&amp;#43;Power&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Megawatt?&lt;/a&gt;

),
with a single facility now touching the
&lt;a href=&#34;https://lifelinedatacenters.com/data-center/gigamom-the-era-of-the-100-mw-data-center/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;100 MW barrier&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;Google specifically is using 100% renewable energy for their
data centers,
&lt;a href=&#34;https://sustainability.google/projects/announcement-100/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;since 2017&lt;/a&gt;

,
for a total of 2600 Megawatt. They are doing this by financing
solar and wind facilities directly, and not through greenwashing
certificates (&lt;a href=&#34;https://storage.googleapis.com/gweb-environment.appspot.com/pdf/renewable-energy.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;

).&lt;/p&gt;
&lt;p&gt;Azure claims to have reached CO2 neutrality in 2014, and also
talks about Power Usage Effectiveness (We&amp;rsquo;ll be touching that
subject in more depth below). In their
&lt;a href=&#34;https://azure.microsoft.com/nl-nl/global-infrastructure/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dutch page&lt;/a&gt;


they claim&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We hebben in 2014 CO2-neutraliteit bereikt en voldoen aan onze
doelstelling om een gemiddelde PUE (Power Usage Effectiveness)
van 1,125 voor elk nieuw datacenter te realiseren. Hiermee
zitten we 30 procent boven het industriegemiddelde.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&amp;ldquo;We have reached CO2 neutrality in 2014 and reached our targets
for power usage effectiveness of 1.125 for all new data centers.
With this, we are 30% better than industry average.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Amazons energy page is &lt;a href=&#34;https://aws.amazon.com/about-aws/sustainability/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AWS &amp;amp;
Sustainability&lt;/a&gt;


and they claim more than 50% power from renewables, and commit
to a 100% renewable goal for their global infrastructure.&lt;/p&gt;
&lt;p&gt;Like all the other cloud providers, they also highlight the way
lower PUE and much higher utilitzation of their servers compared
to on-premises IT (65% vs. 15% average utilization, and 29%
better power effectiveness, adding up to a 84% better overall
power bilancing in their math).&lt;/p&gt;
&lt;p&gt;They also list their ongoing solar and wind farm builds. For the
amount of power usage they can&amp;rsquo;t yet cover from direct renewable
sources, the page states &amp;lsquo;AWS purchases and retires
environmental attributes, like Renewable Energy Credits and
Guarantees of Origin, to cover the non-renewable energy we use
in these regions&amp;rsquo;, making them overall CO2 neutral.&lt;/p&gt;
&lt;p&gt;Finally, computer power usage is not linear: Power management on
a data center CPU turns cores on and off as needed, and thus, a
CPU uses already circa 50% power at 10% utilisation. From a cost
and from an environmental perspective it is best to utilise any
CPU to the fullest. Some older &lt;a href=&#34;https://blog.koehntopp.info/2017/07/19/threads-vs-watts.html&#34;&gt;measurements of mine&lt;/a&gt;

 on this.&lt;/p&gt;
&lt;h2 id=&#34;networking&#34;&gt;
    &lt;a href=&#34;#networking&#34;&gt;
	Networking
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The article from Deutsche Welle states correctly that the Last
Mile is what counts. That is, because everything before the Last
Mile is already fiber, and fiber lines allow extremely fast
networking at relatively low energy.&lt;/p&gt;
&lt;p&gt;Germany, specifically, is wasting a lot of energy on the last
mile, because in order to make Germanys aging copper
infrastructure capable of handling modern data rates, a stunning
amount of signal processing is required.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/data-centers-and-energy/outdoor_dslam.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Siemens DSL DSLAM (not VDSL) as can be found in typical outdoor
cabinets all over the city. VDSL requires even more compute to
send Megabits/s over what is hardly better than a wet cow wire
(Image via
&lt;a href=&#34;https://en.wikipedia.org/wiki/Digital_subscriber_line_access_multiplexer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;

)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nokia.com/blog/vdsl2-and-gpon-study-finds-sweet-spots/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nokia&lt;/a&gt;


lists Fiber as 45% more cost efficient and way more energy
efficient than VDSL2:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Copper infrastructure maintenance is typically the most costly
of all network types. By comparison, GPON is significantly
cheaper — upwards of 45% less to operate than VDSL2.&lt;/p&gt;
&lt;p&gt;That’s why replacing aging copper plant with fiber has
significant operational benefits. New fiber is more reliable
than old copper while, at the same time, consumes much less
energy. Also, with GPON operational costs are further reduced
with the elimination of the digital subscriber line access
multiplexer (DSLAM) from the access architecture.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.seeclearfield.com/assets/documents/data-sheets/clfd-odc-100-outdoor-cabinet.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Clearfield&lt;/a&gt;


specifices the total installable power in their typical grey
&amp;lsquo;outdoor VDSL cabinets&amp;rsquo; as up to 3000W, but most cabinets will
not be fully powered to the max. German Telekom
&lt;a href=&#34;https://en.wikipedia.org/wiki/File:Outdoor_DSLAM.JPG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSLAMs&lt;/a&gt;


are being fed 48V/25A, around 1000W for the street level devices
with four cards.&lt;/p&gt;
&lt;p&gt;Moving this to fiber will dramatically save energy, and in fact,
everywhere in the world outside of Germany this is happening
right now.&lt;/p&gt;
&lt;p&gt;Mobile data networks are indeed
&lt;a href=&#34;https://www.lightreading.com/mobile/5g/power-consumption-5g-basestations-are-hungry-hungry-hippos/d/d-id/749979&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great power sinks&lt;/a&gt;

.
If you can use cabled networking, do so.&lt;/p&gt;
&lt;h2 id=&#34;modern-data-centers-and-power&#34;&gt;
    &lt;a href=&#34;#modern-data-centers-and-power&#34;&gt;
	Modern Data Centers and Power
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/rumperedis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Laban&lt;/a&gt;

 is an ambassador
for the &lt;a href=&#34;https://www.opencompute.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Compute Project&lt;/a&gt;

.
Funded originally by Facebook, OCP is a project that tries to
build more power efficient cloud data centers. In his
&lt;a href=&#34;https://www.slideshare.net/JohnLaban/ocp-copenhagen-presentation-sept-2017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;presentations&lt;/a&gt;

,
John explains how OCP achieves this by getting rid of old
technology in the data center and making the room and the rack a
system:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/data-centers-and-energy/traditional-data-center.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Traditional Data Centers: Dark Green - Payload. Everything else - Payload Support.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/data-centers-and-energy/ocp-data-center.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;OCP Data Centers: Dark Green - Payload. Still around: Generator, Airco is now adiabatic instead of CRAC. Everything else: gone.&lt;/p&gt;
&lt;p&gt;OCP data centers redesign the air flow in the data center.&lt;/p&gt;
&lt;p&gt;Traditionally data centers have a raised floor, which transports
cold air to the racks, goes through the racks and is then cooled
again. Air-in temperatures are often as low as 17-25C, and
humidity is tightly controlled. OCP data centers allow
non-condensing air of up to 35-45C to go into the servers, which
does basically away with heat pumps and air condition and allows
ambient temperature air to go into the DC.&lt;/p&gt;
&lt;p&gt;Traditional servers have often a relatively low height (&amp;ldquo;1U&amp;rdquo;,
one rack unit), which forces the designers to use relatively
small fans, small heat sinks (== higher air flow speed necessary
for cooling) and produces a lot of friction for the air in the
devices. In a 350W server, the worst case I have personally
observed is 50W for air movement.&lt;/p&gt;
&lt;p&gt;OCP uses higher rack units (&amp;ldquo;OU&amp;rdquo;, Open Compute Units) and
usually makes devices no flatter than 2 OU. Instead, horizontal
partitions are used (3 devices in 2 OU) for a more square device
front, minimising friction and allowing larger heat sinks. It
also allows larger fans, with in turn allows moving the same
volume of air with less RPM and a lot less power. Best cases I
have personally observed were as low as 5-10W for a 350W server.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/data-centers-and-energy/ocp-server.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;OCP Server: 2 OU high, 1/3 Rack wide, less friction, larger
coolers, no panels impeding airflow, larger fans = slower RPM =
less energy for air movement. Can take up to 45C air-in
(&amp;lsquo;ambient air intake&amp;rsquo;, no heat pump required for cooling).&lt;/p&gt;
&lt;p&gt;OCP also improves power consumption by centralising power
supplies in a rack (larger power supplies are usually more
efficient), reducing the number of power conversions in the
total power flow, and making uninterruptible power supplies more
efficient.&lt;/p&gt;
&lt;p&gt;Total OCP power savings vary by use-case and climate zone, but
are usually 20-50%. South Korean Telekom tested OCP servers in a
climate chamber with various settings
(&lt;a href=&#34;https://www.youtube.com/watch?v=BBcFXAXXqRE#t=11m33s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Testbed&lt;/a&gt;

,
&lt;a href=&#34;https://www.youtube.com/watch?v=BBcFXAXXqRE#t=14m30s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Findings&lt;/a&gt;

)
and what really shows is how the OCP power consumption stays the
same at all possible air-in temperatures while legacy equipment
consumes a lot more power because it has to turn up the fans.&lt;/p&gt;
&lt;h2 id=&#34;pue-power-usage-efficiency&#34;&gt;
    &lt;a href=&#34;#pue-power-usage-efficiency&#34;&gt;
	PUE, Power Usage Efficiency
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The Power Usage Efficiency, PUE, is the ratio of total power
consumption of a data center compared to power intake of the
compute equipment. The overhead typically contains cooling, air
movement, and power transformation losses.&lt;/p&gt;
&lt;p&gt;The data center built by web.de in Amalienbadstraße, Durlach
(Karlsruhe), had a PUE of 2.0 - for each Megawatt used for
compute, another Megawatt was required for cooling and air
movement. The reasons for that are manifold: The building
itself, an old factory building built by Pfaff, had low ceilings
forcing warm air back into the machines and forcing bad
airflows. The hardware used had a lot of loss due to badly
designed airflows. And the location of the data center, down in
the valley of the river rhine instead of high up in the Black
Forest, combined high ambient air temperatures with high air
humidity, making evaporative or adiabatic cooling an
inefficient option - compressive cooling with heat pumps
was necessary.&lt;/p&gt;
&lt;p&gt;More modern data traditional centers can achieve a design PUE as
low as 1.2 and an effective operational PUE of 1.6. Rigidly
optimized OCP data centers can go below 1.1 effective PUE (sic!)
in good conditions. Google published their numbers in their
&lt;a href=&#34;https://www.google.com/about/datacenters/efficiency/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PUE dashboard&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;All current cloud vendors deploy OCP equipment or (in the case
of Microsoft) self-developed equipment that is
comparable in efficiency. They also add additional special
purpose hardware to their cloud offerings that allows offloading
of common functionality from the sellable cores of their
machines, in order to maximise sellable inventory, save power
and, of course, increase utilisation.&lt;/p&gt;
&lt;p&gt;All aspects of improved efficiency and utilisation included,
cloud hardware often is 3x (or more) efficient than a traditional data center.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; A longer article by &lt;a href=&#34;https://twitter.com/jessfraz/status/1232847716964716544&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jessie Frazelle&lt;/a&gt;

 on Data Center Energy use: &lt;a href=&#34;https://blog.jessfraz.com/post/power-to-the-people/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Power to the People&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; A study by Masanet, Koomey et al, &lt;a href=&#34;https://science.sciencemag.org/content/367/6481/984&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recalibrating global data center energy-use estimates&lt;/a&gt;

. The &lt;a href=&#34;https://twitter.com/jgkoomey/status/1233113568934907904&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TL;DR is&lt;/a&gt;

: »We find that computing output from data centers went up six fold from 2010 to 2018 but electricity use only went up 6%.« Masanet, Koomey et al have a second article, &lt;a href=&#34;https://www.nature.com/articles/nclimate1786&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Characteristics of low-carbon data centres&lt;/a&gt;

&lt;a href=&#34;https://sci-hub.se/https://www.nature.com/articles/nclimate1786&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;.&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; I have been sent a &lt;a href=&#34;https://www.se.com/ww/en/work/campaign/data-center-design-overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link to a writeup on data center planning considerations&lt;/a&gt;

. While this was part of a SEO effort, the source (Schneider Electric) is competent - Scheider specializes in Data Center Design and Planning and Energy Systems - and the writeup is actually useful. The text gives a bit of insight into the process and the factors that go into site planning and choices. A lot depends on intended purpose, local climate, available space and power and ultimately also site size and building increments.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;
    &lt;a href=&#34;#summary&#34;&gt;
	Summary
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Compared to technology from 10 years ago, in the data center we
are now using a lot less energy and with cloud technology also
have dramatically improved utilisation (ie use less hardware to
achieve the same outcome). Also,
all of this is green energy, mostly from actual green production
and not grey-green with certificates. Cloud operators are some
of the largest investors in Wind and Solar all over the world.&lt;/p&gt;
&lt;p&gt;At home, savings are even larger, because we went from Desktop
machines with abysmal power profiles to modern low power
hardware which uses single digit wattages to produce results.&lt;/p&gt;
&lt;p&gt;Networking, especially the last mile, especially mobile
networking, especially 5G, is a power hog. The amount of signal
processing that goes into 5G and VDSL is amazing. If you want
low power networking, use fiber, maybe bridge the last 5 meters
with Wi-Fi.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What has Kubernetes ever done for us?</title>
      <link>https://blog.koehntopp.info/2019/04/25/what-has-kubernetes-ever-done-for-us.html</link>
      <pubDate>Thu, 25 Apr 2019 13:28:48 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2019/04/25/what-has-kubernetes-ever-done-for-us.html</guid>
      <description>&lt;h1 id=&#34;kubernetes-and-complexity&#34;&gt;
    &lt;a href=&#34;#kubernetes-and-complexity&#34;&gt;
	Kubernetes and Complexity
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;On Twitter, &lt;a href=&#34;https://twitter.com/crcsmnky/status/1120189000474681347&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sandeep&lt;/a&gt;

 remarked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The complexity and amount of code involved is staggering.
I&amp;rsquo;m not sure how we can possibly justify the Kubernetes, Istio, Docker, Envoy, Prometheus, Grafana, Jaeger, Kiali, Helm stack.&lt;/p&gt;
&lt;p&gt;What problem are we solving that were not solving 15 years ago, again?
How much time and effort are being saved (by organisations smaller than Google)?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;One reply was:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The amount of code is staggering.
I&amp;rsquo;m not sure how we can justify the Linux, dpkg, HAProxy, Nagios, Check_MK, puppet, supervisor, apt stack.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Unlike other systems that try to solve similar problems, Kubernetes - at least in the past - has been driven by users that had a problem to solve, and not by gaggle of 400 vendors which think that it improves the flavour when they are pissing into a repo.
Unlike these other systems, Kubernetes actually works and scales, with no added or proprietary components, if you take the public repo, build and deploy it.
So that is still a lot of complexity, but at least it accomplishes something.&lt;/p&gt;
&lt;h1 id=&#34;what-problem-are-we-solving-that-we-were-not-solving-2004&#34;&gt;
    &lt;a href=&#34;#what-problem-are-we-solving-that-we-were-not-solving-2004&#34;&gt;
	What problem are we solving that we were not solving 2004?
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;In 2004, we were running bare metal with maybe puppet, but more likely cfengine or even manual ssh execution running shell scripts out of a NFS file-share.
And were monitoring with Nagios.&lt;/p&gt;
&lt;p&gt;The progress is obvious, but let us list it:&lt;/p&gt;
&lt;p&gt;Today we are living in an age of abundance in the data center.
In our data centers we have machines with at least 32 threads per machine, many have much higher number.
Three digit GB of memory are standard.
IOPS are not a problem with SSD.
And multi-tiered leaf-and-spine L3 networks create fabrics that can connect any CPU in any machine to any disk in any other machine in the same data center, at nominal media speed and with minuscule latency added from the mesh, as long as we can do cut-through switching.&lt;/p&gt;
&lt;p&gt;That is, if we planned our data center right, we are having &lt;a href=&#34;https://ai.google/research/pubs/pub41606&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Data Center as a Computer&lt;/a&gt;

.
A uniform collection of threads with memory, a network as a connecting mesh and sufficient storage in capacity and IOPS somewhere within the same mesh.&lt;/p&gt;
&lt;p&gt;From this hardware substrate we can build clusters of Kubernetes, K8s.
The K8s is basically &amp;ldquo;init for a network of machines&amp;rdquo;, a system that accepts declarative instructions of how to mesh a set of images of software into a network and a service.
It then schedules resources from our substrate, and pushes the images onto that.&lt;/p&gt;
&lt;p&gt;Because the images are self-contained and immutable, once they complete running, we can remove them from their respective machines and receive the original hosts back in original state.
Because the images are self-contained and immutable, they can be scheduled on any node that has sufficient free capacity, specifics no longer matter.
State is isolated and specially managed, K8s provides concepts for that, stateful sets and operators.&lt;/p&gt;
&lt;p&gt;This allows hardware operators to provide capacity without caring much about the users, the devs and their apps running on the clusters.
Conversely, it allows devs and apps to work with hardware using an API instead of people and tickets.
Hardware and Data Center operations become a completely autonomous commodity.&lt;/p&gt;
&lt;p&gt;Which we call &amp;ldquo;The Cloud&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;For devs and their apps, Istio and Envoy provide an interface to interconnect images to allow them to communicate.
These systems also provide routing, balancing, authentication, enforce TLS, provide backoff and fault tolerance and collect metrics in a standardised way.
And in a way that is invisible and inescapable to the people that write the application.&lt;/p&gt;
&lt;p&gt;All these properties of the communication become an &lt;a href=&#34;https://en.wikipedia.org/wiki/Aspect_%28computer_programming%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aspect&lt;/a&gt;

, taken out of the codebase, and the scope of the application programmer.
They just are, whether the developer cares or knows about these things or not.&lt;/p&gt;
&lt;p&gt;Similarly, Prometheus and the surrounding ideology have a story about what to collect (ie &lt;a href=&#34;https://www.vividcortex.com/blog/monitoring-and-observability-with-use-and-red&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;USE and RED&lt;/a&gt;

, and follow @mipsytipsy on Twitter for a lot of insight).
We are getting best practice around this, and it is being wrapped into an Aspect again.&lt;/p&gt;
&lt;p&gt;Look back 15 years - systems like Nagios and Graphite are extremely host centric.
This does no longer work in the new operational model.
We do no longer care about hosts at all, we care about capacity (let&amp;rsquo;s have more than we strictly need), and about churn (we have capacity, but there is a high death-and-reprovisioning rate and that is suspect, so we alert).
Nagios-era systems are not equipped to deal with environments like this at all.&lt;/p&gt;
&lt;p&gt;Tracing and Debugging are evolving and adapting in a similar way.
Jaeger, sysdig and similar systems help you to debug problems in collections of microservices that are running in multiple instances across an arbitrary number of hosts somewhere in a much larger cluster of machines.
They give you capability to follow requests up and down the stack where components are not bound to specific hosts.
And they give you capability to view the flow of a request through services from a logical point of view without actually accessing individual physical hosts (or caring about these hosts at all.&lt;/p&gt;
&lt;p&gt;There is any number of benefits from this change, this decoupling of deployments from specific instances of compute:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Immutability makes it possible to reason about production and production state.&lt;/li&gt;
&lt;li&gt;Workloads can migrate (actually respawn) within a cluster as the need and the load changes.&lt;/li&gt;
&lt;li&gt;Deployments become a swift automated operational procedure.&lt;/li&gt;
&lt;li&gt;Interfaces to standard Aspects of operations (deployment, upgrade, monitoring, interconnection, cluster membership, shared state management) become standardised and become interchangeable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We get codified (literally, as code) best practice.&lt;/p&gt;
&lt;p&gt;So what problem are we solving that we were not solving 15 years ago?
Well, feel free to place my rant as a soundtrack on top of &lt;a href=&#34;https://www.youtube.com/watch?v=uvPbj9NX0zc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Monty Python&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;Yes, there is complexity.
You need to learn Go, JS and Python as a solid foundation to be able to play.
You need to understand the distributed computing and security basics necessary in order to play in this arena.
And that is quite a large piece.
On the other hand, &amp;ldquo;The amount of code is staggering.
I&amp;rsquo;m not sure how we can justify the Linux, dpkg, HAProxy, Nagios, Check_MK, puppet, supervisor, apt stack.&amp;quot;« as written elsewhere in this thread.&lt;/p&gt;
&lt;p&gt;From the K8s setup, you get a much higher average architectural quality by comparison.
That is in no small part because of the way K8s is opinionated about how to handle and deliver things.
It is also in the delivery, in the way of Aspects, wrappers around your stuff that provide functionality for operations, in a way that you do not even see when coding your stuff.
You are getting sane defaults in a way that is mostly agnostic to what you care about.&lt;/p&gt;
&lt;p&gt;This is an important decoupling in multiple dimensions, making everybody&amp;rsquo;s job easier and enabling them to deliver more and better quality with less work.&lt;/p&gt;
&lt;p&gt;Yes, this is relatively new, but you can see how good and well-thought-out it is.
There is a reason the K8s ecosystem owned everybody else in less than 3 years time, and this is why.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Service Mesh</title>
      <link>https://blog.koehntopp.info/2018/11/28/service-mesh.html</link>
      <pubDate>Wed, 28 Nov 2018 20:57:34 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2018/11/28/service-mesh.html</guid>
      <description>&lt;p&gt;Wenn man was mit Infrastructure As Code macht, also Openstack, AWS, GCS, oder auch Kubernetes, dann hat man meistens eine ganze Flotte von willkürlichen IPs mit komischen Ports, auf denen je eine Instanz desselben Dienstes läuft.
Das heißt, man braucht einen Load-Balancer, der einem die Requests annimmt und auf die ganzen Endpoints verteilt.&lt;/p&gt;
&lt;p&gt;Das ist eine Idee, die man mal einen Moment festhalten sollte, während man einen Blick auf Altbekanntes wirft:&lt;/p&gt;
&lt;h1 id=&#34;sdn-in-openstack&#34;&gt;
    &lt;a href=&#34;#sdn-in-openstack&#34;&gt;
	SDN in Openstack
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Denn Openstack zum Beispiel treibt auf den unteren Ebenen des Netzwerks eine Menge Aufwand, um ein Underlay und ein Overlay-Netz zu bauen.
Und einem Tenant (also einem Openstack-Kunden, mit Benutzern im Kunden) dann die Gelegenheit zu geben, in seinem Tenant eine komplette IP-Infrastruktur aufzubauen, mit virtuellen Bridges, Routern, Subnetzen, Firewalls und Load Balancern.
Das tut der Kunde, weil er IP basierend Access Control (also Firewalls) aufbauen will, die den Zugang zu den Diensten regeln.&lt;/p&gt;
&lt;p&gt;Dienste, die sowieso nicht zugänglich sind, weil sie eigentlich nur durch den Load-Balancer erreicht werden können, und deren IP-Nummern auch egal sind, weil sie nur den LB interessieren.&lt;/p&gt;
&lt;p&gt;Ich weise da deshalb so darauf hin, weil Software Defined Networking zwar eine atemberaubend geile Sache ist.
Man kann damit endlich alle Netzwerk-Zuckerbäckereien bauen kann, die man schon immer haben wollte, ohne jemals ein Kabel oder ein Chassis in die Hand zu nehmen.
Aber SDN ist auch etwas, das genau gar nicht skaliert und für das man keine Operations machen kann.&lt;/p&gt;
&lt;p&gt;Also, ein SDN hat einen Haufen State.
Es weiß, welche Komponenten des Overlays von welchem Tenant wo physikalisch terminiert werden (&amp;ldquo;welche VM wo liegt und welche virtuellen Komponenten des Overlays vor der VM liegen&amp;rdquo;).
Und es weiß, was ein virtueller Router, eine virtuelle Firewall oder ein virtuelles NAT mit einem Paket machen müssen, das da durch geht und simuliert das dann, bevor das Paket ausgeliefert wird.&lt;/p&gt;
&lt;p&gt;State ist aber ein Problem:
Jedes Mal, wenn eine VM hoch geht oder terminiert wird.
Schlimmer noch, jedes Mal, wenn eine aktive Netzwerkkomponente oder gar ein ganzes Netz erzeugt oder terminiert wird.
Immer dann ändert sich die Topologie des Netzes und damit der State, der dem ganzen SDN und allen Komponenten bekannt sein muß.
Auch die simulierten Aktionen des Overlays ändern sich, und müssen neu berechnet werden.
Und ein Haufen verteilter Komponenten müssen aktualisiert werden.&lt;/p&gt;
&lt;p&gt;Wenn also der Cluster größer wird, also mehr Underlay bekommt, dann wird der Kreis der Geräte größer, denen der State bekannt sein muß.
Damit geht die Convergence Time nach oben, also die Zeit, die es braucht, bis eine Änderung allen Geräten im Overlay und Underlay bekannt ist.&lt;/p&gt;
&lt;p&gt;Und zugleich sind vermutlich auch mehr Tenants da, die mehr machen müssen.
Damit geht dann auch die Change Rate nach oben.
Also die Anzahl der Änderungen im Cluster, die eine neue Konvergenz des Clusters erforderlich machen können.&lt;/p&gt;
&lt;p&gt;Und irgendwann brennt dann alles nieder.
Dann sind alle Overlays down, und alle Kunden sind sehr traurig.&lt;/p&gt;
&lt;p&gt;Dazu kommt dann natürlich noch, das so Hersteller von SDN Software gelegentlich neue Versionen von ihrem Kram releasen, die man dann einspielen muß.&lt;/p&gt;
&lt;p&gt;Der Hersteller eines SDN ist natürlich sehr verantwortungsvoll und kennt sich mit den Tücken von Operations Teams aus:
neue Versionen sind immer kompatibel zu vorhergehenden Versionen und man kann von einer alten Version auf eine neue Version so upgraden, daß auch ein Rollback auf die alte Version jederzeit möglich ist.
Und zwar ohne dass die persistierten Strukturen, die das Overlay beschreiben, verloren gehen oder ungültig werden.
Dadurch sind Updates eine sichere Sache und risikolos ohne Downtime möglich.&lt;/p&gt;
&lt;p&gt;Außerdem ist die Erde eine Scheibe.&lt;/p&gt;
&lt;h1 id=&#34;stattdessen-service-mesh&#34;&gt;
    &lt;a href=&#34;#stattdessen-service-mesh&#34;&gt;
	Stattdessen Service Mesh
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Weil das so ist, wie es ist, haben sich einige Leute überlegt:
wieso überhaupt ein Overlay?
Am Ende sind doch IP-Nummern egal, und die Discovery der Endpunkte ist komplett nur ein Problem für den Load-Balancer.&lt;/p&gt;
&lt;p&gt;Warum packt man nicht einfach alle Tenants in ein flaches IP-Netz, nimmt den Kunden die Möglichkeit weg, IP-Nummern und Netzwerktopologien zu bauen und zwingt sie dazu, Services sicher im &amp;ldquo;offenen Netz&amp;rdquo; zu betreiben?&lt;/p&gt;
&lt;p&gt;Tatsächlich ist das eine berechtigte Frage und eine Antwort ist:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Weil man dann immer noch einen technisch einheitlichen und zuverlässigen Weg braucht, um Verbindungen und Partner zu identifizieren, und nur zulässige Verbindungen zu erlauben.
Am Besten auf eine Weise, daß Entwickler das nicht willkürlich falsch machen können&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Im Kubernetes-Umfeld ist so etwas ein Sidecar-Proxy.&lt;/p&gt;
&lt;p&gt;Ein Sidecar ist ein Prozeß, der mit dem eigentlichen Payload-Prozeß zusammen gestartet wird, und der im selben Pod läuft wie die Payload.
Das heißt, die Dinger können einander sehen und befummeln, verwenden dieselben Netzwerkinterfaces und IP-Tables Regeln und teilen auch sonst alle Ressourcen - mit Ausnahme des Dateisystem-Images.&lt;/p&gt;
&lt;p&gt;Die Payload verbindet sich also an localhost, Port irgendwas, und der Sidecar-Proxy ist zuverlässig auf diesem Port und kümmert sich um die Verbindungen, oder andersherum, der Proxy nimmt eingehende Verbindungen an und leitet sie an die Payload weiter.&lt;/p&gt;
&lt;p&gt;Dadurch kann der Proxy eine ganze Menge Dinge tun - er kann TLS erzwingen und Zertifikate oder JWT Token kontrollieren, er kann Verbindungsdaten sammeln (Timing Histogramme), er kann Retries kontrollieren, wenn ein Dienst nicht in einer bestimmten Zeit reagiert oder er kann Retries und Services komplett kurzschließen, wenn ein optionaler Dienst mal weg ist.&lt;/p&gt;
&lt;p&gt;Ein solcher Proxy für das Kubernetes Umfeld ist Envoy, und das besondere an Envoy ist, daß seine Konfiguration nicht aus statischen Dateien kommt, sondern auch von einem clusterweit-zentralen Dienst dynamisch generiert werden kann.
Die Konfiguration kann dann ohne Proxy Neustart im Betrieb geändert werden.&lt;/p&gt;
&lt;p&gt;Kubernetes verwendet das, um neue Endpunkte in den Load-Balancer zu integrieren, wenn sie verfügbar werden, um in Rollouts Traffic sanft von einer alten auf eine neue Version zu migrieren und um zentral clusterweit einheitlich Metriken über Dienstaufrufe und deren Zeitverhalten zu sammeln und zu loggen.
Das kann man verwenden, um Request-Kaskaden zu tracen und um Qualitätsüberwachung für seine Dienste zu haben.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jvns.ca/blog/2018/10/27/envoy-basics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ein paar Grundlagen über Envoy&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;Es stellt sich dann heraus, daß aus einer Reihe von Gründen das besser skaliert und schöner failed als ein SDN, und daß man außerdem eine Technologie verwendet, die von Entwicklern besser verstanden wird und einfacher zu debuggen ist als ein SDN.&lt;/p&gt;
&lt;p&gt;Es skaliert besser, weil hier State pro Dienst und nicht pro Cluster gehalten werden könnte.
Und es failed schöner, weil man hier einzelne Envoys unabhängig voneinander verlieren kann, bzw. einzelne Dienste ohne gleich den ganzen Cluster zu droppen.
Verliert man die Control Plane, also den Dienst, der Envoy steuert, dann ist das loss-of-control, man kann die Envoy Config nicht mehr ändern.
Es ist nicht loss-of-service, das heißt, das Netz läuft eingefroren weiter - und die Control Plane kann (hoffentlich) rediscovery, d.h. eine neu gestartete Control Plane kann die existierenden Envoy-Nodes und deren Config ohne Betriebsunterbrechung einsammeln, konsolidieren und dann nahtlos den Betrieb aufnehmen.&lt;/p&gt;
&lt;p&gt;Ich rede hier von Control Planes und eventuell später auch von Service Discovery und anderen Dingen.
Hier ist noch einmal ein wenig Hintergrund dazu:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Modern Network Load Balancing&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;Der Artikel erklärt erst mal, was ein Load-Balancer ist, was L4 und L7 Load-Balancing tut und hebt dann insbesondere auf L7 LBs (Proxies) ab, und geht auf zentrale Steuerung mit dynamisch generiert Konfiguration ein.
Diese zentrale Steuerung mit dynamischer Config-Generierung ist eine Controlplane.
Generell finden wir in dem Artikel eine schöne Erklärung von Begriffen, und was sie bedeuten und warum man diese Konzepte abgedeckt haben will - Service Discovery ,Health Checking, Load-Balacing, Stickyness von Sessions, TLS Termination, Observability, Security and DoS Mitigation, und schließlich Config and Control Plane.&lt;/p&gt;
&lt;p&gt;Wenn wir jedoch über Kubernetes reden, dann reden wir in der Regel über L7 Proxies, und wir finden nginx plus, haproxy, linkerd und Envoy.
Envoy wischt gerade mit dem Rest den Boden auf und der Grund dafür ist die Controlplane - für Envoy gibt es mehrere, und die, die alle haben wollen, ist Istio.&lt;/p&gt;
&lt;p&gt;Istio macht aus einer Flotte von Payloads mit Envoy Sidecars ein Service Mesh.
Es generiert aus der State Database von Kubernetes eine Konfiguration für alle diese Envoys, so daß die Payloads auf die richtige Weise miteinander reden dürfen, das immer über TLS tun, mit anderen Dingen nicht reden können und einander finden und all die anderen schönen Dinge tun, die Envoy für jemanden tun kann, wenn man ihn den nur korrekt konfigurierte.&lt;/p&gt;
&lt;p&gt;Der Artikel
&lt;a href=&#34;https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Service Mesh vs. Control Plane&lt;/a&gt;


erklärt das in mehr Detail und Tiefe.&lt;/p&gt;
&lt;p&gt;Und weil das alles so toll und aufregend ist, haben Google, das Kubernetes Projekt, die Cloud Native Computing Foundation (CNCF) und ein paar andere Leute gesagt
&amp;ldquo;Dann integrieren wir das alles eben&amp;rdquo;,
und so kommt Kubernetes jetzt mit Istio und Envoy als Service Mesh und flache L3 Netze mit Sidecar L7 Loadbalancers werden der Default - mindestens in GKE, aber vermutlich sowieso bei jedem, der Kubernetes macht.&lt;/p&gt;
&lt;p&gt;Eric Brewer schreibt darüber in aller Ausführlichkeit in
&lt;a href=&#34;https://cloud.google.com/blog/products/application-development/kubernetes-users-get-ready-for-the-next-chapter-in-microservices-management&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes users, get ready for the next chapter in microservices management&lt;/a&gt;


Damit werden Istio und Envoy das SDN an Stelle des SDN in normalen K8s Installationen - nicht nur bei Google.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Verschlüsselt Dropbox überhaupt?</title>
      <link>https://blog.koehntopp.info/2011/04/26/verschluesselt-dropbox-ueberhaupt.html</link>
      <pubDate>Tue, 26 Apr 2011 17:32:36 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2011/04/26/verschluesselt-dropbox-ueberhaupt.html</guid>
      <description>&lt;p&gt;Dropbox ist in den letzten Tagen und Wochen ein wenig seltsam in die
Schlagzeilen geraten.&lt;/p&gt;
&lt;p&gt;Da ist einmal der Artikel von Derek Newton:
&lt;a href=&#34;http://dereknewton.com/2011/04/dropbox-authentication-static-host-ids/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dropbox Authentication: Insecure by design&lt;/a&gt;

:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After some testing (modification of data within the config table, etc) it
became clear that the Dropbox client uses only the host_id to
authenticate. Here’s the problem: the config.db file is completely
portable and is &lt;em&gt;not&lt;/em&gt; tied to the system in any way. This means that if
you gain access to a person’s config.db file (or just the host_id), you
gain complete access to the person’s Dropbox until such time that the
person removes the host from the list of linked devices via the Dropbox
web interface.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Dann ist da die Änderung der TOS, die vorher ein wenig irreführend
formuliert waren: Vorher konnte man die TOS so lesen, daß die
Dropbox-Dateien auf den Dropbox-Servern verschlüsselt gespeichert werden und
Dropbox selber keine Kopien dieser Schüssel hatte. Das ist natürlich nicht
der Fall: Für den Betreiber ist das alles Klartext, wenn er denn nur will.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://blog.dropbox.com/?p=735&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Und er will&lt;/a&gt;

:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We may disclose to parties outside Dropbox files stored in your Dropbox
and information about you that we collect when we have a good faith belief
that disclosure is reasonably necessary to (a) comply with a law,
regulation or compulsory legal request; (b) protect the safety of any
person from death or serious bodily injury; (c) prevent fraud or abuse of
Dropbox or its users; or (d) to protect Dropbox’s property rights. If we
provide your Dropbox files to a law enforcement agency as set forth above,
we will remove Dropbox’s encryption from the files before providing them
to law enforcement. However, Dropbox will not be able to decrypt any files
that you encrypted prior to storing them on Dropbox.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Und schließlich nun:
&lt;a href=&#34;http://www.techdirt.com/articles/20110425/15541514030/dropbox-tries-to-kill-off-open-source-project-with-dmca-takedown.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dropbox Tries To Kill Off Open Source Project With DMCA Takedown&lt;/a&gt;

.
Diese Geschichte ist recht interessant, denn sie beleuchtet eine
Inkonsistenz in den Behauptungen von Dropbox - und hat mit der
Verschüsselung aus dem vorhergehenden Fall zu tun.&lt;/p&gt;
&lt;p&gt;Dropbox speichert Dateien. Die meisten Leute haben Dateien, die auch andere
Leute haben. Das ist schon deswegen so, weil ein guter Teil von Dropbox sich
ja um Kollaboration, also das Teilen von Dateien miteinander dreht. Das
können Unterlagen einer Wohnungseigentümergemeinschaft betreffend eine
Dachsanierung sein, oder halt ein paar MP3-Dateien.&lt;/p&gt;
&lt;p&gt;Wenn man nun also eine Datei auf Dropbox hoch lädt, dann berechnet der
Client eine Prüfsumme und lädt erst einmal diese hoch.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/sascha_lobo_singt.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Abkürzung beim Datei-Upload, wenn die Datei schon bei einem anderen
Dropbox-Kunden liegt.&lt;/p&gt;
&lt;p&gt;Existiert eine Datei mit dieser Prüfsumme, dem passenden Datum und Namen
schon auf den Servern von Dropbox, dann muß Dropbox die Datei selber gar
nicht mehr bekommen, denn Dropbox hat ja bereits passende Daten. Es genügt
also serverseitig, die Datei auf dem Dropbox-Server für diesen Kunden
sichtbar zu machen. Das ist schnell, spart Bandbreite und für Dropbox spart
es Speicherplatz. Genau genommen macht es Dropbox wahrscheinlich überhaupt
erst rentabel, denn andernfalls würde viel zu viel Plattenplatz verbraucht
werden.&lt;/p&gt;
&lt;p&gt;Die Werkzeuge von Drittanbietern, gegen die Dropbox dort vorgeht, tun nun
folgendes: Sie &amp;lsquo;wissen&amp;rsquo; Prüfsummen und senden diese Prüfsummen an die
Dropbox-Server, ohne daß der Anwender diese Dateien auf seiner lokalen
Platte hat - der Client behauptet also, die Daten zu haben und zeigt eine
Prüfsumme vor, hat die Daten in Wahrheit aber gar nicht.&lt;/p&gt;
&lt;p&gt;Dropbox macht diese Dateien dann für den Benutzer dieses Tools sichtbar,
wenn Dropbox diese Datei überhaupt in irgendeinem Useraccount hat. Man kann
also eine Datei-Prüfsumme etwa eines MP3-Verzeichnisses twittern, und jeder
kann mit diesem Tool und der Prüfsumme, die er über Twitter erhalten hat,
dann diese MP3s in seine Dropbox importieren und sie sich dann runterladen.
Klar, daß Dropbox das doof findet.&lt;/p&gt;
&lt;p&gt;Das wäre technisch relativ leicht korrigierbar: Dropbox kennt ja die Datei,
und ihre Länge. Dropbox kann also den Client nach einem Sample der Daten
oder einer zweiten Prüfsumme über ein Sample der Daten fragen, und dabei das
Sample aus der ganzen Datei über einen variablen, zufällig bestimmten
Bereich legen. Wenn ich also behaupte, das Album &amp;ldquo;Sascha Lobo singt&amp;rdquo; zu
haben und die Prüfsumme dafür vorzeige, dann gibt Dropbox mir die Dateien
erst, wenn ich die Prüfsumme über die Bytes von Offset 4826267 bis
4826267+4096  dieser Datei berechnet habe. Wenn Du das machst, wird
stattdessen die Prüfsumme über einen anderen Bereich verlangt. Auf diese
Weise kann Dropbox leicht prüfen, ob der Client wirklich im Besitz der Daten
ist, die er behauptet zu haben.&lt;/p&gt;
&lt;p&gt;Nun sollte einmal das Denken einsetzen. Ich erzeuge also eine Datei mit den
Inhalt &amp;ldquo;Hallo, Welt!&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Wenn &lt;strong&gt;ich&lt;/strong&gt; diese Datei mit meinem Key verschlüssele, dann sieht die Datei so aus:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;KK:~ kris$ openssl enc -e -k g33k -aes-128-cbc -base64 -in /dev/stdin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Hello, World!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;U2FsdGVkX19isizbD981L7EOV+1Ou8O4xhBMQqDo3nw=
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wenn &lt;strong&gt;Du&lt;/strong&gt; das machst, dann ist der Inhalt der Datei offensichtlich anders:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;KK:~ kris$ openssl enc -e -k s3cr3t -aes-128-cbc -base64 -in /dev/stdin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Hello, World!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;U2FsdGVkX18Fqrwp51fq8Y23MBas01+z4gJBdFs+vWE=
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Beide Dateien haben offensichtlich unterschiedliche Prüfsummen, wenn man
verschlüsselten Content betrachtet. Die Prüfsumme muß also über den
unverschlüsselten Content berechnet werden, damit das vergleichbar wird. Für
den Client ist das leicht, da die Daten ja auf meiner und Deiner Platte für
den Client unverschlüsselt vorliegen.&lt;/p&gt;
&lt;p&gt;Danach stellt mir Dropbox die Datei auf dem Server zur Verfügung, wenn ich
die passende Prüfsumme vorzeigen kann. Ich kann die Datein dann runterladen,
mit meinem Schlüssel verschlüsselt. Das heißt, die Daten, die angeblich
verschlüsselt auf dem Dropbox Server liegen, können mit meinem und mit
Deinem Schlüssel verschlüsselt runtergeladen werden. Dropbox speichert die
Daten physikalisch auch nur einmal, um Speicherplatz zu sparen, denn sonst
hätten sie tausende Kopien von &amp;ldquo;Sascha Lobo singt&amp;rdquo; auf dem Server liegen,
die alle Platz wegnehmen. Naja, ok, Dutzende.&lt;/p&gt;
&lt;p&gt;Wahrscheinlicher ist also, daß Dropbox die Daten auf ihren Servern gar nicht
verschlüsselt, oder alle Daten mit einem einzigen globalen &amp;ldquo;Dropbox&amp;rdquo;-Key
verschlüsselt oder etwas ähnlich simples. Für die Kommunikation wird dann
eine Transportverschlüsselung über den Content gebraten, also für mich mein
Key und für Dich Dein Key.&lt;/p&gt;
&lt;p&gt;Dropbox hat also die Wahl zwischen Dateideduplikation und effizienten
Uploads auf der einen Seite und Datensicherheit durch Verschlüsselung auf
der anderen Seite. Klar, wo businessmodelltechnisch die Präferenzen liegen.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
