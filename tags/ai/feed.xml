<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on Die wunderbare Welt von Isotopp</title>
    <link>https://blog.koehntopp.info/tags/ai.html</link>
    <description>Recent content in Ai on Die wunderbare Welt von Isotopp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>kris-blog@koehntopp.de (Kristian Köhntopp)</managingEditor>

    
    <webMaster>kris-blog@koehntopp.de (Kristian Köhntopp)</webMaster>

    
    <lastBuildDate>Mon, 11 Nov 2024 18:49:27 +0000</lastBuildDate><atom:link href="https://blog.koehntopp.info/tags/ai/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Werk und AI</title>
      <link>https://blog.koehntopp.info/2024/09/29/werk-und-ai.html</link>
      <pubDate>Sun, 29 Sep 2024 04:05:06 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2024/09/29/werk-und-ai.html</guid>
      <description>&lt;p&gt;Jemand fragt im Kontext
&lt;a href=&#34;https://www.creativebloq.com/ai/ai-art/controversial-competition-winner-still-hopes-to-copyright-his-ai-art&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;People are &amp;ldquo;blatantly stealing my work,&amp;rdquo; AI artist complains&lt;/a&gt;

:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What the hell is an &amp;ldquo;AI artist&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;und das ist ziemlich kompliziert.&lt;/p&gt;
&lt;h1 id=&#34;werke-und-werkzeuge&#34;&gt;
    &lt;a href=&#34;#werke-und-werkzeuge&#34;&gt;
	Werke und Werkzeuge
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Du kannst ein Bild auf Leinwand malen und dann ist es ein Werk und durch das Urheberrecht geschützt.&lt;/p&gt;
&lt;p&gt;Du kannst Midjourney oder DALL-E bitten, Dir ein Bild nach folgendem Prompt zu malen,
und dann ist es kein Werk und auch nicht durch das Urheberrecht geschützt, denn das Gesetz verlangt &amp;ldquo;human authorship&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Der Prompt, den der Künstler geschaffen hat, wäre geschützt,
denn das Urheberrecht schützt ausdrücklich auch &lt;a href=&#34;https://medien-wiki.de/kleine-muenze&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kleine Münze&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;Du kannst ein Bild in Photoshop malen oder ein Foto von Dir in Photoshop bearbeiten,
und das wäre durch das Urheberrecht geschützt.&lt;/p&gt;
&lt;p&gt;Du kannst dann Teile des Bildes manuell in Photoshop retuschieren,
und auch das wäre durch das Urheberrecht geschützt.&lt;/p&gt;
&lt;p&gt;Du kannst Teile des Bildes durch Photoshop mit AI Inpainting retuschieren lassen und das… ist unklar.&lt;/p&gt;
&lt;p&gt;Ab irgendeinem Punkt ist es kein Werk von einem Menschen mehr,
oder nur noch Ausschnitte des Werkes sind von einem Menschen gemacht.&lt;/p&gt;
&lt;p&gt;Es ist offen, und darum geht es in diesem Prozeß, ab wieviel menschlichem Anteil etwas ein Werk ist.
Und ob dann das Gesamtbild ein Werk ist, oder nur die Teile, die von einem Menschen gemacht sind.&lt;/p&gt;
&lt;h1 id=&#34;bildgebende-werkzeuge&#34;&gt;
    &lt;a href=&#34;#bildgebende-werkzeuge&#34;&gt;
	Bildgebende Werkzeuge
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Es ist damit im Grunde auch offen, welche bildgebenden Werkzeuge, die durch einen Menschen benutzt werden,
ein menschliches Werk erzeugen und welche nicht.&lt;/p&gt;
&lt;p&gt;Fotos, also Bilder, die durch einen physikalischen Prozeß erzeugt werden, sind urheberrechtlich geschützt.&lt;/p&gt;
&lt;p&gt;Aber nur dann, wenn das Werkzeug durch einen Menschen bedient wird.
Ein
&lt;a href=&#34;https://www.stern.de/panorama/weltgeschehen/wie-das-beruehmte-affen-selfie-einen-fotografen-ruinierte--7561910.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nichtmenschliches Foto&lt;/a&gt;


ist nicht geschützt.&lt;/p&gt;
&lt;p&gt;Also, es ist bisher unstrittig,
dass ein digitaler Pinsel oder ein digitaler Mustergenerator den Werkscharacter nicht in Frage stellen.&lt;/p&gt;
&lt;p&gt;Es ist zur Zeit aber auch unstrittig, dass DALL-E keine Werke produzieren kann.&lt;/p&gt;
&lt;p&gt;Es ist unstrittig,
dass ein Blur-Filter oder ein Solarisations-Filter auf einem Bild oder Teilen eines Bildes den Werkscharacter nicht in Frage stellt.&lt;/p&gt;
&lt;p&gt;Es ist vollkommen unklar, was ein Photoshop Inpainting,
das man prompted &amp;ldquo;Entferne den Menschen im markierten Gebiet&amp;rdquo; mit dem Werkscharacter eines Bildes macht.
Oder &amp;ldquo;Ersetze den Himmel in diesem Bild durch dramatische Sturmwolken&amp;rdquo; auf das ganze Bild
mit dem Werkscharacter eines Bildes macht.&lt;/p&gt;
&lt;p&gt;Offenbar ist das Recht derzeit der Ansicht, dass es irgendwo eine Grenze zwischen Werk und Nichtwerk gibt,
aber es ist absolut unklar, in Gesetz und Anwendung, wo diese liegt.&lt;/p&gt;
&lt;h1 id=&#34;auch-für-texte&#34;&gt;
    &lt;a href=&#34;#auch-f%c3%bcr-texte&#34;&gt;
	Auch für Texte
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Dasselbe gilt auch für Texte:&lt;/p&gt;
&lt;p&gt;Du kannst ChatGPT bitten, &amp;ldquo;erzeuge mir eine Kurzgeschichte mit &amp;hellip;&amp;rdquo; und ChatGPT macht eine solche Kurzgeschichte.
Abgesehen davon, dass die langweilig ist, ist es auch nach aktuellem Recht kein Werk.&lt;/p&gt;
&lt;p&gt;Du kannst aber auch eine Reihe von Prompts schreiben, in denen Du mit ChatGPT eine Geschichte entwickelst,
indem Du interaktiv Szenario, Figuren, Konflikte und Arc generierst.
Du kannst nach diesen automatisch generierten Ideen eine Story schreiben – ein menschliches Werk?&lt;/p&gt;
&lt;p&gt;Du kannst aber auch in einer neuen Session mit ChatGPT eine Reihe von Prompts entlang des Arc einwerfen,
und den resultierenden Text in eine zusammenhängende Geschichte umbauen.
Kein menschliches Werk.&lt;/p&gt;
&lt;p&gt;Dann nimmst Du sowohl diesen Text als auch den ersten, selbst geschriebenen, kapitelweise,
bearbeitest jeden von denen ein wenig und pastest ihn wieder nach ChatGPT.
Der Prompt ist dabei die Bitte, die Rechtschreibung und Grammatik zu korrigieren,
sprachlich die folgenden Redewendungen zu vermeiden und den Text aggressiv im Stil von &amp;hellip; umzuformulieren.
Was davon ist ein Werk?&lt;/p&gt;
&lt;p&gt;Das Resultat ist etwas, das ChatGPT alleine nicht hätte erzeugen können,
es hat definitiv eine menschliche Komponente und eine große Menge menschliche Führung.
Selbst dann, wenn jedes Wort im sichtbaren Text von ChatGPT generiert ist.&lt;/p&gt;
&lt;p&gt;Die genaue Formulierung und der Stil der Sprache sind aber autogeneriert.
Ist das ein Werk?&lt;/p&gt;
&lt;p&gt;Wir wissen es nicht.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LLMs daheim mit Ollama</title>
      <link>https://blog.koehntopp.info/2024/05/31/llms-daheim-mit-ollama.html</link>
      <pubDate>Fri, 31 May 2024 05:06:07 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2024/05/31/llms-daheim-mit-ollama.html</guid>
      <description>&lt;p&gt;Dieser Text ist eine Art Follow-up für &lt;a href=&#34;https://blog.koehntopp.info/2024/02/06/wie-chatgpt-funktioniert.html&#34;&gt;Wie ChatGPT funktioniert&lt;/a&gt;

.
Es geht darum, ein LLM lokal auszuführen und damit zu experimentieren.&lt;/p&gt;
&lt;p&gt;Wer mit LLMs daheim experimentieren möchte, der steht vor dem Problem,
einen Haufen furchtbar empfindlicher und schlecht zu aktualisierender Abhängigkeiten in Python zu installieren,
monströse Downloads zu verwalten und die Ausführung für die eigene Maschine zu optimieren.
Das wird ein bisschen verbessert mit &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;llama.cpp&lt;/a&gt;

,
einer C++/C Bibliothek, die einem einen Teil der Arbeit abnimmt,
aber man hat immer noch kein Paket, mit dem man einfach Ergebnisse bekommt.&lt;/p&gt;
&lt;p&gt;Abhilfe schafft hier &lt;a href=&#34;https://github.com/ollama/ollama&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ollama&lt;/a&gt;

,
eine in Golang geschriebene Anwendung,
die llama.cpp integriert und den Download, die Verwaltung und die Ausführung von LLMs automatisiert.
Die &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;

 verweist auf das Blog,
den Discord und das Github von Ollama, und hat eine Suche,
mit der man auf Ollama angepasste LLms finden und herunterladen kann.&lt;/p&gt;
&lt;h1 id=&#34;installation&#34;&gt;
    &lt;a href=&#34;#installation&#34;&gt;
	Installation
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;LLMs sind rechenintensive Monster, die eine große Maschine zur Ausführung brauchen.
Aber eingedampfte Spezialversionen kann man schon auf einem großen Raspi ausführen.
Größe ist jedoch ein qualitativer Unterschied:
Ein LLM mit 0.3 Mrd. Parameters verhält sich ganz anders als eines mit 8 Mrd. Parametern,
und dieses wiederum wird von einem LLM mit 70 Mrd. Parameters vollkommen deklassiert.
Ersteres kann man auf einem Raspi ausführen,
die 8 Mrd. Parameter-Modelle oft auf einem Mac oder Windows-Rechner mit 32 GB RAM oder 12 GB VRAM in der Grafikkarte,
und die 70 Mrd. Parameter-Modelle brauchen in der Regel Spezialrechner mit 128 GB RAM und zwei 24 GB Nvidia-Karten kombiniert.&lt;/p&gt;
&lt;p&gt;In meinem Fall steht mir&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ein Mac mini M2pro mit 32 GB RAM und 8+4 Cores&lt;/li&gt;
&lt;li&gt;ein Windows 11 Rechner mit 64 GB RAM und Nvidia 4070Ti (12 GB VRAM)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;zur Verfügung. Beide Maschinen haben ausreichend Plattenplatz auf NVME.&lt;/p&gt;
&lt;p&gt;Man kann sich Ollama von der Website herunterladen und installieren,
oder das Golang Projekt selbst clonen und compilieren,
aber die einfachste Weise der Installation sind Paketmanager für MacOS und Windows.&lt;/p&gt;
&lt;p&gt;Auf dem Mac habe ich Homebrew, und &lt;code&gt;brew install ollama&lt;/code&gt; ist die schnellste und einfachste Methode,
Ollama zu installieren und aktuell zu halten.
Auf Windows geht dasselbe mit &lt;a href=&#34;https://scoop.sh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scoop&lt;/a&gt;

, &lt;code&gt;scoop install ollama&lt;/code&gt; regelt die Details.&lt;/p&gt;
&lt;h1 id=&#34;ollama-serve-ausführen&#34;&gt;
    &lt;a href=&#34;#ollama-serve-ausf%c3%bchren&#34;&gt;
	&lt;code&gt;ollama serve&lt;/code&gt; ausführen
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Ollama wird als Server ausgeführt.
Dazu muss man &lt;code&gt;ollama serve&lt;/code&gt; starten.&lt;/p&gt;
&lt;p&gt;Alle weiteren Operationen setzen voraus, daß in einem Fenster irgendwo &lt;code&gt;ollama serve&lt;/code&gt; aktiv ist,
denn alle weiteren Operationen tun nichts anderes als REST-Requests an &lt;code&gt;ollama serve&lt;/code&gt; zu senden.
Dies kann mit der &lt;code&gt;ollama&lt;/code&gt;-Kommandozeile passieren, oder man schreibt sich eigene Python-Programme, die das tun&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;$&lt;/span&gt; ollama serve
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kk:~ kris$ ollama serve
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2024/05/31 09:52:25 routes.go:1008: INFO server config 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;  env=&amp;#34;map[OLLAMA_DEBUG:false OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;   OLLAMA_MAX_VRAM:0 OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;   OLLAMA_ORIGINS:[http://localhost ... https://0.0.0.0:*] 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;   OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;time=2024-05-31T09:52:25.124+02:00 level=INFO source=images.go:704 msg=&amp;#34;total blobs: 28&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;time=2024-05-31T09:52:25.131+02:00 level=INFO source=images.go:711 msg=&amp;#34;total unused blobs removed: 0&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;time=2024-05-31T10:02:15.032+02:00 level=INFO source=payload.go:44 msg=&amp;#34;Dynamic LLM libraries [metal]&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;time=2024-05-31T10:02:15.090+02:00 level=INFO source=types.go:71 msg=&amp;#34;inference compute&amp;#34; 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;  id=0 library=metal compute=&amp;#34;&amp;#34; driver=0.0 name=&amp;#34;&amp;#34; total=&amp;#34;21.3 GiB&amp;#34; available=&amp;#34;21.3 GiB&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Ollama lädt später Parameterdaten für die LLMs herunter. Diese nehmen einige Gigabyte Plattenplatz ein.
Per Default legt das MacOS Ollama dies im Home des Benutzers ab, in &lt;code&gt;$HOME/.ollama/models&lt;/code&gt;.
Man kann dies mit einer Umgebungsvariable umstellen.&lt;/p&gt;
&lt;p&gt;In diesem Fall muss man &lt;code&gt;OLLAMA_MODELS&lt;/code&gt; setzen:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kk:~ kris$ ollama serve --help
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Start ollama
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;go&#34;&gt;Usage:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;  ollama serve [flags]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;go&#34;&gt;Aliases:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;  serve, start
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;go&#34;&gt;Flags:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;  -h, --help   help for serve
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;go&#34;&gt;Environment Variables:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;go&#34;&gt;    OLLAMA_HOST         The host:port to bind to (default &amp;#34;127.0.0.1:11434&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;    OLLAMA_ORIGINS      A comma separated list of allowed origins
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;    OLLAMA_MODELS       The path to the models directory (default &amp;#34;~/.ollama/models&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;    OLLAMA_KEEP_ALIVE   The duration that models stay loaded in memory (default &amp;#34;5m&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;    OLLAMA_DEBUG        Set to 1 to enable additional debug logging
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gp&#34;&gt;$&lt;/span&gt; kk:~ kris$ mkdir /Volumes/Ablage/Torch/ollama-models/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;OLLAMA_MODELS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/Volumes/Ablage/Torch/ollama-models/ ollama serve
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Der Speicher dort sollte schnell sein und ausreichend Raum bereitstellen:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kk:~ kris$ df -h /Volumes/Ablage
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Filesystem      Size    Used   Avail Capacity iused ifree %iused  Mounted on
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;/dev/disk9s1   3.6Ti   804Gi   2.9Ti    22%    360k   31G    0%   /Volumes/Ablage
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kk:~ kris$ du -sh /Volumes/Ablage/Torch/ollama-models
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; 54G	/Volumes/Ablage/Torch/ollama-models
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kk:~ kris$ ollama list
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;NAME                    	ID          	SIZE  	MODIFIED
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;llama3:70b-instruct-q2_K	693db6efd8f9	26 GB 	6 days ago
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;llama3:instruct         	365c0bd3c000	4.7 GB	6 days ago
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;mistral:instruct        	2ae6f6dd7a3d	4.1 GB	6 days ago
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;gemma:instruct          	a72c7f4d0a15	5.0 GB	6 days ago
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;llama3-gradient:instruct	5d1398df5b8b	4.7 GB	6 days ago
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;phi3:14b                	1e67dff39209	7.9 GB	7 days ago
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;llava-llama3:latest     	44c161b1f465	5.5 GB	7 days ago
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;llama3-gradient:latest  	5d1398df5b8b	4.7 GB	7 days ago
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;phi3:14b-instruct       	1e67dff39209	7.9 GB	7 days ago
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Der Ollama-Server bindet sich an localhost, Port 11434 und lauscht nun auf eingehende REST-Requests.
Diese können dazu führen, daß ein Model geladen und ausgeführt wird.
Das kann beim ersten Mal einige Zeit dauern, da viele Gigabyte Daten geladen werden müssen.&lt;/p&gt;
&lt;p&gt;Der Server hält das Model einige Zeit im Speicher, &lt;code&gt;OLLAMA_KEEP_ALIVE&lt;/code&gt; viele Minuten.
Der Default ist 5 m.
Danach wird der Speicher wieder freigegeben.&lt;/p&gt;
&lt;h1 id=&#34;modelle-aus-dem-internet-laden&#34;&gt;
    &lt;a href=&#34;#modelle-aus-dem-internet-laden&#34;&gt;
	Modelle aus dem Internet laden
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;LLMs sind recht komplizierte Konstrukte mit der eigentlichen Parameter-Datei, die den Großteil der Daten aus macht,
einem Systemprompt und weiteren Layers, die meist eher nicht so groß sind.
Auf einem handelsüblichen Desktop-Rechner kann man erwarten, ein &amp;ldquo;7B&amp;rdquo; oder &amp;ldquo;8B&amp;rdquo;-Modell auszuführen.
Dies belegt zwischen 5 GB und 9 GB an Speicher, im RAM oder auf der Grafikkarte.&lt;/p&gt;
&lt;p&gt;Richtig große Modelle, 70B-Modelle, brauchen wesentlich mehr Speicher und eine Hardware,
die für die Ausführung von LLMs auf mehreren Grafikkarten vorbereitet ist.&lt;/p&gt;
&lt;p&gt;Ein guter Startpunkt ist das Mistral AI-Model &lt;a href=&#34;https://ollama.com/library/mistral&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mistral&lt;/a&gt;

.
Man lädt es mit &lt;code&gt;ollama pull mistral:instruct&lt;/code&gt; herunter.
Das wird Zeit lang dauern und das Model-Directory entsprechend voll machen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HINWEIS:&lt;/strong&gt; Noch einmal die Erklärung, daß alle Arbeit von &lt;code&gt;ollama serve&lt;/code&gt; erledigt wird.
Wenn dieses Kommando nicht aktiv ist, funktioniert keines der anderen Kommandos.
&lt;code&gt;ollama pull&lt;/code&gt;, &lt;code&gt;ollama run&lt;/code&gt; und auch &lt;code&gt;ollama rm&lt;/code&gt; senden lediglich REST-Requests an den Server,
der dann die Arbeit macht.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kk:~ kris$ ollama pull mistral:instruct
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;pulling manifest
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;pulling ff82381e2bea... 100% ▕████████████████▏ 4.1 GB
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;pulling 43070e2d4e53... 100% ▕████████████████▏  11 KB
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;pulling c43332387573... 100% ▕████████████████▏   67 B
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;pulling ed11eda7790d... 100% ▕████████████████▏   30 B
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;pulling 42347cd80dc8... 100% ▕████████████████▏  485 B
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;verifying sha256 digest
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;writing manifest
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;removing any unused layers
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;success
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;modelle-ausführen&#34;&gt;
    &lt;a href=&#34;#modelle-ausf%c3%bchren&#34;&gt;
	Modelle ausführen
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Das Modell kann man mit &lt;code&gt;ollama run mistral:instruct&lt;/code&gt; testen.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;&amp;gt;&lt;/span&gt;&amp;gt;&amp;gt; /set verbose
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Set &amp;#39;verbose&amp;#39; mode.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gp&#34;&gt;&amp;gt;&lt;/span&gt;&amp;gt;&amp;gt; Explain in a single sentence &lt;span class=&#34;s2&#34;&gt;&amp;#34;Why is the sky blue?&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; The sky appears blue because molecules in the Earth&amp;#39;s atmosphere scatter
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;sunlight in all directions, and blue light is scattered more because it
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;travels in shorter, smaller waves.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;go&#34;&gt;total duration:       1.559908875s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;load duration:        2.719417ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;prompt eval count:    23 token(s)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;prompt eval duration: 303.421ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;prompt eval rate:     75.80 tokens/s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;eval count:           41 token(s)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;eval duration:        1.245361s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;eval rate:            32.92 tokens/s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wir sehen hier, daß mein M2 pro mini 32 GB 1.56 Sekunden gebraucht hat, um diese Antwort zu generieren.
Die Frage hat &amp;ldquo;23 Token&amp;rdquo; belegt, die mit 75 Token/s analysiert worden sind.&lt;/p&gt;
&lt;p&gt;Die Generierung der Ausgabe hat 41 Token erzeugt, 1.25s gedauert und ist mit 33 Token/s gelaufen.
Der Rechner hat dabei ca. 50 Watt gezogen. Normal verbraucht er circa 7 Watt, also werden 43 Watt mehr verbraucht.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/05/ollama-3.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ausgabe der MacOS App Stats (&lt;code&gt;brew install stats&lt;/code&gt;), der Rechner verbraucht normal ca. 7 Watt, mit dem LLM aktiv 50 Watt.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/05/ollama-1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ausgabe der MacOS App Stats, hier: GPU Auslastung. Dies ist die einzige Anwendung, die die Mac mini GPU an den Anschlag bringt.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Die Nvidia 4070Ti kommt auf mehr als 70 Token/s, zieht aber über 400 W.&lt;/p&gt;
&lt;p&gt;Der &lt;code&gt;ollama run&lt;/code&gt;-Prompt kann mit &lt;code&gt;/bye&lt;/code&gt; verlassen werden.&lt;/p&gt;
&lt;p&gt;Die vorhandenen Modelle können mit &lt;code&gt;ollama list&lt;/code&gt; aufgelistet werden.
Mit &lt;code&gt;ollama rm&lt;/code&gt; löscht man ein lokales Modell und gibt den Plattenplatz frei.
Es muss dann neu heruntergeladen werden.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kk:~ kris$ ollama rm llama3:instruct
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;deleted &amp;#39;llama3:instruct&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;arbeiten-mit-ollama-run-texte-zusammenfassen&#34;&gt;
    &lt;a href=&#34;#arbeiten-mit-ollama-run-texte-zusammenfassen&#34;&gt;
	Arbeiten mit &lt;code&gt;ollama run&lt;/code&gt;: Texte zusammenfassen
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Für das folgende Beispiel ist ein laufender Server &lt;code&gt;ollama serve&lt;/code&gt; erforderlich,
das Modell &lt;code&gt;mistral:instruct&lt;/code&gt; muss bereitstehen (d.h. &lt;code&gt;ollama list&lt;/code&gt; zeigt es an),
und wir verwenden den Text &lt;a href=&#34;https://www.gutenberg.org/files/61213/61213.txt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The 64-Square Madhouse&lt;/a&gt;

 von Fritz Leiber.&lt;/p&gt;
&lt;p&gt;Das Ziel ist es, Ollama diesen Text mithilfe von Llama 3 zusammenfassen zu lassen.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Den Text von Fritz Leiber vom Projekt Gutenberg herunterladen.
Es ist hilfreich, allen Copyright-Text am Anfang und am Ende wegzuschneiden,
um dem Modell nur die Story selbst zu füttern.&lt;/li&gt;
&lt;li&gt;Server starten: &lt;code&gt;ollama serve&lt;/code&gt;.
Dabei sind ggf. die notwendigen Parameter als Umgebungsvariablen zu setzen, damit die Modelle gefunden werden.&lt;/li&gt;
&lt;li&gt;Modell prüfen: &lt;code&gt;ollama list&lt;/code&gt; zeigt das Modell &lt;code&gt;mistral:instruct&lt;/code&gt; als vorhanden an.
Ansonsten muss es mit &lt;code&gt;ollama pull mistral:instruct&lt;/code&gt; heruntergeladen werden.&lt;/li&gt;
&lt;li&gt;Den Prompt mit &lt;code&gt;ollama run mistral:instruct&lt;/code&gt; starten.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Wir verwenden hier das Modell &lt;code&gt;mistral:instruct&lt;/code&gt; von &lt;a href=&#34;https://en.wikipedia.org/wiki/Mistral_AI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mistral AI&lt;/a&gt;

.
Das ist eine Firma von ehemaligen Google und Meta/Facebook Mitarbeitern, die vor einem Jahr in Paris gegründet wurde.&lt;/p&gt;
&lt;p&gt;Auf dem Prompt erfolgt die Kommunikation mit dem Modell in Englisch.
Manche Modelle sind ausdrücklich mehrsprachig trainiert (zum Beispiel &lt;a href=&#34;https://ollama.com/library/mixtral&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mixtral&lt;/a&gt;

),
aber wenn dies in der Anleitung zum Modell nicht angegeben ist, ist Englisch zu verwenden.&lt;/p&gt;
&lt;p&gt;Wir können dem Modell eine Anfrage in einer Zeile stellen.
Mehrzeilige Anweisungen werden mit &lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt; eingeleitet und genau so beendet.&lt;/p&gt;
&lt;p&gt;Es ist ein &lt;code&gt;instruct&lt;/code&gt;-Modell, es wird also keine Annahmen machen, sondern wir müssen Arbeitsanwendungen geben.&lt;/p&gt;
&lt;p&gt;Wir können dem Client Parameter setzen.
Dies erfolgt mit &lt;code&gt;/&lt;/code&gt;-Kommandos, &lt;code&gt;/help&lt;/code&gt; für eine Übersicht.&lt;/p&gt;
&lt;p&gt;Modelle haben Parameter.
Ein wichtiger Parameter ist &lt;code&gt;num_ctx&lt;/code&gt; (manchmal auch n_ctx).
Er bestimmt die Anzahl der Token, die das Modell als Kontextspeicher für die Antwortgenerierung verwendet.&lt;/p&gt;
&lt;p&gt;Ein Embedding ist ein vieldimensionaler Vektor, der Bedeutung von Worten codiert.
Es belegt etwa 1 Kilobyte an Speicher.
Ein Token ist ein Zeiger auf ein Embedding, aber im Kontext der Antwortgenerierung wird das Token kopiert.
Ein Kontext von 65536 belegt also in etwa 64 MB Speicher.
In englischer Sprache entspricht ein Wort in etwa 1.3 Tokens.&lt;/p&gt;
&lt;p&gt;Unser Beispieltext hat, nachdem wir vorne und hinten alles das wegschneiden,
was Projekt Gutenberg als Disclaimer hinzugefügt hat,
eine Größe von circa 14.000 Worten:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kk:~ kris$ wc leiber-64-square.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;    1824   13853   84236 leiber-64-square.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wenn wir dem Modell einen Kontext von 65536 Token geben, dann sollte es mit der Geschichte gut zurechtkommen.
&lt;strong&gt;HINWEIS:&lt;/strong&gt;* Es heißt &lt;code&gt;/set parameter num_ctx 65536&lt;/code&gt;, nicht &lt;code&gt;/set num_ctx 65536&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kk:~ kris$ ollama run mistral:instruct
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gp&#34;&gt;&amp;gt;&lt;/span&gt;&amp;gt;&amp;gt; /set verbose
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Set &amp;#39;verbose&amp;#39; mode.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gp&#34;&gt;&amp;gt;&lt;/span&gt;&amp;gt;&amp;gt; /set parameter num_ctx &lt;span class=&#34;m&#34;&gt;65536&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Set parameter &amp;#39;num_ctx&amp;#39; to &amp;#39;65536&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gp&#34;&gt;&amp;gt;&lt;/span&gt;&amp;gt;&amp;gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;&lt;span class=&#34;go&#34;&gt;... You are an english teacher.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... Summarize the following story,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... providing an overview over the main characters of the story,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... and a paragraph for each scene or major event that happens in the course of
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... the story.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... As a conclusion, provide context: What is the story about?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... The story:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wir setzen also &lt;code&gt;/set verbose&lt;/code&gt;, um Statistiken zu bekommen.
Wir setzen dann den Parameter &lt;code&gt;/set parameter num_ctx 65536&lt;/code&gt;, um dem Modell genug Arbeitskontext zu geben.
Danach leiten wir mit &lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt; einen mehrzeiligen Input ein.
Dieser wird durch &lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt; auf einer neuen Zeile genau so abgeschlossen.&lt;/p&gt;
&lt;p&gt;In einem anderen Fenster laden wir nun die Story in den Paste-Buffer:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;(venv) kk:keks kris$ cat leiber-64-square.txt | pbcopy
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;(venv) kk:keks kris$
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Diese kann nun in den Ollama-Prompt gepasted werden.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... Sandra said, &amp;#34;Well, there&amp;#39;s something you can&amp;#39;t build into a
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... machine--ego.&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... &amp;#34;Oh, I don&amp;#39;t know about that,&amp;#34; said Simon Great.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;... &amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Die generierte Antwort des Modells lautet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This story is about a chess tournament featuring an electronic computing
machine named the Machine. The Machine is entering its first international
grandmaster chess tournament, and the large hall where the event takes
place is filled with energetic dark-suited men who are either playing
chess or discussing it. There are signs featuring various combinations of
letters such as FIDE, WBM, USCF, USSF, USSR, and UNESCO, and many clocks
bedside table size with red flags and wheels on their faces are present to
keep track of each player&amp;rsquo;s moves. The Machine is programmed by a
psychologist named Simon Great, who has been out of tournament chess for
twenty years but is now making a comeback.&lt;/p&gt;
&lt;p&gt;The main characters in the story are Sandra Lea Grayling, a journalist for
the &lt;em&gt;Chicago Space Mirror&lt;/em&gt;, and Dr. Krakatower, a former grandmaster of
chess who is also a special correspondent for the London Times. The
Machine is also a character, as it plays against various human opponents
throughout the tournament.&lt;/p&gt;
&lt;p&gt;The story unfolds over the course of several days, with each scene or
major event taking place during one of the nine rounds of the tournament.
In the first round, the Machine plays against Bela Grabo and wins the
game. This causes a stir among the players and spectators, as many believe
that the Machine is too good to be true. In the second round, the Machine
faces off against Votbinnik and again emerges victorious. This pattern
continues throughout the tournament, with the Machine consistently
outplaying its human opponents.&lt;/p&gt;
&lt;p&gt;Dr. Krakatower struggles throughout the tournament, losing several games
and finding himself in last place. However, he manages to draw his game
against Sherevsky in the next to last round, which gives him a small
glimmer of hope. In the final round, the Machine plays against Willie
Angler, who has been consistently strong throughout the tournament. The
outcome of this match will determine the winner of the tournament.&lt;/p&gt;
&lt;p&gt;As the story reaches its conclusion, it is revealed that Angler had
intentionally played a flawed move in his game against the Machine, as he
had discovered an error in the programming of the Machine&amp;rsquo;s opening moves.
This tactic pays off, and Angler wins the game and the tournament. The
final standing shows that Angler and Votbinnik are tied for first place,
with the Machine in second.&lt;/p&gt;
&lt;p&gt;The story concludes with a discussion between Simon Great, Willie Angler,
and Dr. Krakatower about the future of chess and computer programming. It
is suggested that grandmaster chess without computers will become less
popular, as players will increasingly rely on computers to aid them in
their games. The story also touches on the idea of man-machine symbiosis,
with the implication that future grandmasters may be a combination of
human and machine. The story serves as a commentary on the changing nature
of chess and the role of technology in society.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;und&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;total duration:       3m33.240299458s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;load duration:        20.467748958s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;prompt eval count:    22580 token(s)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;prompt eval duration: 1m58.450477s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;prompt eval rate:     190.63 tokens/s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;eval count:           634 token(s)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;eval duration:        1m14.263155s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;eval rate:            8.54 tokens/s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wir haben eine recht gute Evaluation Rate: 190 Token/s.
Die Generierung der Antwort ist langsamer, auch wegen des großen Kontexts, den wir gesetzt haben: 8.5 Token/s.&lt;/p&gt;
&lt;p&gt;Eine Laufzeit von 3 Minuten bei einem Mehrverbrauch von 43 Watt entspricht einem Energieaufwand von 2.15 Wh.
Bei einem Preis von 32 Cent pro 1000 Wh (1 kWh) sind das 2.15/1000 * 32 = .0672 Cent zur Generierung der Ausgabe.&lt;/p&gt;
&lt;h1 id=&#34;selbst-coden&#34;&gt;
    &lt;a href=&#34;#selbst-coden&#34;&gt;
	Selbst coden
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Es gibt eine Ollama Python Bibliothek.
Diese tut nicht mehr, als Requests an &lt;code&gt;ollama serve&lt;/code&gt; zu senden:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama-python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ollama-python&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama/tree/main/docs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ollama API Dokumentation&lt;/a&gt;

, api.md&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.mistral.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mistral Model Documentation&lt;/a&gt;

, die Code-Beispiele dort gehen davon aus,
daß man deren Server verwendet. Das tun wir nicht, wir verwenden Ollama und eine lokale Instanz.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Die REST API ist in &lt;code&gt;api.md&lt;/code&gt; im Ollama Quelltext-Tree auf Github dokumentiert.&lt;/p&gt;
&lt;p&gt;Die &lt;code&gt;ollama.chat()&lt;/code&gt;-Funktion (und auch die gleichnamige Funktion der Mistral API, die gegen deren Instanz arbeitet),
hat einen Parameter &lt;code&gt;messages&lt;/code&gt; (Plural, eine Liste).
Das LLM ist stateless.
Es kann sich nicht an vorhergehende Nachrichten erinnern.
Daher muss man dem Modell bei jedem Request alle vorhergehenden Fragen und Antworten und auch den Systemprompt mitsenden.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;o&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Client&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;127.0.0.1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;o&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;mistral:instruct&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;instructions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;old_message&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;assistant&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;old_answer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;s2&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;s2&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;message&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Genauso muss man &lt;code&gt;options={}&lt;/code&gt; mitsenden, wenn man zum Beispiel mehr Kontext braucht.&lt;/p&gt;
&lt;p&gt;Die Anleitung zu Mistral gibt bessere
&lt;a href=&#34;https://docs.mistral.ai/guides/prompting_capabilities/#summarization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Summarization Prompts&lt;/a&gt;


als Beispiel und erklärt im hinteren Teil mehr zu Tokens, Tokenization und zum direkten Umgang mit dem LLM.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wie ChatGPT funktioniert</title>
      <link>https://blog.koehntopp.info/2024/02/06/wie-chatgpt-funktioniert.html</link>
      <pubDate>Tue, 06 Feb 2024 01:02:03 +0000</pubDate><author>kris-blog@koehntopp.de (Kristian Köhntopp)</author>

      <guid>https://blog.koehntopp.info/2024/02/06/wie-chatgpt-funktioniert.html</guid>
      <description>&lt;p&gt;Es gibt einen wunderbaren Artikel von Stephen Wolfram,
&lt;a href=&#34;https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What Is ChatGPT Doing … and Why Does It Work?&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;Dies ist ein etwa ein Jahr alter und langer Text, und man muss dran bleiben, wenn man ihn verstehen möchte.
Er erklärt wie ChatGPT intern funktioniert, warum das so viel Ressourcen kostet,
und er erklärt auch, daß wir keine Ahnung haben, wieso das &amp;ldquo;funktioniert&amp;rdquo;.
Er zeigt aber auch, was ChatGPT alles prinzipbedingt niemals erreichen kann, und auch nicht lernen wird.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-01.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is ChatGPT doing…&lt;/a&gt;


von Stephen Wolfram vom 14. Februar 2023.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Stephen Wolfram ist ein Informatiker und Physiker,
der einen recht guten Prolog-Interpreter mit Regeln zur Termumformung und Fakten gefüttert hat,
und dies unter dem Namen Mathematica vertreibt.&lt;/p&gt;
&lt;p&gt;Prolog ist eine deklarative Sprache, die vor 30 Jahren die Grundlage regelbasierter künstlicher Intelligenz-Forschung war.
Deklarative Sprachen wie SQL oder Prolog schreiben keine Programme, die dem Computer sagen, was zu tun ist,
sondern sie beschreiben das gewünschte Ergebnis.
Ein Programmgenerator, der Planer, versucht dann einen Ausführungspfad zu finden,
der das beschriebene Ergebnis auf eine effiziente Weise findet.&lt;/p&gt;
&lt;p&gt;Die Idee hinter diesem Ansatz in der KI-Forschung war, daß man die Welt regelbasiert beschreiben könnte,
ein Welt-Wissensmodell bauen und daß so &amp;ldquo;Intelligenz&amp;rdquo; entsteht.
Das war ein Fehlschlag, aber Mathematica ist eines der beeindruckendsten Arbeitssysteme für Mathematik,
Physik-Visualisierung und Modellierung und alle verwandten Arbeiten.
Eine private Lizenz ist teuer, aber ich gönne mir die,
weil es so einen Heidenspaß macht mit dem Ding zu arbeiten und Sachen auszuprobieren.
Rein optisch und von der Arbeitsweise ist Jupyter ein gnadenloser Mathematica-Clone,
aber in Mathematica steckt eine komplett andere Sprache (etwas, das mal ein Prolog war) und eine viel, viel größere Bibliothek.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-02.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ein Mathematica Arbeitsblatt, das die Approximation von Rechteckwellen durch Fouriersynthese zeigt,
indem es eine Formelschar generiert und plotted.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-03.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Wir generieren daraus Audio, stellen das Audio dar, und spielen es ab.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Wolfram geht in seinem Text auf einen der simpelsten Textgeneratoren ein (und zeigt, wie man ihn in Mathematica realisiert) –
ein Zeichenfrequenz-Zähler, der Wikipedia-Seiten konsumiert und zeigt,
wie oft (normiert, also in %) jeder Buchstabe in der Seite auftaucht.&lt;/p&gt;
&lt;p&gt;Er vergleicht die Seiten von &amp;ldquo;Cat&amp;rdquo; und &amp;ldquo;Dog&amp;rdquo; und zeigt, daß in der &amp;ldquo;Dog&amp;rdquo;-Seite natürlich die o-Wahrscheinlichkeit höher ist,
aber daß beide für Englisch charakteristische Buchstabenhäufigkeiten haben.
Er greift sich dann die Häufigkeiten von Buchstabenpaaren (Bi-Grammen) und zeigt an der Spalte für den Buchstaben &amp;ldquo;q&amp;rdquo;,
daß &amp;ldquo;q&amp;rdquo; quasi immer von &amp;ldquo;u&amp;rdquo; gefolgt wird, und man kann das sehen.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-04.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Entlang der X-Achse die ersten Buchstaben von Paaren,
entlang der Y-Achse die zweiten Buchstaben.
Je roter und dunkler das Feld, umso häufiger die Kombination.
Die Spalte &amp;ldquo;q&amp;rdquo; ist ganz weiß, mit Ausnahme der &amp;ldquo;qu&amp;rdquo;-Kombination.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nimmt man mehr Kontext, also statt Paaren Dreiergruppen (Tri-Gramme) oder noch längere Gruppen,
dann bekommt man irgendwann fast aussprechbare oder reale englische Worte.
Schon 4-Gramme produzieren gute Worte.
Englisch hat also einen Buchstabenkontext von circa 4-5.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-05.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Eine Reihe von Worten, die aus Nachfolgewahrscheinlichkeiten von Bi-, Tri- und n-Grammen erzeugt worden ist.
Ab einer Länge von etwa 4 entstehen oft gültige Worte der englischen Sprache.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Würde man das mit chinesischen Schriftzeichen machen,
bekäme man aus einem Grundwortschatz von einigen tausend Worten auf diese Weise Sätze.
Viele chinesische Begriffe haben ein Schriftzeichen, manche zur Klärung der Bedeutung zwei.
ChatGPT macht das genau so.&lt;/p&gt;
&lt;p&gt;Dort sind es keine Schriftzeichen, sondern Token.
Aber ein Token entspricht ziemlich genau der Funktion, die ein Schriftzeichen in Chinesisch hat.
Und es sind fast genau so viele – in der Größenordnung von Zehntausenden.
Davon sind viele Begriffe Ein-Token und einige Mehr-Token.&lt;/p&gt;
&lt;p&gt;Bigramme auf Token produzieren Dinge wie dies hier: Wortsalat.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-06.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Eine Reihe von Wortfolgen, die alle zufällig generiert wurden, auf der Basis von Wortfolgewahrscheinlichkeiten,
die über Bi-Grammen, also Wortpaaren, errechnet worden sind.
Alle Beispiel starten mit dem Wort &amp;ldquo;cat&amp;rdquo;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Stellt man mehr Kontext, arbeitet man also mit n-Grammen für n=3, 4, …,
dann bekommt man eher Dinge, die wie korrekte Sätze aussehen.&lt;/p&gt;
&lt;p&gt;Man bekommt auch eine kombinatorische Explosion:
Statt 26, 26*26, 26*26*26, &amp;hellip; Wahrscheinlichkeiten zum Abspeichern hat man jetzt zum Beispiel 40.000 als Basis.
Und bei einem großen Kontext – statt 4 oder 5 eher was bei 100 – hat man dann mit 40.000 hoch 100 zu tun.&lt;/p&gt;
&lt;p&gt;Also, nicht – weil man das so nicht verarbeiten kann,
und ein wenig Arbeit in die Speicherung von dünn besetzten Matrizen und große Datenmengen stecken muss.
Aber das ist ein lösbares Problem, auch wenn es Aufwand erfordert, der auf der Mathematik nicht sichtbar ist.&lt;/p&gt;
&lt;p&gt;Wolfram erklärt dann &amp;ldquo;Modellierung durch Interpolation&amp;rdquo;.
Er hat Messwerte, die den Weg eines geworfenen Balls durch den Raum beschreiben,
und will dann nacheinander eine Gerade, eine Parabel oder &amp;ldquo;was Kompliziertes&amp;rdquo; da hindurchlegen.&lt;/p&gt;
&lt;p&gt;Er nutzt dieses Beispiel, um viele Dinge um den Modellbegriff zu erklären:
Einmal gibt es verschiedene Wege, um den Fehler zu messen,
also die Abweichung der interpolierten Linie von den Messwerten zu beschreiben.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-07.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Die Messwerte eines Ballwurfes, und die verschiedenen hindurch interpolierten Kurven.
Eine Gerade, eine Parabel und &amp;ldquo;was Kompliziertes&amp;rdquo;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Dann setzt die &amp;ldquo;Beweglichkeit&amp;rdquo; der Funktion, die zur Interpolation genutzt wird, dem Modell Grenzen.
Egal, welche Koeffizienten man findet – die &amp;ldquo;komplizierte&amp;rdquo; Funktion aus dem letzten Beispiel hat immer große Fehler.
Sie kann keinen Fit ergeben.&lt;/p&gt;
&lt;p&gt;Die zweite Funktion, die Parabel, ist immer ein guter Fit.
Das ist so, weil das real-weltliche Phänomen sich aus Gründen tatsächlich parabolisch verhält,
bis es durch Luftwiderstand gestört wird.
In jedem Modell stecken Annahmen – und die können, wenn sie falsch sind, – das Modell unmöglich machen.&lt;/p&gt;
&lt;p&gt;Er geht dann zu neuronalen Netzen über und diskutiert, wie diese aufgebaut sind,
und welche Mathematik da drin steckt und wofür man sie einsetzen kann.
Es sind Geradengleichungen, mx+b, und Summen, aber sehr, sehr viele.
Sein Beispiel ist die Schrifterkennung.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-08.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ein Datentyp in Mathematica sind zweidimensionale Bitmaps – Bilder.
Hier sind es handgeschriebene Ziffern.
Man kann diese in eine Menge (einen weiteren Datentyp) tun, und dann einem NetModel (dem Datentyp für ein Neurales Netz) vorwerfen.
Dieses spezielle NetModel gibt dann als Ausgabe die erkannten Ziffern.
Dies ist eine Anwendung eines NN als Classifier, also als ein Ding, das erkennen und gruppieren kann.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-09.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Auf ein Bitmuster einer gut lesbaren 2 wird sukzessive ein immer stärkerer Gaussian Blur angewendet.
Die resultierende Eingabemenge wird in das NetModel-Objekt verfüttert und man bekommt &amp;ldquo;2, 2, 2, 1, 1, 1, 1, 1, 1&amp;rdquo; als Ausgabe.
Das Modell bricht bei schlechten Daten irgendwann zusammen.
Als Mensch erkennt man mehr Ziffern korrekt als 2, aber am Ende wird es wirklich sehr unlesbar.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Wolfram macht dann weiter und diskutiert das Konzept der Nähe oder Ähnlichkeit, indem er mit Voronoi-Maps herumexperimentiert.
Er stellt sich jetzt die Aufgabe, eine bestimmte Funktion mit zwei Parametern x und y, die die Werte -1, 0 und +1 liefert,
zu &amp;ldquo;lernen&amp;rdquo; und durch ein neuronales Netz zu produzieren.
Er benutzt das, um zu zeigen, wie man Koeffizienten einer Näherungsfunktion findet,
indem man simple Geradengleichungsrechner (Neuronen) verknüpft.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-10.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Von einem NN erkannte handgeschriebene Ziffern nach Ähnlichkeit sortiert und angeordnet.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-11.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Eine Reihe von zufällig auf einer Ebene verteilte Punkte und die von ihnen &amp;ldquo;beherrschten&amp;rdquo; Gebiete:
Jeder Punkt der Ebene wird dem orangen Punkt zugeordnet, der ihm nach irgendeiner Distanzmetrik am nächsten ist.
Es entsteht eine Partition, eine Aufteilung, der Karte in Zonen um die Punkte.
Es sieht aus wie Seifenblasen, die aneinander kleben und gerade Schnittflächen bilden.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-12.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Die Aufgabe: Ein simples Voronoi, das drei Punkte mit den Werten +1, 0 und -1 auf einer Ebene verteilt.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-13.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Die 3D-Darstellung desselben Diagramms.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Er nimmt dann die vorherigen Gedanken zur Interpolation wieder auf.
Dazu erfindet er ohne weitere Erläuterung eine Aneinanderreihung von &amp;ldquo;Neuronen&amp;rdquo;, schreibt Koeffizienten dran,
und diskutiert verschiedene mögliche Abstandsberechnungsfunktionen (Fehler-Meßfunktionen).
Wenn man die Mathematik und den Prozess der Suche nach den passenden Koeffizienten mal wegläßt,
kommt man am Ende bei einer Näherung des Wunsch-Graphen an, die so aussieht.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-14.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Das von Wolfram gewählte neuronale Netz.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-16.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Das Resultat weicht vom Wunschergebnis ab, ist aber eine gute Näherung.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Er diskutiert und demonstriert dann durch Simulation, daß man bestimmte Dinge nicht darstellen kann,
wenn man nicht genügend Neuronen hat.
Das zu modellierende Subjekt ist zu kompliziert, sodaß der mögliche Parameterraum nicht groß genug ist:
Eine Gerade kann keine Parabel annähern.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-16.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Verschiedene NN und die resultierenden Näherungen.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Es geht dann in einen längeren Exkurs, in dem er die Anatomie der Netze zu erklären, die da verwendet werden.
Man erkennt, daß die Modelle Schichten haben und daß sie keine Rückflüsse haben:
Daten fließen immer von der oberen Schicht in die unmittelbar darunter liegende – es gibt keine Rückflüsse und auch kein Überspringen.&lt;/p&gt;
&lt;p&gt;Man erkennt auch, daß generell größere Modelle bessere Ergebnisse liefern.
Wolfram gibt zu, daß wir nicht wissen, wie groß ein NN für ein bestimmtes Problem sein muss.
Wir wissen auch nicht, wie die konkrete Verschaltung der Schichten aussehen sollte,
oder muss, um gute Ergebnisse zu liefern.&lt;/p&gt;
&lt;p&gt;Er versucht immerhin, die Inhalte der Schichten zu verstehen und ihnen Bedeutung zuzuweisen,
aber auch das ist nicht wirklich Wissenschaft und teilweise Kaffeesatzlesen.
Fast alles am Bau von NN ist &amp;ldquo;Erfahrung&amp;rdquo; und &amp;ldquo;Empirik&amp;rdquo; ohne viel Theorie dahinter.&lt;/p&gt;
&lt;p&gt;Wir können Daten aus Schichten von NN herausziehen und sie nach einer Abstandsfunktion sortieren lassen,
und die Anordnung ergibt für unsere menschlichen Hirne irgendwie Sinn.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-18.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hunde und Katzenbilder nach einer Abstandsfunktion eines NN sortiert.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Wir können auch die Neuronen einiger Schichten nehmen, die Koeffizienten in einer Bildmatrix anordnen,
und bekommen vereinfachte Bilder von den Dingen, die da gespeichert oder erkannt werden sollen.&lt;/p&gt;
&lt;p&gt;Aber wir haben keine Funktionstheorie oder Mathematik von NN und Kognition.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-19.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aus einer Schicht aus dem NN eines Katzen-Hunde-Sortierers herausgezogene Daten ergeben Katzenbilder.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Wolfram beschäftigt sich weiter mit dem Training von NN, der Datenbeschaffung und der Auszeichnung von Daten.
Das Korpus von Trainingsdaten muss groß und zuverlässig ausgezeichnet sein.
Wir brauchen also viele Katzenbilder (Dafür gibt es das Internet!) und Hundebilder,
und diese müssen von Menschen korrekt mit Tags versehen worden sein, damit ein NN das lernen kann.&lt;/p&gt;
&lt;p&gt;Er zeigt dann Verbesserungen der Koeffizienten und der Resultate über Iterationen des Lernprozesses.
&amp;ldquo;Lernen&amp;rdquo; bedeutet für ein NN, die Koeffizienten der verschiedenen Neuronen zu verändern.
Dazu nimmt man die Eingabe-Koeffizienten als &amp;ldquo;Ebene&amp;rdquo; und den Fehler als &amp;ldquo;Höhe&amp;rdquo;
und wandert dann in der so entstehenden Fehlerlandschaft auf einem zufälligen Pfad in die Täler.&lt;/p&gt;
&lt;p&gt;Das findet lokale Minima und man hofft, aus &amp;ldquo;lokalen Löchern&amp;rdquo; zu entkommen, indem man zufällige Schritte zufälliger Größen macht.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-20.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Die Berg- und Tallandschaft des Fehlergraphen und der Weg des Modells zum lokalen Minimum.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Wieder diskutiert Wolfram mögliche Layouts von NN –
nicht immer kann man jedes Neuron mit allen Neuronen der nächsten Lage verschalten und in der Tat ist das bei großen NN nie machbar.
Bestimmte Formen haben sich als besser lernfähig erwiesen:
Ein schmaler Layer der zwischen zwei breite Layers eingefügt wird scheint Ergebnisse stark zu verbessern.
Niemand weiß, warum das so ist oder hat ein Modell, das dies erklärt.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-21.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Verschiedene Formen von NN und die Ergebnisse, die sie produzieren.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Etwa in der Mitte des Textes wendet sich Wolfram generativen Modellen und LLMs zu.
Training von LLMs ist sehr, sehr einfach, weil sie einfach Text essen:
Man nimmt beliebigen Text, lässt das Modell das folgende Wort generieren und vergleicht es mit dem Wort, das hätte kommen müssen.
Daraus kann man Trainingsfeedback erzeugen.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-22.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Eine Karte, auf der englische Worte nach Ähnlichkeit angeordnet sind.
Säugetiere sind links oben, links unten sind &amp;ldquo;alligator&amp;rdquo; und &amp;ldquo;crocodile&amp;rdquo; dicht beieinander, &amp;ldquo;ant&amp;rdquo; und &amp;ldquo;bee&amp;rdquo; sind es auch.
Rechts unten Früchte.
Tiere und Früchte sind klar durch einen Graben getrennt.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Es entsteht dadurch sogar so etwas wie eine Wissens-Karte nach Ähnlichkeits-Metriken.
Das ist nicht so, weil das LLM ein Weltmodell hat.
Sondern es ist so, weil Menschen ein Weltmodell haben und wenn sie Worte gebrauchen,
bestimmte Worte &amp;ldquo;nach beieinander&amp;rdquo; in Sätzen angeordnet sind.
Ein Weltverständnis entsteht so nicht, ein gewisses Sprachverständnis wohl schon.&lt;/p&gt;
&lt;p&gt;Wieder und wieder und wieder versucht Wolfram zu einem Verständnis der Koeffizienten von LLMs oder NNs zu kommen,
oder zu irgendeiner Theorie, die die Funktionsweise von LLMs und NNs planbar und vorhersagbar macht und sich über pure Empirik erhebt.
Er macht dies mit verschiedenen Visualisierungen von Koeffizienten auf die unterschiedlichsten Weisen,
aber er findet nur quälende Muster, die wie Inhalt wirken und nichts enthüllen.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2024/02/chatgpt-23.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Eine Vergröberung einer ChatGPT Koeffizientenmatrix und Einfärbung nach bestimmten Werten erzeugt eine interessante Teal-and-Orange Batik,
die nichts bedeutet.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Am Ende geht es dann darum,
wie man mit solchen riesigen Koeffizienten-Matrizen und einem austrainierten Modell generativ arbeitet und &amp;ldquo;interessante&amp;rdquo; Texte erzeugt.
Dabei geht es auch um den Parameter der &amp;ldquo;Temperatur&amp;rdquo; und den &amp;ldquo;Seed&amp;rdquo;:&lt;/p&gt;
&lt;p&gt;Würde man ein generatives LLM mit einer Null-Temperatur laufen lassen,
würde es immer dasselbe, wahrscheinlichste Folgetoken wählen, um einen Satz zu komplettieren.
Indem man die Wahrscheinlichkeiten der Folgetoken sortiert,
und dann zufällig ein anderes wahrscheinliches Token wählt, bekommt man immer noch &amp;ldquo;gültige&amp;rdquo; Fortsetzungen, die aber variabel sind.&lt;/p&gt;
&lt;p&gt;Das Seed ist dabei der Startwert des Pseudo-Zufallszahlen-Generators,
die Temperatur die &amp;ldquo;Sprungweite&amp;rdquo; vom wahrscheinlichsten Wert weg.
Auch hierzu gibt es keine Theorie, wir wissen nur, daß &amp;ldquo;0.8 ein guter Wert ist&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Wolfram schließt:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So … What Is ChatGPT Doing, and Why Does It Work?&lt;/p&gt;
&lt;p&gt;The basic concept of ChatGPT is at some level rather simple.
Start from a huge sample of human-created text from the web, books, etc.
Then train a neural net to generate text that’s “like this.”
And in particular, make it able to start from a “prompt” and then continue with text that’s “like what it’s been trained with”.&lt;/p&gt;
&lt;p&gt;As we’ve seen, the actual neural net in ChatGPT is made up of very simple elements — though billions of them.
And the basic operation of the neural net is also very simple, consisting essentially
of passing input derived from the text it’s generated so far “once through its elements” (without any loops, etc.)
for every new word (or part of a word) that it generates.&lt;/p&gt;
&lt;p&gt;But the remarkable—and unexpected—thing is that this process can produce text that’s successfully “like” what’s out there on the web,
in books, etc.
And not only is it coherent human language, it also “says things” that “follow its prompt” making use of content it’s “read”.
It doesn&amp;rsquo;t always say things that “globally make sense”
(or correspond to correct computations because it’s just saying things that “sound right” based on what things “sounded like” in its training material).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Das ist der Stochastische Papagei, von dem bei LLMs gerne mal die Rede ist.
Er ist wortwörtlich gemeint.&lt;/p&gt;
&lt;p&gt;Ein LLM reproduziert Versatzstücke aus Texten, die es verarbeitet hat,
auf der Basis von Auftretenswahrscheinlichkeiten von &amp;ldquo;großes n&amp;rdquo;-Grammen von chinesischen Schriftzeichen (&amp;ldquo;Token&amp;rdquo;).
n ist dabei gerne groß zweistellig.&lt;/p&gt;
&lt;p&gt;Ein Konzept von Korrektheit gibt es nicht.
Wir wissen auch nicht, wie es das geben kann, denn es gibt keine Theorie.&lt;/p&gt;
&lt;p&gt;Die Idee, daß ein LLM in &amp;ldquo;full autopilot&amp;rdquo; Mode irgendwas Funktionales in irgendeiner Gesellschaft leisten kann – Arbeiten kann – ist lächerlich.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Es reproduziert und würfelt.&lt;/li&gt;
&lt;li&gt;Es hat kein Weltmodell.&lt;/li&gt;
&lt;li&gt;Es kennt keine Grenzen in den Termen von Ethik, Moral, Physik oder Logik.&lt;/li&gt;
&lt;li&gt;Es reiht nur Token aneinander.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ein LLM kann Menschen Arbeit abnehmen, indem es bestimmte Standardfunktionen erleichtert,
und es ist wahrscheinlich ein ausgezeichneter Sprachübersetzer,
und sicher ein wunderbarer Imitator, denn das ist, als was es konstruiert ist.&lt;/p&gt;
&lt;p&gt;Aber Imitation schafft nicht.&lt;/p&gt;
&lt;p&gt;Die Redewendung des &amp;ldquo;Blurry JPEG of the Internet&amp;rdquo; ist wieder exakt zutreffend,
denn das ist, was der Konstruktionsprozeß eines LLM produziert – eine verlustbehaftete Kompression von Wort-Folge-Wahrscheinlichkeiten.&lt;/p&gt;
&lt;p&gt;In meiner beruflichen Karriere habe ich um mich herum eine Reihe von KI-Projekten in kommerzieller Anwendung gelingen und scheitern sehen.
Das Muster, daß sich dabei ergeben hat, bestätigt sich auch bei LLMs:&lt;/p&gt;
&lt;p&gt;Wann immer meine Arbeitgeber KI eingesetzt haben, um Menschen full-auto zu ersetzen ist das Projekt ein unmitigiertes Desaster gewesen,
das der Firma geschadet und den Kunden Leid zugefügt hat.&lt;/p&gt;
&lt;p&gt;Wann immer meine Arbeitgeber KI eingesetzt haben um Menschen zu augmentieren war das Projekt eine Effizienzsteigerung
um eine halbe Größenordnung (den Faktor 3-5), die durch Feedback menschlicher Entscheidungen in den Trainingskorpus noch verbessert werden konnte.
Dabei war oft entscheidend, Confidence-Werte (aus der Modell-Fehler-Metrik abgeleitete Werte) herauszuführen und für Menschen sichtbar zu machen.&lt;/p&gt;
&lt;p&gt;Die Grundidee dahinter ist, daß wir eine Maschine bauen, die wir zwingen eine Entscheidung zu treffen, egal wie groß die Fehlermetrik ist.
Wenn die Maschine sich sicher ist, daß ein Standardfall in einer oder ander Weise vorliegt, ist das wahrscheinlich gut.
Wenn die Maschine aber eine hohe Fehlermetrik anzeigt, dann erzwingen wir eine Entscheidung, die wahrscheinlich zufällig ist.&lt;/p&gt;
&lt;p&gt;Hier ist es gut, den Fall herauszunehmen und Menschen vorzulegen.
Anders als die KI haben diese ein Weltmodell, und insbesondere ein Modell der ganzen Welt inklusive all der Informationsquellen,
Kontexte und Eskalationsstufen, die dem KI-Modell nicht zugänglich sind.
Menschen können Dinge auf Weisen kontextualisieren und umordnen, die einem Reproduktions-Papagei nicht möglich sind.
Und diese Menschen sind glücklicher, schneller und effektiver, wenn sie mit den Standardfällen nichts zu tun haben.&lt;/p&gt;
&lt;p&gt;Wir können KI erfolgreich einsetzen, wenn wir es nach diesem Modell tun.&lt;/p&gt;
&lt;p&gt;Wir können die Hölle auf Erden erschaffen, indem wir Menschen einer unkontrollierten Papagei-KI aussetzen,
die nach zufällig aufgerufenen Reproduktionen vorheriger Fälle entscheidet, ohne Ausweg.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
