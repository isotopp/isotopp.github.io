<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Filesystems on Die wunderbare Welt von Isotopp</title>
    <link>https://blog.koehntopp.info/tags/filesystems.html</link>
    <description>Recent content in Filesystems on Die wunderbare Welt von Isotopp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Apr 2024 13:48:25 +0000</lastBuildDate><atom:link href="https://blog.koehntopp.info/tags/filesystems/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Invalid-UTF8 vs the filesystem</title>
      <link>https://blog.koehntopp.info/2023/09/14/invalid-utf8-vs-the-filesystem.html</link>
      <pubDate>Thu, 14 Sep 2023 01:02:03 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2023/09/14/invalid-utf8-vs-the-filesystem.html</guid>
      <description>&lt;p&gt;A UNIX filename can contain arbitrary bytes in an arbitrary sequence, with two exceptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It cannot contain NUL (&lt;code&gt;\0&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;It cannot contain slash (&lt;code&gt;/&lt;/code&gt;), because that is the directory separator.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But will all filesystems accept such filenames?
And how will this work with languages such as Python, which require all strings to be valid &lt;code&gt;utf-8&lt;/code&gt; and
which declare the filesystem interface to accept and return strings?&lt;/p&gt;
&lt;h1 id=&#34;a-test-program&#34;&gt;
    &lt;a href=&#34;#a-test-program&#34;&gt;
	A test program
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s check.
Here is a small C program, based on &lt;a href=&#34;https://stackoverflow.com/questions/1301402/example-invalid-utf8-string&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this stackoverflow article&lt;/a&gt;

.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;stdio.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;sys/stat.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;unistd.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;examples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xc3\xb1&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xc3\x28&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xa0\xa1&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xe2\x82\xa1&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xe2\x28\xa1&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xe2\x82\x28&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xf0\x90\x8c\xbc&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xf0\x28\x8c\xbc&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xf0\x90\x28\xbc&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xf0\x28\x8c\x28&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xf8\xa1\xa1\xa1\xa1&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\xfc\xa1\xa1\xa1\xa1\xa1&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;FILE&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;mkdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;keks&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mo&#34;&gt;0755&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// don&amp;#39;t care if exists
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;chdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;keks&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;       &lt;span class=&#34;c1&#34;&gt;// don&amp;#39;t care if succeed.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;examples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nf&#34;&gt;printf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;%s&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;examples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;fp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;fopen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;examples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nf&#34;&gt;printf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;unable to open %d (%s)!&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;examples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;continue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nf&#34;&gt;fprintf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;keks&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nf&#34;&gt;fclose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;xfs&#34;&gt;
    &lt;a href=&#34;#xfs&#34;&gt;
	XFS
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;When running this on a Linux system with XFS, this works:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/09/invalid-utf8-xfs.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The program runs and generates all files. Even those with byte sequences that are not valid utf-8.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This works the same with ext4 on Linux.&lt;/p&gt;
&lt;h2 id=&#34;zfs&#34;&gt;
    &lt;a href=&#34;#zfs&#34;&gt;
	ZFS
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;When running this on a Linux system with ZFS, filenames containing invalid utf-8 sequences are rejected.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/09/invalid-utf8-zfs.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The program runs and generates files with valid utf-8 names. The &lt;code&gt;open(2)&lt;/code&gt; syscall fails for names with invalid utf-8 filenames.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;apfs&#34;&gt;
    &lt;a href=&#34;#apfs&#34;&gt;
	APFS
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;When running this on a MacOS Ventura system with APFS, filenames containing invalid utf-8 sequences are rejected.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/09/invalid-utf8-apfs.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The program runs and generates files with valid utf-8 names. The &lt;code&gt;open(2)&lt;/code&gt; syscall fails for names with invalid utf-8 filenames.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;unpacking-invalid-utf-8-filenames-with-python&#34;&gt;
    &lt;a href=&#34;#unpacking-invalid-utf-8-filenames-with-python&#34;&gt;
	Unpacking invalid utf-8 filenames with Python
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;I went and creates a &lt;code&gt;tar&lt;/code&gt; archive of the files generated on XFS and copied it over to my Mac.&lt;/p&gt;
&lt;p&gt;The following test program was used to unpack the &lt;code&gt;tar&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ch&#34;&gt;#! /usr/bin/env python&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tarfile&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TarFile&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;chardet&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TarFile&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;test.tar&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;names&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;getnames&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;bname&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;bytearray&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;raw&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chardet&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;detect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The test run fails:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;(venv) $ python tarnames.py
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Traceback (most recent call last):
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;  File &amp;#34;.../tarnames.py&amp;#34;, line 15, in &amp;lt;module&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;    bname = bytearray(name, &amp;#39;raw&amp;#39;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;            ^^^^^^^^^^^^^^^^^^^^^^
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;LookupError: unknown encoding: raw
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;intermediate-result&#34;&gt;
    &lt;a href=&#34;#intermediate-result&#34;&gt;
	Intermediate result
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;It probably is useful to restrict POSIX further and demand that filenames are always valid utf-8.
But that is not what the standard currently says, and also not what all filesystems guarantee.
And it can lead to unexpected behavior.&lt;/p&gt;
&lt;p&gt;Also, some programming languages such as Python demand of strings that they have valid utf-8 encoding,
but use filenames and strings equivalently.&lt;/p&gt;
&lt;p&gt;That can lead to weird behavior.&lt;/p&gt;
&lt;h1 id=&#34;further-research&#34;&gt;
    &lt;a href=&#34;#further-research&#34;&gt;
	Further research
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Apparently there is a function
&lt;a href=&#34;https://docs.python.org/3/library/sys.html#sys.getfilesystemencoding&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;sys.getfilesystemencoding()&lt;/code&gt;&lt;/a&gt;


without parameters.
Python seems to assume that all filesystems have the same encoding and that it is not path dependent.&lt;/p&gt;
&lt;p&gt;Apparently there are &lt;code&gt;os.fsencode()&lt;/code&gt; and
&lt;a href=&#34;https://docs.python.org/3/library/os.html#os.fsencode&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;os.fsdecode()&lt;/code&gt;&lt;/a&gt;

.&lt;/p&gt;
&lt;h2 id=&#34;modified-python-code&#34;&gt;
    &lt;a href=&#34;#modified-python-code&#34;&gt;
	Modified Python code
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;We are using &lt;code&gt;os.fsencode()&lt;/code&gt; to get &lt;code&gt;bytes&lt;/code&gt; from the filesystem interface.
We are then using &lt;code&gt;chardet.detect()&lt;/code&gt; on this byte-string and check the guesses.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ch&#34;&gt;#! /usr/bin/env python&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tarfile&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TarFile&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;chardet&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TarFile&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;test.tar&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;names&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;getnames&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;bname&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fsencode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bname&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;=}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chardet&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;detect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bname&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The result:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;ascii&amp;#39;, &amp;#39;confidence&amp;#39;: 1.0, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/a&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;ascii&amp;#39;, &amp;#39;confidence&amp;#39;: 1.0, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xc3\xb1&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;ISO-8859-1&amp;#39;, &amp;#39;confidence&amp;#39;: 0.73, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xc3(&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;ISO-8859-1&amp;#39;, &amp;#39;confidence&amp;#39;: 0.73, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xa0\xa1&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;IBM866&amp;#39;, &amp;#39;confidence&amp;#39;: 0.99, &amp;#39;language&amp;#39;: &amp;#39;Russian&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xe2\x82\xa1&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;Windows-1252&amp;#39;, &amp;#39;confidence&amp;#39;: 0.73, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xe2(\xa1&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;ISO-8859-1&amp;#39;, &amp;#39;confidence&amp;#39;: 0.73, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xe2\x82(&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;Windows-1252&amp;#39;, &amp;#39;confidence&amp;#39;: 0.73, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xf0\x90\x8c\xbc&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;Windows-1254&amp;#39;, &amp;#39;confidence&amp;#39;: 0.48310298982778344, &amp;#39;language&amp;#39;: &amp;#39;Turkish&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xf0(\x8c\xbc&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;Windows-1252&amp;#39;, &amp;#39;confidence&amp;#39;: 0.73, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xf0\x90(\xbc&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;Windows-1254&amp;#39;, &amp;#39;confidence&amp;#39;: 0.5521177026603239, &amp;#39;language&amp;#39;: &amp;#39;Turkish&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xf0(\x8c(&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;Windows-1252&amp;#39;, &amp;#39;confidence&amp;#39;: 0.73, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xf8\xa1\xa1\xa1\xa1&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;ISO-8859-1&amp;#39;, &amp;#39;confidence&amp;#39;: 0.73, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;bname=b&amp;#39;keks/\xfc\xa1\xa1\xa1\xa1\xa1&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;{&amp;#39;encoding&amp;#39;: &amp;#39;ISO-8859-1&amp;#39;, &amp;#39;confidence&amp;#39;: 0.73, &amp;#39;language&amp;#39;: &amp;#39;&amp;#39;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The guesses are trying to find a valid charset for the bytestring, and they find the lowest (least large) charset that matches.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Früher hatten wir Festplatten...</title>
      <link>https://blog.koehntopp.info/2023/08/09/frueher-hatten-wir-festplatten.html</link>
      <pubDate>Wed, 09 Aug 2023 01:02:03 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2023/08/09/frueher-hatten-wir-festplatten.html</guid>
      <description>&lt;p&gt;Früher hatten wir Festplatten.
Das ist ein Stapel Glasplatten, die mit Eisenoxid beschichtet sind, gleichmäßig schnell rotieren
und auf dem wir mit einem Kamm voller Schreib-Lese-Köpfe Teile magnetisieren oder die Magnetspuren auslesen.
Platten hatten CHS-Adressen für Sektoren, also Cylinder (drinnen, weiter draußen), Head (Scheibe) und Sektor (also welches Stück Scheibe).
Diese CHS-Adressen waren für das Betriebssystem sichtbar und das hat damit optimiert.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/08/disk.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Platten wurden dann schnell kompliziert.
Der Radius einer Spur auf einer drehenden Platte wächst, wenn man die Köpfe nach draußen schiebt.
Also passen draußen mehr Sektoren darauf.&lt;/p&gt;
&lt;p&gt;Das bedeutet aber, daß das ganze CHS-System so nicht mehr funktioniert,
außer das OS kennt die spezielle und variable Geometrie der Platte.
Stattdessen hat die Platte einen eigenen Computer, den Controller, die Elektronik an der Unterseite der Platte.
Der kennt die Geometrie.
Das OS nicht.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/08/disk-02.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Für das OS ist die Platte nun sehr viel einfacher: ein Array aus Sektoren.
Diese auf die wie auch immer geartete Plattengeometrie abzubilden ist Sache des Controllers,
der physisch mit der Platte verbunden ist.&lt;/p&gt;
&lt;p&gt;Wir bekommen &amp;ldquo;Lineare Block Addressen&amp;rdquo;, LBA.
Viele OS kannten am Anfang gar keine LBA.
Daher geben Platten eine Fake-Geometrie raus,
die aufgeht und darstellbar ist &amp;ndash; und unsinnig ist.
255 Cylinders, 63 sectors/track –– von einer SSD.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;root@server:/run/user/1000# fdisk -l -u=cylinders /dev/sde
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Disk /dev/sde: 3.64 TiB, 4000787030016 bytes, 7814037168 sectors
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Disk model: Samsung SSD 860
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Geometry: 255 heads, 63 sectors/track, 486401 cylinders
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Units: cylinders of 16065 * 512 = 8225280 bytes
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Sector size (logical/physical): 512 bytes / 512 bytes
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;I/O size (minimum/optimal): 512 bytes / 512 bytes
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Disklabel type: gpt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Disk identifier: A5C7EF27-A7B1-4E84-AFC8-24FC1DB1C7FB
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;go&#34;&gt;Device     Start    End  Size Type
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;/dev/sde1      1 486402  3.6T Linux LVM
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Viele Leute hätten gerne, aus was für Gründen auch immer, mehr als eine Festplatte - auch wenn sie nur eine haben.
Zum Beispiel um mehr als ein Betriebssystem zu installieren oder um sonst wie Dinge aufzuteilen.
Die Lösung war die &amp;ldquo;Partition&amp;rdquo;.
Das ist eine Gruppe zusammenhängender Cylinder, die vom System als &amp;ldquo;eine logische Festplatte&amp;rdquo; angesehen werden.
Man schließt also sda an, und sieht bis zu 4 Geräte: sda1, sda2, sda3 und sda4.&lt;/p&gt;
&lt;p&gt;Aus Gründen, die mit Microsoft und MS-DOS zu tun haben, gingen nur 4 Partitionen –
die Partitionstabelle am Anfang der Platte war ein festes Array mit genau 4 Einträgen.
Es sind auch immer 4, nur manche sind als unbenutzt markiert und werden dann stillschweigend ignoriert.&lt;/p&gt;
&lt;p&gt;Wenn man nun mehr als 4 haben will?
Sehr einfach:
macht man sich eine &amp;ldquo;Erweiterte Partition&amp;rdquo;, die den ganzen Rest der Platte abdeckt und kann darin dann weitere Partitionen einrichten.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/08/disk-03.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Das ist dann eine lineare Liste.
Die Nummern der logischen Partitionen in der erweiterten Partition fangen immer bei 5 an.
Man kann also eine Bootpartition sda1 haben und eine erweiterte Partition sda2, die dann sda5, sda6 und sda7 enthält.
Kein normal denkender Mensch will das so, weil das auch nervt:
Partitionen müssen ja zusammenhängend sein und auf eine Platte passen.
Man kann sie also nur dann erweitern, wenn hinter ihnen freier Platz ist.&lt;/p&gt;
&lt;p&gt;Ist da keiner, muss man den ganzen Rest der Platte &amp;ldquo;weiter nach hinten&amp;rdquo; kopieren und dann die Partition der Wahl erweitern.
Das kann sehr, sehr weh tun.
Was man stattdessen macht:&lt;/p&gt;
&lt;p&gt;Man tut alle seine Platten in einen Sack.
Diesen Sack nennen wir Volume Group, VG.
Die einzelnen Platten, die Physical Volumes, PVs, schneiden wir in gleich große Scheiben, die Physical Extents, PEs.
Die VG ist also ein Sack voll PEs, alle gleich groß, die auf unterschiedlichen Platten liegen.&lt;/p&gt;
&lt;p&gt;Man will so um die 1000 bis 10.000 PEs in seiner VG haben.
Dann hantiert man mit Platz in Units von 1 Promille bis 0.1 Promille &amp;ndash; das ist genau genug.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/08/disk-04.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Wir können uns so Logical Volumes, also Laufwerke, zusammenbauen, die ein ganzes Vielfaches eines Extents groß sind und die aus LEs bestehen, Logical Extents.
Jedem LE wird mindestens ein PE als Backing Storage zugewiesen, aber da dies über eine Lookup-Tabelle passiert, können die PEs von irgendwoher kommen.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/08/disk-05.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Das Subsystem in Linux, das das macht, heißt LVM2, der Logical Volume Manager.
Er kann noch mehr:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Es ist möglich, einem LE mehr als ein PE zuzuweisen. Dann hat man ein RAID 1.&lt;/li&gt;
&lt;li&gt;Es ist möglich, die Blöcke aus zwei PE auf unterschiedlichen PVs zu nehmen und blockweise abwechselnd zu benutzen. Dann hat man ein RAID 0.&lt;/li&gt;
&lt;li&gt;Mithilfe der Device Mapper Layer, die unterhalb von LVM Dienst tut, kann man so jede Menge absurde Storage Artistik abziehen.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Für uns ist wichtig zu verstehen, daß Partitionen in LVM – wegen LVM! – nicht mehr zusammenhängend sein müssen.&lt;/p&gt;
&lt;p&gt;Es ist 2023.
Die Leute benutzen noch Devices, Partitionen und LVM, aber niemand benutzt noch Festplatten.
Alles ist SSD, eventuell sogar ohne beknacktes SATA-Interface, sondern mit dem Flash direkt auf dem PCIe-Bus.
Das ist dann NVME.&lt;/p&gt;
&lt;p&gt;Das ist super, weil das S in SATA für Seriell steht, also auch Zugriffe auf das Gerät serialisiert.
Das Gerät hat aber einen Haufen Chips, die man alle parallel anblasen könnte,
wenn das SSSSSSSATA nicht Zugriffe sssssserialsierte.
Also weg damit!&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/08/disk-06.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Flash Storage hat nun noch ein anderes Problem.
Flash beschreibt &amp;ldquo;gelöschte&amp;rdquo; Blöcke in Sektoren von 512 Byte oder – neuerdings - 4096 Bytes.&lt;/p&gt;
&lt;p&gt;Flash kann solche Sektoren aber nicht löschen, nur als &amp;ldquo;gelöscht&amp;rdquo; markieren.
Die eigentliche Löschung dauert recht lange und passiert in recht großen Erase Segmenten, die viele Sektoren enthalten.&lt;/p&gt;
&lt;p&gt;Daher gibt es einen Hintergrundprozeß, der auf dem Controller läuft und guckt, ob valide Daten umsortiert werden können, sodaß ganze Erase Segmente frei werden.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/08/disk-07.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Dieser Hintergrundprozeß ist die Flash Translation Layer (FTL).&lt;/p&gt;
&lt;p&gt;Sie hat eine große Zuordnungstabelle, die die eingehende LBA vom Computer auf eine physikalische Blockadresse im Flash mapped.
Und die FTL kann im Hintergrund Blöcke verschieben, umsortieren oder reorganisieren, ohne daß das vorne an der LBA sichtbar wäre.
Das hat eine Reihe von Konsequenzen.
Eine ist, daß man eine SSD nicht zum Löschen überschreiben kann –
die FTL sortiert hinten lustig Kram hin und her und es ist nicht gesagt,
dass ein Durchschreiben von LBA 0 nach LBA n alle Daten auf der SSD überschreibt, oder daß Allgemein nichts Lesbares mehr auf der SSD steht.&lt;/p&gt;
&lt;p&gt;Daher gibt es bei vielen SSD eine Hardware Drive Encryption,
und zum Löschen des Laufwerkes geben wir der Disk einfach das Kommando, den Key zu vergessen.
Damit ist die faktisch gelöscht.&lt;/p&gt;
&lt;p&gt;Oder wäre es, &lt;a href=&#34;https://www.bleepingcomputer.com/news/security/flaws-in-popular-ssd-drives-bypass-hardware-disk-encryption/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;könnte man der Implementierung der Hardware Drive Encryption vertrauen&lt;/a&gt;

.
Eventuell ist man mit so was wie LUKS und dem Wegwerfen der LUKS-Schlüssel besser dran als mit der Hardware Verschlüsselung von Drive-Controllern aus Chinesium.
Aber in jedem Fall ist man mit einem verschlüsselten Flash Drive besser dran als mit &amp;ldquo;Überschreiben&amp;rdquo;.
Oder man etabliert einen Prozess,
bei dem jeder Storage in einem Server in unseren RZ dies nur noch durch einen Shredder verlassen kann.
Falls man seinen Leuten vertraut, dass sie nicht doch mal ein Drive aus der Schrottkiste mitnehmen.&lt;/p&gt;
&lt;p&gt;Eine bemerkenswerte Eigenschaft von Flash sind &amp;ldquo;Seeks&amp;rdquo;, also Datentransfers zwischen nicht physisch aufeinander folgenden Blöcken.
Disks schaffen so um die 200 Disk Seeks pro Sekunde, und die tatsächlichen Zeiten variieren auch stark – je Seek, desto Wait.
Bei SATA/SAS SSD haben wir circa 20.000 Seeks pro Sekunde und bei NVME bis zu 800.000 davon (weniger, je mehr Bits pro Flash-Zelle).&lt;/p&gt;
&lt;p&gt;Wenn Ihr bei der FTL aufgepasst habt,
dann wißt ihr, dass aufeinanderfolgende LBA eventuell gar keine aufeinanderfolgenden PBA sind,
also im Flash nicht nebeneinander liegen müssen.
Das merkt niemand, weil die Seek Zeiten bei Flash uniform sind.
Aber Moment mal &amp;hellip;&lt;/p&gt;
&lt;p&gt;Wir haben uniforme Seekzeiten und wir haben in der FTL eine Lookup-Tabelle die verdammt ähnlich aussieht wie die Lookup-Tabelle in LVM.
Können wir dann nicht auch Nichtzusammenhängende Partitionen in Hardware bekommen?
Können wir. Jedenfalls ein paar.
Das sind NVME Namespaces, also die &amp;ldquo;nX&amp;rdquo; an Euren /dev/nvme1n1-Devices.&lt;/p&gt;
&lt;p&gt;Die kann man mit dem &amp;ldquo;nvme&amp;rdquo; Kommando einrichten und abreißen und ihnen auch Platz zuweisen.
Der liegt dann irgendwo und die FTL weiß, wo genau.
Aber zusammenhängend ist das nicht, auch wenn Ihr das nie sehen werdet.
NVME Namespaces sind also so eine Art Hardware LVM mit einer limitierten Anzahl Plätzen.&lt;/p&gt;
&lt;p&gt;Wie Hardware ist mein Hardware LVM denn?
Nun.
Mit SR-IOV bekommt jede einer Hardware-Nichtpartitionen auch ein Haufen Fake-PCI-Register,
die ich in den Adressraum etwa einer VM mappen könnte, sodaß effektiv jede VM eine &amp;ldquo;Partition&amp;rdquo; meiner NVME als fast echtes PCIe Busdevice sehen kann.&lt;/p&gt;
&lt;p&gt;Leider gerät an dieser Stelle die Implementierung ins Zielfeld,
und teilt uns mit, daß das zwar schnell ist, aber leider nicht dicht,
und man darauf also nicht unbedingt vertrauen sollte.
Aber schön wäre es gewesen!&lt;/p&gt;
&lt;p&gt;Wie dem auch sei:
Jetzt haben wir dieselbe Idee also&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;einmal in einem ARM-Rechner, der am Flash jenseites des PCIe Busses klebt – dem Flash Controller mit der FTL, als NVME Namespaces.&lt;/li&gt;
&lt;li&gt;Und dann noch einmal eine Ebene höher, diesseits des PCIe, als LVM2 unter dem Filesystem.&lt;/li&gt;
&lt;li&gt;Und dann noch einmal eine Ebene höher, im Dateisystem, jedenfalls wenn es APFS oder ZFS ist, auf Dateisystem und Datei-Ebene.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dreimal dieselbe Idee, mit einer Lookup-Tabelle, Namespaces und dem Zusammenkleben von Blocks.
Außer, wenn man Apple ist.
Dann baut man seine Hardware selbst, realisiert den Flashcontroller in denselben Die wie die CPU oder lässt ihn gleich als Teil der CPU laufen,
schließlich ist es sowieso ein System-on-a-Chip.
Und redet dann mit den Flash-Chips neben der CPU.
Die sind eh nur neben dem Die mit der CPU, weil sie dort noch nicht hineinpassen.&lt;/p&gt;
&lt;p&gt;Und dann kann man diese Ebenen zumindest teilweise integrieren, einige Indirektionen sparen und generell schneller sein.
Das setzt aber natürlich voraus, daß man seine Hardware komplett kontrolliert, sodaß die Komponenten aufeinander abgestimmt werden können.&lt;/p&gt;
&lt;p&gt;Außer Apple können das nur zwei Hände voll andere Firmen auf diesem Planeten,
und die anderen sind alle Cloudbetreiber, die ihre Hardware oder gar – wie Amazon-Annapurna-ARM – ihre CPUs selbst fertigen können.
Dann gehen auf einmal ein Haufen Abkürzungen und man kann so Sachen wie kaputte Hypervisoren und SR-IOV direkt selbst fixen.
Und das ist, warum Amazon schöne Dinge haben kann, aber ein &amp;ldquo;Private Cloud&amp;rdquo; Openstack nicht.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/08/disk-08.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jan schreibt: &amp;ldquo;Was Betriebssysteme noch nicht verstanden haben, ist das es nur noch Memory gibt.
In unterschiedlichen Geschwindigkeitsklassen.
Und Anbindungen.
Lokal, Netzwerk.
Das ist der einzige Unterschied.
Es gibt nur noch wenige Fälle.
Memory ist da oder nicht.
Latenz ist close to zero oder nicht deterministisch.
Deal with it.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In Unix ist ja alles entweder ein File, also persistent, und via open/read/write/seek/tell/close zu bearbeiten.
Oder ephemeral, und via malloc zu bekommen.&lt;/p&gt;
&lt;p&gt;Speicher, so wie ihn Unix kennt, hat keine Eigenschaften (ephemeral, persistent), Granularität (byte, page welcher Größe auch immer),
und auch keine Eigner und Zugriffsrechte, sowie keine Verzeichnisdienste.
Die Infrastruktur, die ein Dateisystem bereitstellt, ist ja weit mehr als nur die open/read/write/close API.&lt;/p&gt;
&lt;p&gt;Es ist auch der Verzeichnisbaum und die an den Dateien klebenden Zugriffsrechte, Flags, xattr und was es sonst noch so gibt, und eine bestimmte Locking-API.
Speicher hat zwar auch Locks, aber andere, und kennt die Konzepte Eigner, hierarchischer Namespace und was da sonst noch so existiert nicht.
Sun hat mal eine Zeit lang mit &lt;a href=&#34;https://en.wikipedia.org/wiki/Transactional_memory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wilden Dingen&lt;/a&gt;

 herumgespielt, die Teile dieses Problemraumes adressiert haben.
HP auch, mit &lt;a href=&#34;https://www.nextplatform.com/2019/09/09/inside-hpes-gen-z-switch-fabric/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;einem anderen Teilaspekt&lt;/a&gt;

 dieser Sache.
In beiden Fällen geht es um einen Haufen CPUs, die gemeinsam auf einem Speicher herumdengeln, der vielleicht oder vielleicht auch nicht lokal ist.
Das löst aber jeweils nur einen Teilaspekt des Problems.
Und es gibt keinerlei Software, die damit um könnte.&lt;/p&gt;
&lt;p&gt;In Unix.
Anderswo gibt es schon seit den späten 80er Jahren Rechner mit einer Art &amp;ldquo;hardware unabhängigem Bytecode&amp;rdquo;,
64 Bit Uniform Address Space und einer Art speicherbasierten, transaktionsorientierten Datenbank, Namespaces und Zugriffsrechten.
Das ist dann aber kein Unix, sondern etwas anderes, bizarres, außerirdisches, das keiner von Euch je gesehen hat.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/IBM_AS/400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alien Systems/400&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;Das ist das, was man bekommt, wenn man eine JVM erfindet,
bevor es Java gibt und wenn man persistenten Speicher mit RAM und &amp;ldquo;mmap() aber in richtig&amp;rdquo; und wie eine Datenbank riechend erfindet.
AS/400 ist irgendeine CPU – das wurde in der Zwischenzeit auch mehrfach geändert und keiner hat es gemerkt – und
irgendwelcher Speicher (viel weniger als 64 Bit, aber das hat auch keiner gemerkt, das RAM ist ja nur Cache).
Statt &amp;ldquo;everything is a file&amp;rdquo; &amp;ldquo;everything is an object&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;In der Zwischenzeit hat Intel Optane wieder eingestellt.
Das Pricing war falsch, aber vor allen Dingen hat keiner das richtig nutzen können.
Zu RAM für ein Filesystem, und zu persistent für RAM – meh, was soll ich damit?
Wenn alles entweder ephemeral oder ein File ist, dann ist &amp;ldquo;persistent, und so schnell wie RAM&amp;rdquo; irgendwie nicht zu gebrauchen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>50 years in filesystems: towards 2004 – LFS</title>
      <link>https://blog.koehntopp.info/2023/05/17/50-years-in-filesystems-towards-2004-lfs.html</link>
      <pubDate>Wed, 17 May 2023 01:02:03 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2023/05/17/50-years-in-filesystems-towards-2004-lfs.html</guid>
      <description>&lt;p&gt;This is part 5 of a series.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/05/50-years-in-filesystems-1974.html&#34;&gt;1974&lt;/a&gt;

&amp;rdquo; on the traditional Unix Filesystem.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/06/50-years-in-filesystems-1984.html&#34;&gt;1984&lt;/a&gt;

&amp;rdquo; on the BSD Fast File System.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/12/50-years-in-filesystems-1994.html&#34;&gt;1994&lt;/a&gt;

&amp;rdquo; on SGI XFS.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/15/50-years-in-filesystems-vnodes.html&#34;&gt;Vnodes&lt;/a&gt;

&amp;rdquo; on how to have multiple filesystems in Unix.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Progress is sometimes hard to see, especially when you have been part of it or otherwise lived through it.
Often, it is easier to see by comparing modern educational material and the problems discussed with older material.
Or look for the research papers and sources that fueled the change. So this is what we do.&lt;/p&gt;
&lt;h1 id=&#34;frontiers-in-the-nineties&#34;&gt;
    &lt;a href=&#34;#frontiers-in-the-nineties&#34;&gt;
	Frontiers in the Nineties
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;SGI&amp;rsquo;s XFS is pretty much the culmination point in filesystem technology for anything that does in-place updates.
Extents, generous usage of B+-trees and lock splitting across allocation groups make it a great filesystem that is fast and scales well.
The introduction of a metadata log allows it to recover quickly.&lt;/p&gt;
&lt;p&gt;The Nineties were also busy with operating systems research.
Specifically, cluster operating systems were very much en vogue:
Tanenbaum was in Amsterdam, busy with &lt;a href=&#34;https://en.wikipedia.org/wiki/Amoeba_%28operating_system%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amoeba&lt;/a&gt;

.
Bell Labs was busy with &lt;a href=&#34;https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Plan 9&lt;/a&gt;

.
And at the UCB, Ousterhout was working on &lt;a href=&#34;https://en.wikipedia.org/wiki/Sprite_%28operating_system%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sprite&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;All were experimenting with cluster-unified filesystems, distributed processing, workload migration,
and generally trying to build what 20 years later would become the &lt;a href=&#34;https://www.amazon.com/Datacenter-Computer-Introduction-Warehouse-Scale-Architecture/dp/159829556X&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Warehouse-Scale Computer&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;Part of the Sprite development at UCB, specifically, was the Log-Structured File System (LFS), and
Mendel Rosenblum and John K. Ousterhout present it in &lt;a href=&#34;https://www.usenix.org/legacy/publications/library/proceedings/sd93/seltzer.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;

 from 1992.
This is a long paper, 27 pages, but if you read it with hindsight, you can really appreciate how enlightened it was.&lt;/p&gt;
&lt;h1 id=&#34;lfs&#34;&gt;
    &lt;a href=&#34;#lfs&#34;&gt;
	LFS
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Filesystems with in-place updates are in part already using logs for faster recovery in 1992.
The paper poses the question &amp;ldquo;What if we had a filesystem that only had a log, and never did in-place updates?&amp;rdquo;,
calling it a log-structured file system.
It then proceeds to present an implementation and benchmarks for such a thing.&lt;/p&gt;
&lt;h2 id=&#34;reads-are-cached-only-writes-matter&#34;&gt;
    &lt;a href=&#34;#reads-are-cached-only-writes-matter&#34;&gt;
	&amp;ldquo;Reads are cached, only writes matter&amp;rdquo;
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;In working with distributed operating systems, the Sprite team noticed that they have a lot of memory available.
They found with increasing memory sizes, the probability of a file being served by the buffer cache instead of reading it from disk increased by a lot,
and eventually almost all disk reads are being served from memory.&lt;/p&gt;
&lt;p&gt;Writes cannot be cached very well, and eventually they need to hit persistent storage,
but with the reads being cached it would be worthwhile and possible to construct a filesystem optimized for writes.&lt;/p&gt;
&lt;p&gt;The team also observes that CPU speeds grow exponentially, following Moore&amp;rsquo;s law.
The same seems to be true for memory sizes, which being on-chip silicon structures also obey this law.
But disks do not work that way.
While their capacity grows, their transfer speed and seek time does not improve much, because mechanical parts do not obey Moore&amp;rsquo;s law.
Disks are a problem: While linear writes perform well, seeks are slow and are not getting faster much.&lt;/p&gt;
&lt;p&gt;So they propose never overwriting any data, but always appending changed blocks to the end of a log.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/log-adoption.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;


&lt;em&gt;“Ah you think the log is your ally? You merely adopted the log. I was born with it, designed for it.”&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;garbage-collection&#34;&gt;
    &lt;a href=&#34;#garbage-collection&#34;&gt;
	Garbage collection
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Of course, disks in the Nineties were finite, as they are now.
So there has to be a &lt;em&gt;cleaner&lt;/em&gt; process that identifies, frees and compacts space that is no longer needed by any current view of the filesystem.&lt;/p&gt;
&lt;p&gt;This is much like the Garbage Collection in Java Virtual Machines, which were invented around the same time.
And much like the GC in JVMs, it would turn out to be the weak spot of the system.&lt;/p&gt;
&lt;p&gt;A lot of the paper busies itself with simulating workloads on the filesystem with different cleaner policies,
and the team then lands on a system that very much evolves in the same way Java GC evolved,
with a multi-tier compaction process that mirrors the &amp;ldquo;Young&amp;rdquo;, &amp;ldquo;Old&amp;rdquo;, and &amp;ldquo;Permanent&amp;rdquo; generations of Java objects.
This is not entirely surprising from hindsight:
Other, newer systems such as Cassandra, Zookeeper and other storages that use LSM Trees are using a very similar strategy with good success.&lt;/p&gt;
&lt;p&gt;Specifically, LFS partitions the storage into contiguous segments, and cleans storage segment by segment:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We used a simulator to explore different cleaning policies and discovered a simple but effective algorithm based on cost and benefit:
it segregates older, slowly changing data from younger rapidly changing data and treats them differently during cleaning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Other code takes multiple nearly empty segments of the same age and copies them together into a single segment, freeing up the others.&lt;/p&gt;
&lt;p&gt;This creates a certain amount of background copy activity from the cleaner process.
It also creates a race between the writers to the system using up free space,
and the cleaner process trying to provide sufficient free space.
If the system writes data heavily and free space goes down, the cleaner may need to be prioritized higher,
consuming more I/O, in order to make sufficient progress in providing clean segments to write to.
This will also slow down the writers.&lt;/p&gt;
&lt;p&gt;Benchmarks executed as part of the research show that the system can indeed write to disk at up to 70% of the theoretically available maximum bandwidth.
But this is true only under ideal conditions.
Also, the data is not stored in read-order at all, so read performance is only good if data is actually cached.&lt;/p&gt;
&lt;p&gt;Segments are sufficiently large to amortize the cost of seeking to them.
In the Mid-Nineties, that meant a size of around 0.5 to 1 MB.&lt;/p&gt;
&lt;p&gt;Cleaning is then a three-step process:
After suitable segments have been identified from metadata,
the cleaner reads multiple segments into memory, compacts them into a smaller number of segments,
and writes them out to disk in a single large I/O operation.
The old segments can now be marked as clean, and be returned to the free segment pool.&lt;/p&gt;
&lt;h2 id=&#34;using-ffs-data-structures&#34;&gt;
    &lt;a href=&#34;#using-ffs-data-structures&#34;&gt;
	Using FFS data structures
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;LFS uses data structures from FFS almost unchanged:
The filesystem has superblocks, inodes, direct and indirect blocks, and uses the same structures for directories, too.
All information changed is buffered and then written out sequentially in a single disk write that appends atomically and asynchronously to the log.&lt;/p&gt;
&lt;p&gt;Not overwriting things means duplicating things, so when a file grows by appending a block, the file&amp;rsquo;s inode changes.
This means the block containing the changed inode needs to be written out again, together with block added to the file.
LFS needs to keep track of inodes in an &lt;em&gt;inode map&lt;/em&gt;, and this now also needs to be updated and written out:
even if it is small enough to be cached in memory, it needs to be persisted.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/lfs-structures.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;


&lt;em&gt;LFS does indeed do limited in-place updates: The superblock and checkpoint region are written in fixed locations.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;LFS stops short of also appending new copies of the inode map and ultimately the superblock for each disk write,
and puts these things into fixed locations.
So we do have in-place updates for certain limited metadata structures.
This is unfortunate, as we will see when we are looking at LFS&amp;rsquo; legacy.&lt;/p&gt;
&lt;h2 id=&#34;soft-updates&#34;&gt;
    &lt;a href=&#34;#soft-updates&#34;&gt;
	Soft Updates
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;In order for LFS to write out changes to data and metadata such as indirect blocks, direct blocks, inodes and directories,
updates had to be written in the proper order, even if the entire write happened in a single big I/O operation.
Writing out data &amp;ldquo;from the leaves of the filesystem tree to the top&amp;rdquo; sorts the updates in a way that made recovery easier,
because each data structure that had pointers to dependent blocks would be written out only after these blocks had already been persisted.&lt;/p&gt;
&lt;p&gt;It turns out that this logic also has merit for traditional filesystems that do in-place updates.
It allows &lt;a href=&#34;https://www.usenix.org/legacy/publications/library/proceedings/bsdcon02/mckusick.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;file system checking to go on in the background&lt;/a&gt;

 while the filesystem is already being made available,
and it can &lt;a href=&#34;https://www.usenix.org/legacy/publications/library/proceedings/usenix99/mckusick.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eliminate almost all synchronous metadata updates&lt;/a&gt;

 in the filesystem.&lt;/p&gt;
&lt;h1 id=&#34;lfs-in-bsd&#34;&gt;
    &lt;a href=&#34;#lfs-in-bsd&#34;&gt;
	LFS in BSD
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;The BSD FFS crew, also at UCB, was very aware of Ousterhout&amp;rsquo;s work, and picks it up the year after he publishes.
They port the filesystem to BSD and &lt;a href=&#34;https://www.usenix.org/legacy/publications/library/proceedings/sd93/seltzer.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;write a 1993 paper about it&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/lfs-ffs-feature-comparison.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Features and subsystems of BSD FFS are matched with the equivalent structures and concepts on the BSD LFS side.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;They note a few shortcomings, and present improvements:
The cleaner was single-threaded. No matter how many LFS filesystems were mounted, there was only a single cleaner process.
Now there is one per mounted filesystem.
They also provide a structural verifier for the filesystem, something similar to a &lt;code&gt;fsck&lt;/code&gt;, but a thing that can run in the background,
while the filesystem is mounted.&lt;/p&gt;
&lt;p&gt;Also, the original LFS code was using more memory than necessary, and BSD LFS was made a lot more memory efficient.&lt;/p&gt;
&lt;p&gt;A lot of the paper is then a validation that their implementation is indeed a faithful port, and an improvement over original LFS,
and benchmarking.&lt;/p&gt;
&lt;p&gt;The benchmarks confirm improved write performance, but also show weakness concerning read workloads.
This is for two reasons: data is possibly fragmented,
and the file system buffer cache in their machines is often too small to soak up the disk reads.
And secondly, when the cleaner process is running, it interacts badly with both the disk reads and writes via disk seeks,
and that costs more performance than anticipated.
This is in particular true for a database-like (TPC-B) benchmark load,
which performs badly and requires extensive tuning.&lt;/p&gt;
&lt;p&gt;Notably, the paper already hints at several improvements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There are two places where in-place updates still happen.
By removing them, the filesystem would automatically become transactional and gain snapshot functionality.
In fact, each disk write would eventually create a snapshot, and actually &amp;ldquo;snapshotting&amp;rdquo; the filesystem would simply mean
to prevent the cleaner from collecting certain snapshots.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adding checksums to disk blocks already has happened in a few places.
Turning this into a Merkle tree would be a trivial extension, and make validating the complete integrity not only of the
structure, but also of the file data a lot easier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The paper already notes that ordering writes in the log in a certain way makes background validation easier:
If blocks being pointed to are written before blocks that point to them, referential integrity of the filesystem is being kept at all times.
It is just that a transaction may be incomplete, but because it&amp;rsquo;s not referenced anywhere that is not a problem,
and the disk blocks will be eventually collected and freed by the cleaner.&lt;/p&gt;
&lt;p&gt;Nothing in this idea is actually dependent on the filesystem being LFS.
In fact, it can and was successfully applied to BSD FFS, too, under the name of
&lt;a href=&#34;https://www.usenix.org/legacy/publications/library/proceedings/usenix99/mckusick.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;soft updates&lt;/em&gt;&lt;/a&gt;

,
allowing to mount unchecked filesystems and then running a check in the background.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;performance-war&#34;&gt;
    &lt;a href=&#34;#performance-war&#34;&gt;
	Performance War
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;While Seltzer, Bostic, McKusick et al., the original authors of the BSD FFS, were busy porting Sprite LFS to BSD,
and tuning it,
Larry McVoy and S.R. Kleiman pick up the BSD FFS sources at Sun and add support for extents to it.
The resulting patch is tiny, and the work is being documented in
&lt;a href=&#34;https://people.freebsd.org/~pfg/docs/fs/ExtentlikePerformanceforUFS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extent-like Performance from a UNIX File System&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;To Seltzers chagrin, EFS often and consistently outperforms LFS, requires little to no tuning.
In &lt;a href=&#34;https://people.freebsd.org/~pfg/docs/fs/ExtentlikePerformanceforUFS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;File System Logging Versus Clustering: A Performance Comparison&lt;/a&gt;


this is confirmed, even if it takes a long paper with many benchmarks to arrive at this finding.
The problem is mostly with the disk seeks induced from running the cleaner.&lt;/p&gt;
&lt;p&gt;If only something could be done about this&amp;hellip;&lt;/p&gt;
&lt;p&gt;Something could be done, but it would happen at Sun and NetApp, and not in BSD: We&amp;rsquo;re getting ZFS and WAFL.&lt;/p&gt;
&lt;p&gt;Also, we&amp;rsquo;re getting things that can seek a lot faster than disks: Flash Storage.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>50 years in filesystems: A detour on vnodes</title>
      <link>https://blog.koehntopp.info/2023/05/15/50-years-in-filesystems-vnodes.html</link>
      <pubDate>Mon, 15 May 2023 01:02:03 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2023/05/15/50-years-in-filesystems-vnodes.html</guid>
      <description>&lt;p&gt;This is part 4 of a series.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/05/50-years-in-filesystems-1974.html&#34;&gt;1974&lt;/a&gt;

&amp;rdquo; on the traditional Unix Filesystem.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/06/50-years-in-filesystems-1984.html&#34;&gt;1984&lt;/a&gt;

&amp;rdquo; on the BSD Fast File System.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/12/50-years-in-filesystems-1994.html&#34;&gt;1994&lt;/a&gt;

&amp;rdquo; on SGI XFS.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Progress is sometimes hard to see, especially when you have been part of it or otherwise lived through it.
Often, it is easier to see by comparing modern educational material and the problems discussed with older material.
Or look for the research papers and sources that fueled the change. So this is what we do.&lt;/p&gt;
&lt;h1 id=&#34;how-to-have-multiple-filesystems&#34;&gt;
    &lt;a href=&#34;#how-to-have-multiple-filesystems&#34;&gt;
	How to have multiple filesystems
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Steve Kleiman wrote
&amp;ldquo;&lt;a href=&#34;https://www.semanticscholar.org/paper/Vnodes%3A-An-Architecture-for-Multiple-File-System-in-Kleiman/e0d14c74f23ef9b21c2fc37b5197fbfe348a7fcf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vnodes: An Architecture for Multiple File System Types in Sun UNIX&lt;/a&gt;

&amp;rdquo;
in 1986.&lt;/p&gt;
&lt;p&gt;This is a short paper, with little text, because most of it is listing of data structures and diagrams of C language &lt;code&gt;struct&lt;/code&gt;&amp;rsquo;s pointing at each other.
Kleiman wanted to have multiple file systems in Unix, but wanted the file systems to be able to share interfaces and internal memory, if at all possible.
Specifically, he wanted a design that provided&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one common interface with multiple implementations,&lt;/li&gt;
&lt;li&gt;supporting BSD FFS, but also NFS and RFS, two remote filesystems, and Non-Unix filesystems, specifically MS-DOS,&lt;/li&gt;
&lt;li&gt;with the operations being defined by the interface being atomic.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And the implementations should be able to handle memory and structures dynamically, without performance impact,
reentrant and multicore capable, and somewhat object-oriented.&lt;/p&gt;
&lt;h1 id=&#34;two-abstractions&#34;&gt;
    &lt;a href=&#34;#two-abstractions&#34;&gt;
	Two abstractions
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;He looked at the various operations, and decided to provide two major abstractions:
The filesystem (&lt;code&gt;vfs&lt;/code&gt;, virtual filesystem) and the inode (&lt;code&gt;vnode&lt;/code&gt;, virtual inode), representing a filesystem and a file.&lt;/p&gt;
&lt;p&gt;In true C++ style (but expressed in C), we get a virtual function table for each type, representing the class:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the &lt;code&gt;vfs&lt;/code&gt; type, this is a &lt;code&gt;struct vfsops&lt;/code&gt;,
a collection of function pointers with operations such as &lt;code&gt;mount&lt;/code&gt;, &lt;code&gt;unmount&lt;/code&gt;, &lt;code&gt;sync&lt;/code&gt; and &lt;code&gt;vget&lt;/code&gt;.
Later in the paper, prototypes and functionality of these functions are explained.&lt;/li&gt;
&lt;li&gt;For the &lt;code&gt;vnode&lt;/code&gt; type, likewise, we get &lt;code&gt;struct vnodeops&lt;/code&gt;.
The functions here are &lt;code&gt;open&lt;/code&gt;, &lt;code&gt;rdwr&lt;/code&gt; and &lt;code&gt;close&lt;/code&gt;, of course, but also &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;unlink&lt;/code&gt;, and &lt;code&gt;rename&lt;/code&gt;.
Some functions are specific to a filetype such as &lt;code&gt;readlink&lt;/code&gt;, &lt;code&gt;mkdir&lt;/code&gt;, &lt;code&gt;readdir&lt;/code&gt; and &lt;code&gt;rmdir&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actual mounts are being tracked in &lt;code&gt;vfs&lt;/code&gt; objects, which point to the operations applicable to this particular subtree with their &lt;code&gt;struct vfsops *&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, open files are being tracked in &lt;code&gt;vnode&lt;/code&gt; instances, which again, among other things, have a &lt;code&gt;struct *vnodeops&lt;/code&gt; pointer.
&lt;code&gt;vnodes&lt;/code&gt; are part of their &lt;code&gt;vfs&lt;/code&gt;, so they also have &lt;code&gt;struct *vfs&lt;/code&gt; to their filesystem instance.&lt;/p&gt;
&lt;p&gt;Both, the &lt;code&gt;vfs&lt;/code&gt; and the &lt;code&gt;vnode&lt;/code&gt; need to provide a way for the implementation to store implementation specific data (&amp;ldquo;subclass private fields&amp;rdquo;).
So both structures end with &lt;code&gt;caddr_t ...data&lt;/code&gt; pointers.
That is, the private data is not part of the virtual structure, but located elsewhere and pointed to.&lt;/p&gt;
&lt;h2 id=&#34;vnodes-in-action&#34;&gt;
    &lt;a href=&#34;#vnodes-in-action&#34;&gt;
	Vnodes in action
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/vfs-vnode-structures.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;


&lt;em&gt;One full page in the paper is dedicated to showing the various structures pointing at each other.
What looks confusing at first glance is actually pretty straightforward and elegant, once you trace it out.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Kleiman sets out to explain how things work using the &lt;code&gt;lookuppn()&lt;/code&gt; function, which replaces the older &lt;code&gt;namei()&lt;/code&gt; function from traditional Unix.
Analogous to &lt;code&gt;namei()&lt;/code&gt;, the function consumes a path name, and returns a &lt;code&gt;struct vnode *&lt;/code&gt; to the vnode represented by that pathname.&lt;/p&gt;
&lt;p&gt;Pathname traversal starts at the root vnode or the current directory vnode for the current process,
depending on the first character of a pathname being &lt;code&gt;/&lt;/code&gt; or not.&lt;/p&gt;
&lt;p&gt;The function then takes the next pathname component, iteratively, and calls the &lt;code&gt;lookup&lt;/code&gt; function for the current vnode.
This function takes a pathname component, and a current &lt;code&gt;vnode&lt;/code&gt; assuming it is a directory.
It then returns the &lt;code&gt;vnode&lt;/code&gt; representing that component.&lt;/p&gt;
&lt;p&gt;If a directory is a mountpoint, it has &lt;code&gt;vfsmountedhere&lt;/code&gt; set.
This is a &lt;code&gt;struct vfs *&lt;/code&gt;. &lt;code&gt;lookuppn&lt;/code&gt; follows the pointer,
and can call the &lt;code&gt;root&lt;/code&gt; function for that &lt;code&gt;vfs&lt;/code&gt; to get the root &lt;code&gt;vnode&lt;/code&gt; for that filesystem, replacing the current &lt;code&gt;vnode&lt;/code&gt; being worked on.&lt;/p&gt;
&lt;p&gt;The inverse must also be possible:
When resolving a &amp;ldquo;&lt;code&gt;..&lt;/code&gt;&amp;rdquo; component and the current &lt;code&gt;vnode&lt;/code&gt; has a root flag set in its &amp;ldquo;flags&amp;rdquo; field,
we go from the current &lt;code&gt;vnode&lt;/code&gt; to the &lt;code&gt;vfs&lt;/code&gt; following the &lt;code&gt;vfsmountedhere&lt;/code&gt; pointer.
Then we can use the &lt;code&gt;vnodecovered&lt;/code&gt; field in that &lt;code&gt;vfs&lt;/code&gt; to get the &lt;code&gt;vnode&lt;/code&gt; of the superior filesystem.&lt;/p&gt;
&lt;p&gt;In any case, upon successful completion, a &lt;code&gt;struct vnode*&lt;/code&gt; representing the consumed pathname is returned.&lt;/p&gt;
&lt;h2 id=&#34;new-system-calls&#34;&gt;
    &lt;a href=&#34;#new-system-calls&#34;&gt;
	New system calls
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;In order to make things work,
and to make things work efficiently, a few new system calls had to be added to round out the interfaces.&lt;/p&gt;
&lt;p&gt;It is here in Unix history that we get &lt;code&gt;statfs&lt;/code&gt; and &lt;code&gt;fstatfs&lt;/code&gt;, to get an interface to filesystems in userland.
We also gain &lt;code&gt;getdirentries&lt;/code&gt; (plural) to get multiple directory entries at once (depending on the size of the buffer provided),
which makes directory reading faster a lot for remote filesystems.&lt;/p&gt;
&lt;h1 id=&#34;in-linux&#34;&gt;
    &lt;a href=&#34;#in-linux&#34;&gt;
	In Linux
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Looking at the Linux kernel source, we can find the general structure of Kleiman&amp;rsquo;s design,
even if the complexity and richness of the Linux kernel obscure most of it.
The Linux kernel has a wealth of file system types, and added also a lot of functionality that wasn&amp;rsquo;t present in BSD 40 years ago.
So we find a lot more structures and system calls,
implementing namespaces, quotas, attributes, read-only modes, directory name caches, and other things.&lt;/p&gt;
&lt;h2 id=&#34;the-file&#34;&gt;
    &lt;a href=&#34;#the-file&#34;&gt;
	The file
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Still, if you squint, the original structure can still be found:
Linux splits the in-memory structures around files in two, the opened &lt;code&gt;file&lt;/code&gt;, which is an inode with a current position associated,
and the &lt;code&gt;inode&lt;/code&gt;, which is the whole file.&lt;/p&gt;
&lt;p&gt;We find instances of file objects, &lt;code&gt;struct file&lt;/code&gt;, defined
&lt;a href=&#34;https://github.com/torvalds/linux/blob/v6.3/include/linux/fs.h#L942C3-L981&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;

.
Among all the other things a file has, it has most notably a field &lt;code&gt;loff_t f_pos&lt;/code&gt;,
an offset (in bytes) from the start of the file,
the current position.&lt;/p&gt;
&lt;p&gt;The file&amp;rsquo;s class is defined via a virtual function table.
We find the pointer as &lt;code&gt;struct file_operations *f_op&lt;/code&gt;,
and the definition &lt;a href=&#34;https://github.com/torvalds/linux/blob/v6.3/include/linux/fs.h#L1754-L1798&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;

.
It shows all the things a file can do, most notable, &lt;code&gt;open&lt;/code&gt;, &lt;code&gt;close&lt;/code&gt;, &lt;code&gt;lseek&lt;/code&gt; and then &lt;code&gt;read&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The file also contains pointers to the inode, &lt;code&gt;struct inode *f_inode&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-inode&#34;&gt;
    &lt;a href=&#34;#the-inode&#34;&gt;
	The inode
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Operations on files without the need of an offset work on the file as a whole,
defined as &lt;code&gt;struct inode *&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Check the definition &lt;a href=&#34;https://github.com/torvalds/linux/blob/v6.3/include/linux/fs.h#L595-L705&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;

.
We see other definitions in here that have no equivalent in BSD from 40 years ago, such as ACLs and attributes.&lt;/p&gt;
&lt;p&gt;We find the inode&amp;rsquo;s class is defined via a virtual function table,
&lt;code&gt;struct inode_operations *i_op&lt;/code&gt;,
and the definition &lt;a href=&#34;https://github.com/torvalds/linux/blob/v6.3/include/linux/fs.h#L1800-L1840&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;

.
Again, a lot of them deal with new features such as ACLs and extended attributes,
but we also find those we expect such as &lt;code&gt;link&lt;/code&gt;, &lt;code&gt;unlink&lt;/code&gt;, &lt;code&gt;rename&lt;/code&gt; and so on.&lt;/p&gt;
&lt;p&gt;The inode also contains a pointer to a filesystem, the &lt;code&gt;struct super_block *i_sb&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-superblock&#34;&gt;
    &lt;a href=&#34;#the-superblock&#34;&gt;
	The superblock
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;A mountpoint is represented as an instance of &lt;code&gt;struct super_block&lt;/code&gt;,
defined &lt;a href=&#34;https://github.com/torvalds/linux/blob/v6.3/include/linux/fs.h#L1136-L1268&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;

.
Again, the class is a &lt;code&gt;struct super_operations *s_op&lt;/code&gt;, defined
&lt;a href=&#34;https://github.com/torvalds/linux/blob/v6.3/include/linux/fs.h#L1886-L1918&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;As an added complexity, there is no finite list of filesystems.
It is instead extensible through loadable modules, so we also have a &lt;code&gt;struct file_system_type&lt;/code&gt;,
&lt;a href=&#34;https://github.com/torvalds/linux/blob/v6.3/include/linux/fs.h#L1886-L1918&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;

.
This is basically a class with only one class method as a factory for superblocks, &lt;code&gt;mount&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;
    &lt;a href=&#34;#summary&#34;&gt;
	Summary
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Unix changed.
It became a lot more runtime extensive, added a lot of new functionality and gained system calls.
Things became more structured.&lt;/p&gt;
&lt;p&gt;But the original design and data structures conceived by Kleiman and Joy held up, and can still be found in current Linux, 40 years later.
We can point to concrete Linux code, which while looking completely different, is structurally mirroring the original design ideas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>50 years in filesystems: 1994</title>
      <link>https://blog.koehntopp.info/2023/05/12/50-years-in-filesystems-1994.html</link>
      <pubDate>Fri, 12 May 2023 12:13:14 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2023/05/12/50-years-in-filesystems-1994.html</guid>
      <description>&lt;p&gt;This is part 3 of a series.
The first part is &amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/05/50-years-in-filesystems-1974.html&#34;&gt;1974&lt;/a&gt;

&amp;rdquo;.
The second part is &amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/06/50-years-in-filesystems-1984.html&#34;&gt;1984&lt;/a&gt;

&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Progress is sometimes hard to see, especially when you have been part of it or otherwise lived through it.
Often, it is easier to see if you compare modern educational material, and the problems discussed with older material.
And then look for the research papers and sources that fueled the change.&lt;/p&gt;
&lt;p&gt;In Linux (and Unix in general), this is easy.&lt;/p&gt;
&lt;h1 id=&#34;1994--the-sgi-xfs-filesystem&#34;&gt;
    &lt;a href=&#34;#1994--the-sgi-xfs-filesystem&#34;&gt;
	1994 — The SGI XFS Filesystem
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;In 1994, the paper &lt;a href=&#34;http://www.scs.stanford.edu/nyu/02fa/sched/xfs.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scalability in the XFS File System&lt;/a&gt;

 saw publication.
Computers got faster since 1984, and so did storage.
Notably, we are now seeing boxes with multiple CPUs, and with storage reaching into the Terabytes.
The improvements to the 4.3BSD fast filing system (or the modified version in SGI IRIX called EFS) were no longer sufficient.&lt;/p&gt;
&lt;p&gt;SGIs benchmarks cite machines that had large backplanes with many controllers (one benchmark cites a box with 20 SCSI controllers),
many disks (three-digit-numbers of hard drives),
and many CPUs (the benchmarks quote 12 socket machines) with a lot of memory (up to one gigabyte quoted in the benchmarks).&lt;/p&gt;
&lt;p&gt;Filesystems became larger than FFS could handle,
files became larger than FFS could handle,
the number of files per directory led to large lookup times,
central data structures such as allocation bitmaps did no longer scale,
and global locks made concurrent access to the file system with many CPUs inefficient.
SGI set out to design a fundamentally different filesystem.&lt;/p&gt;
&lt;p&gt;Also, the Unix community as a whole was challenged by Cutler and Custer,
who showed with NTFS for Windows NT 4.0 what was possible if you redesign from scratch.&lt;/p&gt;
&lt;h1 id=&#34;requirements&#34;&gt;
    &lt;a href=&#34;#requirements&#34;&gt;
	Requirements
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;The XFS filesystem was a firework of new ideas, and a large deviation from traditional Unix filesystem design.
The list of new things is long:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facilitate concurrency with
&lt;ul&gt;
&lt;li&gt;allocation zones&lt;/li&gt;
&lt;li&gt;inode lock splitting&lt;/li&gt;
&lt;li&gt;facilities for large, parallel I/O requests, DMA and Zero-Copy I/O&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scalability of access by building the filesystem around the concepts of
&lt;ul&gt;
&lt;li&gt;B+-Trees&lt;/li&gt;
&lt;li&gt;Extents: pairs of (start, length) descriptors&lt;/li&gt;
&lt;li&gt;decoupling &amp;ldquo;file write&amp;rdquo; and &amp;ldquo;file layout&amp;rdquo; on disk to allow for contiguous files by using delayed allocation and preallocation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Introduce a write-ahead log to journal metadata changes
&lt;ul&gt;
&lt;li&gt;log asynchronously to allow for write coalescence&lt;/li&gt;
&lt;li&gt;leveraging the log for recovery, so that recovery time is proportional to the amount of data in flight, not the size of the filesystem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;XFS was written with these requirements,
and primarily in order to provide a filesystem that could leverage all the performance of large SGI boxes for video editing, video serving and scientific computing.&lt;/p&gt;
&lt;h2 id=&#34;a-logging-filesystem-but-not-a-log-structured-filesystem&#34;&gt;
    &lt;a href=&#34;#a-logging-filesystem-but-not-a-log-structured-filesystem&#34;&gt;
	A logging filesystem, but not a log structured filesystem
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This is also happening at about the same time as John K. Ousterhout asking
&amp;ldquo;&lt;a href=&#34;https://web.stanford.edu/~ouster/cgi-bin/papers/osfaster.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Why Aren’t Operating Systems Getting Faster As Fast as Hardware?&lt;/a&gt;

&amp;rdquo;.
Ousterhout started to explore the ideas of a log-based filesystem with the experimental Sprite operating system.&lt;/p&gt;
&lt;p&gt;Log-based filesystems are an extremely radical idea that we need to discuss later, even if they originally predate XFS by a bit.
But they weren&amp;rsquo;t very usable originally, because they need different hardware which can offer a lot more disk seeks.
Log structured file system ideas had to become a lot more refined to actually have an impact,
so we are going to discuss them later in the series.&lt;/p&gt;
&lt;h2 id=&#34;what-irix-had-before&#34;&gt;
    &lt;a href=&#34;#what-irix-had-before&#34;&gt;
	What IRIX had before
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;IRIX as coming already with EFS, the specially sauced-up version of BSD FFS that used extents.
It suffered from an 8 GB filesystem size limit, a 2 GB filesize limit, and it could not utilize the full hardware I/O bandwidth,
which made many customers of these fantastically expensive machines somewhat sad.&lt;/p&gt;
&lt;p&gt;Demands for video playback and from the database community led to requirements
that stated the new filesystem needed to support hundreds of TB of disk space,
hundreds of MB/s of I/O bandwidth and many parallel I/O requests in order to be able to saturate the hardware provided,
and all this without running out of CPU.&lt;/p&gt;
&lt;p&gt;The title of the paper is &amp;ldquo;Scalability in the XFS File System&amp;rdquo; and not &amp;ldquo;Implementation of …&amp;rdquo;,
so it is more a show of the features provided and a superficial discussion of the implementation and the design decisions around it.
It is not an in-depth discussion of the implementation,
nor are extensive benchmarks provided.&lt;/p&gt;
&lt;h1 id=&#34;features&#34;&gt;
    &lt;a href=&#34;#features&#34;&gt;
	Features
    &lt;/a&gt;
&lt;/h1&gt;
&lt;h2 id=&#34;large-filesystems&#34;&gt;
    &lt;a href=&#34;#large-filesystems&#34;&gt;
	Large filesystems
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;XFS supports large filesystems.
Previous filesystems use 32-bit pointers to blocks.
At 8 KB block size, with 32-bit block pointers, the limit is 32 TB.&lt;/p&gt;
&lt;p&gt;Moving to 64-bit block pointers would make many data structures multiple of 8 bytes in size, which seemed like a waste.&lt;/p&gt;
&lt;p&gt;For concurrency (see below), XFS introduces the concept of &lt;strong&gt;allocation groups&lt;/strong&gt; (AGs), which are always smaller than 4 GB.
Allocation groups have local instances of the filesystem data structures, for example, inode maps or free block tracking.
These are independently locked and so allow for concurrent operations in different allocation groups.&lt;/p&gt;
&lt;p&gt;Allocation groups also help to save on pointer sizes:
Where possible, AG-relative block numbers are being used, and these always fit into 32-bit pointers.
In fact, a 4G allocation group can have only 1M blocks or fewer blocks (at 4K minimum blocksize),
so a single maximum sized extent within a single AG can be packed into 40 bits (5 bytes).&lt;/p&gt;
&lt;p&gt;The maximum file size and filesystem size is 8 EB (2^63-1).&lt;/p&gt;
&lt;h2 id=&#34;bandwidth-and-concurrency&#34;&gt;
    &lt;a href=&#34;#bandwidth-and-concurrency&#34;&gt;
	Bandwidth and Concurrency
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Concurrent operations are a design goal for XFS.
1994 is the age of 20 MB/s SCSI controllers, but SGI built machines with large chassis that could house many controllers and many drives.
Benchmarks quote machines with an aggregate bandwidth of 480 MB/s delivering file I/O performance of over 370 MB/s with no tuning, including all overheads.
This is quite an impressive result for everyday usage in 1994.&lt;/p&gt;
&lt;p&gt;XFS achieves this using large blocks (4 KB or 8 KB block size), and the concept of extents.&lt;/p&gt;
&lt;h3 id=&#34;extents-and-b-trees&#34;&gt;
    &lt;a href=&#34;#extents-and-b-trees&#34;&gt;
	Extents and B-Trees.
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extents&lt;/strong&gt; are a core concept in XFS.
They are tuples, most of the time pairs of &lt;code&gt;(startblock, length)&lt;/code&gt;.
For mapping file blocks to disk blocks (&amp;ldquo;bmap&amp;rdquo;), they are triples &lt;code&gt;(offset, length, startblock)&lt;/code&gt;.
Using truncated values, because of the size limits imposed by the maximum AG size,
they can describe a contiguous sequence of blocks up to 2M blocks in size in 4 bytes,
which is a lot more efficient than what BSD FFS did before.&lt;/p&gt;
&lt;p&gt;Extents also allow XFS to do large I/O requests because they describe sections of contiguous blocks,
making it easy to create read or write requests for several blocks apiece.
It does I/O by default with 64 KB memory buffers, unless special provisions are being made to make them even larger.&lt;/p&gt;
&lt;p&gt;The filesystem assumes an underlying disk structure with striping, and provides a number of 2 or 3 outstanding I/O requests to allow for concurrent I/O.
It checks for backpressure, that is, it checks that the application is actually reading data.
If it does, it issues additional read requests to keep the number of requests in flight at 3 by default,
good for 192 KB in flight at once.&lt;/p&gt;
&lt;p&gt;Groups of Extents can be collected in linear lists, but that will lead to scaling problems.
So XFS uses &lt;strong&gt;B+-Trees&lt;/strong&gt;, which degrade to linear lists if there is only one single index block.&lt;/p&gt;
&lt;p&gt;Usually tuples are indexed on their first value, but for some structures such as free lists, multiple indexes are kept:
It is useful to index free space by &lt;code&gt;startblock&lt;/code&gt; for closeness, but also by &lt;code&gt;length&lt;/code&gt; to fit free spaces in the right size.&lt;/p&gt;
&lt;h3 id=&#34;breaking-the-single-writer-inode-lock&#34;&gt;
    &lt;a href=&#34;#breaking-the-single-writer-inode-lock&#34;&gt;
	Breaking the single-writer inode lock
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/overlapping-write.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;


&lt;em&gt;Posix locks the in-memory inode to guarantee &lt;a href=&#34;https://blog.koehntopp.info/2018/11/29/but-is-it-atomic.html&#34;&gt;atomic writes&lt;/a&gt;

.
This makes sure any two large multiple-block writes always happen one-before-the-other.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;XFS also breaks the in-memory inode locks:
Posix demands that large, overlapping, multiple block writes are totally ordered.
When they overlap, it must not happen that there is a block soup of alternating blocks from write A and write B.&lt;/p&gt;
&lt;p&gt;The default implementation in most kernels is simply a file-global lock placed at the in-memory inode, making sure there can be only one writer per inode.
Implementers of databases hate that because it limits the write concurrency on any single file to One.
This is, for example, why Oracle recommends that you make tablespaces from multiple files, each no larger than one GB.&lt;/p&gt;
&lt;p&gt;XFS, in &lt;code&gt;O_DIRECT&lt;/code&gt; mode, removes this lock and allows atomic, concurrent writes, making database people very happy.&lt;/p&gt;
&lt;h2 id=&#34;dynamic-inodes-and-improved-free-space-tracking&#34;&gt;
    &lt;a href=&#34;#dynamic-inodes-and-improved-free-space-tracking&#34;&gt;
	Dynamic Inodes and improved Free Space Tracking
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;With large filesystems, you can never know:
The applications may need a large number of inodes for many small files, or a small number of large files.
Also, what is a good distance between the inode and the data blocks that belong to the file?&lt;/p&gt;
&lt;p&gt;There is no good answer to the first question, and &amp;ldquo;as close as possible&amp;rdquo; is the answer to the second question.
So XFS creates Inodes dynamically, as needed, in chunks of 64 inodes.&lt;/p&gt;
&lt;p&gt;The relatively large inode size of 256 bytes (compared to 128 in BSD FFS and 64 in traditional Unix)
is being compensated by the fact that XFS creates Inodes only as needed, and places them relatively closely to the file start.
This frees up a substantial amount of disk space –
in Unix filesystems with fixed inode counts as much as 3-4% of the disk space can be locked up in pre-allocated inodes.
And even with cylinder groups, there will be considerable distance between an inode and the first data block.&lt;/p&gt;
&lt;p&gt;Because inodes can reside anywhere on the disk and not just behind the superblock, they need to be tracked.
XFS does with one B+-tree per allocation group.
The tree is indexed by the start block, and records for each inode in the chunk if it is available or in-use.
The inodes themselves are not kept in the tree, but in the actual chunks which are close to the file data.&lt;/p&gt;
&lt;p&gt;Similarly, free space is tracked in chunks, and kept in per-AG trees, indexed twice: by start block and by length.&lt;/p&gt;
&lt;h2 id=&#34;write-ahead-log&#34;&gt;
    &lt;a href=&#34;#write-ahead-log&#34;&gt;
	Write-Ahead Log
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Recovering a large filesystem after a crash can be slow.
The recovery time is proportional to the size of the filesystem, and the number of files in it,
because the system basically has to scan the entire filesystem and rebuild the directory tree in order to ensure things are consistent.
With XFS, the filesystem also is a lot more fragile, as it provides a variable number of inodes, spread out non-contiguously over the disk.
Recovering them would be extra expensive.&lt;/p&gt;
&lt;p&gt;Using write-ahead logging for metadata, this can be avoided most of the time.
Recovery time is proportional to the size of the log, that is, the amount of data in flight at the time of the crash.&lt;/p&gt;
&lt;p&gt;The log contains log entries containing a descriptor header and a full image of all changed metadata structures:
inodes, directory blocks, free extent tree blocks, inode allocation tree blocks, allocation group blocks, and the superblock.
Because full images are stored in the block, recovery is simple: the recovery process simply copies these new, changes images into the place where they are supposed to be, without needing to understand what kind of structure it changes.&lt;/p&gt;
&lt;p&gt;The trust of the authors into the log was huge:
Initially XFS had no &lt;code&gt;fsck&lt;/code&gt; program.
This turned out to be overly optimistic, and so now &lt;code&gt;xfs_repair&lt;/code&gt; exists.&lt;/p&gt;
&lt;h3 id=&#34;metadata-update-performance&#34;&gt;
    &lt;a href=&#34;#metadata-update-performance&#34;&gt;
	Metadata update performance
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;XFS is logging metadata updates, which means they need to be written to the filesystem log.
By default, this log is placed inline, in the filesystem.
But it is also possible to take it out, and put onto other media, for example, flash storage or battery-backed memory.&lt;/p&gt;
&lt;p&gt;Writes to the log are asynchronous, if possible, but with partitions serving NFS they cannot be.
Asynchronous writes allow for write batching, with speeds things up.
But NFS servers profit a lot from accelerated log storage.&lt;/p&gt;
&lt;p&gt;Because all metadata updates need to be logged, it can happen that intense metadata operations flood the log.
A &lt;code&gt;rm -rf /usr/src/linux&lt;/code&gt; for example is not an operation where XFS is particularly fast, because the metadata update stream will eventually overflow the log.
And because everything else in XFS is parallel by AG, the log is usually the only source of contention.&lt;/p&gt;
&lt;h2 id=&#34;large-files-and-sparse-files&#34;&gt;
    &lt;a href=&#34;#large-files-and-sparse-files&#34;&gt;
	Large files and sparse files
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;In FFS, files are mapped by the classical dynamic array, with direct blocks and up to three levels of indirect blocks.
With 64-bit filesize, this becomes unwieldy: there will be more than three levels of indirect blocks required,
and a substantial number of blocks would be required what essentially becomes a list of incrementing numbers.
FFS (and EFS) are also forced to layout blocks the moment each block is allocated in the filesystem buffer pool.
So effectively, no attempt to contiguously layout files on disk is being made.
Instead, blocks are placed individually.&lt;/p&gt;
&lt;p&gt;XFS replaces this dynamic array with extents.&lt;/p&gt;
&lt;p&gt;In file placement maps, these mapping extents are triples &lt;code&gt;(blockoffset, length, disk block)&lt;/code&gt;.
These extents are stored in the inode itself until this overflows.
Then XFS starts to root a B+-tree of the mapping extents in the inode, indexed by logical block number for fast seeks.&lt;/p&gt;
&lt;p&gt;This data structure allows compressing a substantial number of blocks (up to 2M blocks) in a single descriptor,
assuming contiguous allocation is possible.
So even large files could be stored in very few extents, in the optimal case one extent per AG.&lt;/p&gt;
&lt;h3 id=&#34;delayed-allocation-and-preallocation-for-contiguous-layout&#34;&gt;
    &lt;a href=&#34;#delayed-allocation-and-preallocation-for-contiguous-layout&#34;&gt;
	Delayed allocation and Preallocation for contiguous layout
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;XFS also provides a new concept, delayed allocation, in which virtual extents can be allocated in the file system buffer pool.
These are blocks full of yet unwritten data that have not been layouted, and hence lack a physical position.
Only on flush these blocks are layouted, contiguously, and then written out linearly in large writes, to speed things up.&lt;/p&gt;
&lt;p&gt;This is a fundamental change to how the filesystem buffer cache works –
previously it was possible to use &lt;code&gt;(device, physical block number)&lt;/code&gt; to identify buffer cache blocks and prevent duplicate buffer allocation.
When porting XFS to Linux, the Linux kernel initially could not accommodate strategies that do not use such identification in the normal buffer cache, so at first XFS required a separate buffer cache.
This got fixed later, as the porting progressed.&lt;/p&gt;
&lt;p&gt;To ensure that files can be layouted without fragmentation, in a single extent, XFS aggressively preallocates storage for open files.
The default amount of disk space preallocated is dependent on the amount of free space in the filesystem, and can be substantial.&lt;/p&gt;
&lt;p&gt;The internet is littered with questions by XFS users asking where their disk space is, and the answer is always &amp;ldquo;in the open file handles of &lt;code&gt;/var/log&lt;/code&gt;. Also, check the &lt;a href=&#34;https://man7.org/linux/man-pages/man5/xfs.5.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;manpage&lt;/a&gt;

 for &lt;code&gt;allocsize=&lt;/code&gt; and also check &lt;a href=&#34;https://linux-xfs.oss.sgi.narkive.com/jjjfnyI1/faq-xfs-speculative-preallocation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;/proc/sys/fs/xfs/speculative_prealloc_lifetime&lt;/code&gt;&lt;/a&gt;

.&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;locality&#34;&gt;
    &lt;a href=&#34;#locality&#34;&gt;
	Locality
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;XFS does not use allocation groups for locality much.
They exist mostly for concurrency.
Instead, file placement is mostly around directories and existing blocks of the current file.
The only exception is &amp;ldquo;new directories&amp;rdquo;, which are placed &amp;ldquo;away&amp;rdquo; from their parent directory by putting them into a different AG.&lt;/p&gt;
&lt;p&gt;In large files, if new extents need to be placed, they go &amp;ldquo;initially near the inode, then near the existing block in the file which is closest to the offset in the file for which we are allocating space&amp;rdquo;, as the paper specifies.
This places the inode close to the start of the file, and blocks added later to whatever is already present.&lt;/p&gt;
&lt;h2 id=&#34;large-directories&#34;&gt;
    &lt;a href=&#34;#large-directories&#34;&gt;
	Large directories
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;In the traditional Unix filesystem and in BSD FFS, directory name lookups are linear operations.
Large directories slow this down a lot, for any kind of pathname to inode translation.&lt;/p&gt;
&lt;p&gt;XFS chose the ubiquitous B+-Tree as a structure for directories, too, but with a quirk:
Since the keys are supposed to be filenames, a variable length structure, they would be completely different from all the other tree implementations in the filesystem.
The XFS authors did not like this idea, so they are hashing the filename into a fixed 4-byte name hash, and then store one or more directory entries as &lt;code&gt;(name, inode)&lt;/code&gt; pairs in the value.&lt;/p&gt;
&lt;p&gt;There was some tradeoff discussion involved in this, but the authors found that short keys allow storing many entries per block,
leading to wide trees, and thus faster lookups.
They boast &amp;ldquo;We can have directories with millions of entries&amp;rdquo;, something that was previously unthinkable in Unix filesystems.&lt;/p&gt;
&lt;h1 id=&#34;a-lot-of-code&#34;&gt;
    &lt;a href=&#34;#a-lot-of-code&#34;&gt;
	A lot of code
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/xfs-scaling.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;


&lt;em&gt;XFS Benchmarks in 1994 show nice and welcome linear scaling behavior that utilizes the hardware offered well.
It handles well on large boxes with (for 1994) high core-counts.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;XFS is a large filesystem.
Linux ext2 is only 5000 lines of kernel code (and about 10x this in user-land).
XFS is 50.000 lines of kernel code, and that is without the IRIX volume manager XLV (in Linux, the XFS port uses LVM2 instead).&lt;/p&gt;
&lt;p&gt;XFS was released under the GNU GPL in May 1999, and was ported into the Linux kernel starting in 2001.
As of 2014, it was supported in most Linux distributions and RHEL used it as the default filesystem.
And even in 2024 it is still holding up reasonably well, on HDD and on flash.&lt;/p&gt;
&lt;p&gt;It still is the filesystem with the best scaling behavior, the best concurrency behavior, and the most consistent commit times,
which makes it the preferred filesystem for any kind of database usage.
This is due to the elimination of several global locks that impair concurrent usage and performance in large filesystems,
and due to the consistent use of B+-Tree structures with &lt;code&gt;O(log(n))&lt;/code&gt; scaling behavior where before algorithms with worse scaling behavior have been used.
The use of extents also allows dynamically growing I/O sizes, benefiting throughput,
and together with the novel idea of delayed allocation encourage contiguous file placement.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>50 years in filesystems: 1984</title>
      <link>https://blog.koehntopp.info/2023/05/06/50-years-in-filesystems-1984.html</link>
      <pubDate>Sat, 06 May 2023 12:13:14 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2023/05/06/50-years-in-filesystems-1984.html</guid>
      <description>&lt;p&gt;This is part 2 of a series. The first part is &amp;ldquo;&lt;a href=&#34;https://blog.koehntopp.info/2023/05/05/50-years-in-filesystems-1974.html&#34;&gt;1974&lt;/a&gt;

&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Progress is sometimes hard to see, especially when you have been part of it or otherwise lived through it.
Often, it is easier to see if you compare modern educational material, and the problems discussed with older material.
And then look for the research papers and sources that fueled the change.&lt;/p&gt;
&lt;p&gt;In Linux (and Unix in general), this is easy.&lt;/p&gt;
&lt;h1 id=&#34;1984--the-bsd-fast-filing-system&#34;&gt;
    &lt;a href=&#34;#1984--the-bsd-fast-filing-system&#34;&gt;
	1984 — The BSD Fast Filing System
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;The original Unix filesystem was doing well, but also had a large number of obvious problems.
BSD Unix undertook an effort to fix them, and this is documented in the book
&amp;ldquo;&lt;a href=&#34;https://www.amazon.de/Design-Implementation-4-3Bsd-Operating-System/dp/0201061961&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Design and Implementation of the 4.3BSD UNIX Operating System&lt;/a&gt;

&amp;rdquo;
by Leffler, McKusick et. al&lt;a href=&#34;http://libgen.rs/book/index.php?md5=61457A629D5DE3B8966141A9D51FE89B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;.&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;A more concise, but also more academic discussion can be found in the classic 1984 paper &lt;a href=&#34;https://dsf.berkeley.edu/cs262/FFS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Fast File System for UNIX&lt;/a&gt;

,
which lists Marshall McKusick, Bill Joy (then at Sun), Samuel Leffler (then at LucasFilm) and Robert Fabry as authors.
The paper promises a reimplementation of the Unix filesystem for higher throughput, better allocation and better locality of reference.&lt;/p&gt;
&lt;h2 id=&#34;the-hardware&#34;&gt;
    &lt;a href=&#34;#the-hardware&#34;&gt;
	The hardware
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;It is 1984.
The computers targeted by 4.3BSD are desktop and cabinet workstations.
These are machines with 32-bit data registers and 32-bit address registers.&lt;/p&gt;
&lt;p&gt;External data and address bus sizes vary:
Earlier 68k CPUs had smaller sized buses, but in 1984 the Motorola 68020 debuted.
It was the first 68k to offer buses with the full width of 32 bits, at a budget of ca. 200k transistors on the die.
Later the 68030 integrated the MMU, previously a separate chip,
and the 68040 also integrated the FPU, again previously a separate chip.&lt;/p&gt;
&lt;p&gt;Early Sun workstations, the Sun-3 series, feature these CPUs.
But Sun took the designs from the experimental Berkeley RISC systems and released the Sun-4 series in 1986 with SPARC architecture RISC chips.
SPARC architecture is not without compromises, but was very viable and saw continuous development until after the purchase of Sun by Oracle, which then killed both the SPARC, and later also the Itanium CPU architecture.&lt;/p&gt;
&lt;p&gt;Curt Schimmel discusses the tradeoffs made by SPARC in the MMU, register and memory access design, and why they made sense. See &lt;a href=&#34;https://www.amazon.de/UNIX-Systems-Modern-Architectures-Multiprocessing/dp/0201633388&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UNIX Systems for Modern Architectures&lt;/a&gt;

&lt;a href=&#34;http://libgen.rs/book/index.php?md5=0E4A02E80A6250838CB1D3C3A1405CAD&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;.&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;In between, in 1985, the MIPS architecture debuted, which is another series of RISC CPU architectures. It also starts out as a fully 32-bit type of system, and found use in SGI workstations.&lt;/p&gt;
&lt;p&gt;HP had another RISC-type of CPU, the PA-RISC, an outgrowth of their &amp;ldquo;Spectrum&amp;rdquo; research programme, coming to market in 1986 (and later replaced by Intel&amp;rsquo;s failed Itanium).&lt;/p&gt;
&lt;p&gt;Systems pioneer DEC themselves had the VAX, a 32-bit cabinet computer with a CISC CPU, and that since 1977 already.
They would not go RISC until 1992, but then fully 64-bit with the Alpha AXP (&amp;ldquo;DEC Alpha&amp;rdquo;) architecture.
While interesting, this did not last long: with the sale to Compaq in 1998, the CPU was discontinued, and the IP was sold to Intel in 2001.&lt;/p&gt;
&lt;p&gt;In general, workstation type systems in 1984 had main memory in the low two-digit MB range, and ran at clock speeds of two-digit MHz system clocks.&lt;/p&gt;
&lt;h1 id=&#34;43bsds-fast-filing-system&#34;&gt;
    &lt;a href=&#34;#43bsds-fast-filing-system&#34;&gt;
	4.3BSD&amp;rsquo;s Fast Filing System
    &lt;/a&gt;
&lt;/h1&gt;
&lt;h2 id=&#34;the-traditional-filesystems-shortcomings&#34;&gt;
    &lt;a href=&#34;#the-traditional-filesystems-shortcomings&#34;&gt;
	The traditional filesystems shortcomings
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The 32-bit VAX systems were being used for typical 1980&amp;rsquo;s workstation work, which include things such as image processing or VLSI chip design.
On these systems, the original Unix filesystem showed structural problems in keeping up with file size, I/O speed, and simple number of files.
Also, the tiny 512-byte I/O size slowed disk subsystem performance considerably.&lt;/p&gt;
&lt;p&gt;The paper mentions the strict segregation of filesystem metadata at the front of the file system from the actual data in the back part of the filesystem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A 150 MB traditional UNIX file system consists of 4 megabytes of inodes followed by 146
megabytes of data.
This organization segregates the inode information from the data; thus accessing a file
normally incurs a long seek from the file’s inode to its data.
Files in a single directory are not typically
allocated consecutive slots in the 4 megabytes of inodes, causing many non-consecutive blocks of inodes to
be accessed when executing operations on the inodes of several files in a directory.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This defines one major goal for BSD FFS: Better filesystem layout, bringing metadata and data closer together,
storing files in a single directory closer together,
and preventing fragmentation of a file into small fragments that can be loaded only inefficiently.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/filesystem-fragmentierung.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;


&lt;em&gt;Fragmentation: Initially, four files are being created, each using 2 blocks.
Then the files B and D are being deleted.
The free space is then being reclaimed by the three-block-sized file E, which is stored in non-adjacent blocks.
This causes small disk seeks, and slow I/O.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Another goal stated is to increase disk block size.
Larger disk blocks benefit throughput in two ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Larger disk blocks provide larger units of I/O, so more data is transferred in a single I/O operation.&lt;/li&gt;
&lt;li&gt;Larger disk blocks also allow the filesystem to store more file pointers in an indirect block, greatly reducing the number of indirect block accesses.
This is primarily a problem if indirect blocks are not cached in a file system buffer cache.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The paper quotes the throughput of an already marginally optimized, traditional Unix filesystem at around 4% of the theoretical maximum,
which is abysmally bad.
This is mainly attributed to fragmentation, non-contiguous storage of adjacent blocks in a file.
Defragmentation, already suggested in 1976, was discarded as a non-viable idea.
The authors instead aim for a solution that places files sensibly in the first place.&lt;/p&gt;
&lt;h2 id=&#34;bsd-ffs-innovations&#34;&gt;
    &lt;a href=&#34;#bsd-ffs-innovations&#34;&gt;
	BSD FFS innovations
    &lt;/a&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;cylinder-groups-and-understanding-chs&#34;&gt;
    &lt;a href=&#34;#cylinder-groups-and-understanding-chs&#34;&gt;
	Cylinder Groups and understanding CHS
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;The BSD FFS understands the physical layout of a harddisk, with &lt;a href=&#34;https://en.wikipedia.org/wiki/Cylinder-head-sector&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cylinders, heads and sectors&lt;/a&gt;

 (CHS).
It divides the disk into cylinder groups, adjacent tracks of all disk heads.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/cylinder-groups.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;


&lt;em&gt;As the disk rotates, various disk heads reach inside the platter stack like a comb.
Each head marks a track on the disk, which is subdivided into physical disk blocks by the controller hardware.
Together, all tracks marked by all heads form a cylinder.
A cylinder group is a set of consecutive cylinders. (Image: &lt;a href=&#34;https://pages.cs.wisc.edu/~remzi/OSTEP/file-ffs.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OSTEP&lt;/a&gt;

, page 3)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Each cylinder group becomes a mini-version of a traditional Unix filesystem, with a copy of the superblock, its own local inode area, and local inode and block usage bitmaps.
The usage of bitmaps is also novel, as they replace the free lists used in the traditional filesystem.
As the filesystem has information about the CHS layout, it also makes sure that the superblock is not always placed on the same platter for each copy,
trying to make the filesystem better redundant against harddisk failure.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id=&#34;excursion-raid-and-other-efforts-at-berkeley&#34;&gt;
    &lt;a href=&#34;#excursion-raid-and-other-efforts-at-berkeley&#34;&gt;
	Excursion: Raid and other Efforts at Berkeley
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://www2.eecs.berkeley.edu/Pubs/TechRpts/1987/CSD-87-391.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAID paper&lt;/a&gt;

 was published only several years later,
but &lt;a href=&#34;http://web.eecs.umich.edu/~michjc/eecs584/Papers/katz-2010.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;according to Katz&lt;/a&gt;

 was developed also in Berkeley, during the same time frame, 1983/1984.&lt;/p&gt;
&lt;p&gt;Katz also mentions that during that time Stonebraker was around, working on Ingres (a Postgres predecessor),
and refers to his demands for low-commit latency as driving the attempts on improving disk bandwidth with FFS and, later,  RAID.
Serious work on the RAID taxonomy we know today did not begin before 1987, though.&lt;/p&gt;
&lt;p&gt;The RAID paper was used by many startups and storage companies as the foundation of their development,
among them NetApp, and EMC (via Data General&amp;rsquo;s Clariion Disk Array)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;BSD FFS not only understood CHS geometry of disks, but also processor speed and disk rotational speed.
This allowed it to configure and record in the superblock an &lt;a href=&#34;https://en.wikipedia.org/wiki/Interleaving_%28disk_storage%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;interleave factor&lt;/a&gt;

 to optimize disk I/O throughput.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/interleave.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;


&lt;em&gt;The harddisk rotates continuously, but the CPU needs time to set up the next transfer.
During this time the head may have moved already past the next block start boundary, and now the system would need to wait one full rotation to be able to write.
Using an appropriate interleave factor, blocks of adjacent numbers are not stored adjacently on disk, but instead other blocks are interleaved in-between.
This gives the CPU enough time to think and set up the next block transfer.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The faster the CPU, the lower the interleave factor required.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;All of these optimizations became irrelevant relatively quickly the moment harddrives were sold with integrated controllers,
started to lie about their CHS geometry and ultimately as linear block addresses (LBA) took over.
But for ten to 15 years, this provided a nice performance advantage.&lt;/p&gt;
&lt;h3 id=&#34;large-blocks-smaller-fragments-and-tail-packing&#34;&gt;
    &lt;a href=&#34;#large-blocks-smaller-fragments-and-tail-packing&#34;&gt;
	Large blocks, smaller fragments, and tail packing
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Internally, FFS uses logical blocks of at least 4 KB size.
Anything with at least 4 KB block size can create files of 4 GB size with at most two levels of indirection.&lt;/p&gt;
&lt;p&gt;Large blocks make for faster I/O, but they also come with storage overhead, as files grow in sizes of blocks.
Since logical blocks in FFS are made up from multiple physical blocks, FFS introduces the concept of fragments to expose the smaller internal physical blocks.
Through tail packing, the ends of multiple files can be stored together in the same logical block, using only as many physical blocks as necessary.&lt;/p&gt;
&lt;p&gt;Additional logic was necessary to prevent a slowly growing file from going through phases of fragment-by-fragment growth and constant re-layouting.
To overcome this, space is being pre-allocated to full logical blocks, and tail packing only happens on file close when the preallocation is canceled.&lt;/p&gt;
&lt;h3 id=&#34;long-seek-layout-policy&#34;&gt;
    &lt;a href=&#34;#long-seek-layout-policy&#34;&gt;
	Long Seek Layout Policy
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;BSD FFS introduces a number of layout policies that control the placement of new directories, new files and the handling of large files.
Global policies are mostly concerned with choosing a well-suited cylinder group to place data in,
while local policies then handle the placement inside a cylinder group.&lt;/p&gt;
&lt;p&gt;The new filesystem layout has cylinder groups. Each has their own inode table, and free space bitmaps for inodes and blocks.
The filesystem aims to prevent fragmentation.&lt;/p&gt;
&lt;p&gt;This is of course impossible in certain circumstances:
If, for example, a cylinder group is 512 MB in size, and a file larger than 512 MB is to be written, it will use up one inode in that cylinder group, but all available free blocks are gone.
If a second file is to be placed into this cylinder group, the inode can be used, but the data blocks for that file need to be placed somewhere else – which is undesirable.&lt;/p&gt;
&lt;p&gt;It would be better to force a long seek, a switch from one cylinder group to the next, for large files.
The filesystem would profit from forcing such a long seek every megabyte of filesize or so.
This would use up free blocks from one cylinder group to the next, evenly, while at the same time leaving some number of free blocks for other files in each cylinder group.&lt;/p&gt;
&lt;p&gt;This would, of course, fragment a file, on purpose, but also make sure the fragments are sufficiently large to allow large file I/O.
Fragmentation (non-adjacent placement of blocks in a file) is only really a performance problem if the fragments are too small to be read efficiently.&lt;/p&gt;
&lt;h3 id=&#34;directory-layout-policy&#34;&gt;
    &lt;a href=&#34;#directory-layout-policy&#34;&gt;
	Directory Layout Policy
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Files in the same directory are often used together.
It is useful to place all files in the same directory together in the same cylinder group.&lt;/p&gt;
&lt;p&gt;Of course, when this is done, it is also necessary to put different directories into different cylinder groups, to ensure even use of the filesystem space available.
That means a shell script such as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#! /usr/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; i in &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;seq -w &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 10&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  touch file&lt;span class=&#34;nv&#34;&gt;$i&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  mkdir dir&lt;span class=&#34;nv&#34;&gt;$i&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;will create ten files named &lt;code&gt;fileXX&lt;/code&gt;, which will all be placed in the same cylinder group as the current directory.&lt;/p&gt;
&lt;p&gt;It will also create ten subdirectories of the current directory named &lt;code&gt;dirXX&lt;/code&gt;.
Each of them will be placed in a different cylinder group, if possible.
FFS will choose the cylinder group that has a greater than average number of free inodes, and the smallest number of directories already in it.&lt;/p&gt;
&lt;p&gt;The actual choice of the inode in a cylinder group is &amp;ldquo;next available&amp;rdquo;, so pretty simple.
But that is not a problem, because the whole cylinder group inode table fits into 8-16 blocks.&lt;/p&gt;
&lt;p&gt;For placement of data blocks, a lot of effort is invested into finding rotationally optimal block, given the needed interleave factor for this machine.&lt;/p&gt;
&lt;p&gt;BSD FFS requires some free space to be available in the filesystem at all times.
Many of its algorithms degenerate to the performance of the traditional file system if the filesystem fills up more than 90%.&lt;/p&gt;
&lt;h2 id=&#34;other-changes-and-improvements&#34;&gt;
    &lt;a href=&#34;#other-changes-and-improvements&#34;&gt;
	Other changes and improvements
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;BSD FFS also removes several limits that came with the traditional filesystem.&lt;/p&gt;
&lt;h3 id=&#34;long-inode-numbers-and-block-addresses&#34;&gt;
    &lt;a href=&#34;#long-inode-numbers-and-block-addresses&#34;&gt;
	Long Inode Numbers and Block Addresses
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;For example, &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/BSD-4_3_Tahoe-Snapshot-Development/.ref-BSD-4_3/usr/src/sys/h/dir.h#L42&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inode numbers are now 32-bit numbers&lt;/a&gt;

.
This increases the number of files possible per filesystem from 64 K to 4 G.&lt;/p&gt;
&lt;p&gt;The size of an &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/BSD-4_3_Tahoe-Snapshot-Development/.ref-BSD-4_3/usr/src/sys/h/inode.h#L40-L59&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;inode&lt;/a&gt;

 has doubled:
It is now &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/BSD-4_3_Tahoe-Snapshot-Development/.ref-BSD-4_3/usr/src/sys/h/inode.h#L61-L65&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forced to be 128 bytes&lt;/a&gt;

 in size (with 20 unused bytes)
Also, disk block addresses are now 4 bytes.
At 4 KB block size, this is sufficient to account for 4 G blocks, or a maximum of 16 TB filesystem size.&lt;br&gt;
File length is recorded in a &lt;code&gt;quad&lt;/code&gt;, allowing for more than 4 G individual filesize.&lt;/p&gt;
&lt;p&gt;Inodes now contain 12 direct blocks, and three types of indirect blocks.
At 4 KB block size, this is good for 1024 block addresses per indirect block, resulting in
&lt;code&gt;12 + 1024 + 1024^2 + 1024^3 = 1074791436&lt;/code&gt; blocks per file, or a maximum filesize just north of 4 TB.&lt;/p&gt;
&lt;p&gt;Unix User-ID and Group-ID are still limited to a short, limiting the number of users and groups per system to 64 K.&lt;/p&gt;
&lt;p&gt;Space has been preallocated for 8-byte timestamps, even if the time types in the inode are still limited to 4 bytes.&lt;/p&gt;
&lt;h3 id=&#34;long-filenames&#34;&gt;
    &lt;a href=&#34;#long-filenames&#34;&gt;
	Long filenames
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;The traditional filesystem has directory slots of a fixed 16-byte length,
with 2 bytes for the inode number and 14 bytes for the filename.&lt;/p&gt;
&lt;p&gt;BSD FFS defined a &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/BSD-4_3_Tahoe-Snapshot-Development/.ref-BSD-4_3/usr/src/sys/h/inode.h#L61-L65&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more complex directory entry structure&lt;/a&gt;

.
A single entry contains a 4-byte inode number, a 2-byte record length and a 2-byte name length, and then the actual filename.
Filenames are limited to 255 bytes for each pathname component,
and directory entries are rounded up in length to the next 4-byte boundary.&lt;/p&gt;
&lt;p&gt;Directories are still essentially a linked list, and searching for names in large directories is slow.&lt;/p&gt;
&lt;p&gt;Searching for free space in directories is now more complicated:
To create a new directory entry, we now need to search through the directory from the start, trying to find a gap in the current structure that is large enough for the name we are being asked to create.
If none is found, the new name is appended at the end, growing the directory in size.&lt;/p&gt;
&lt;p&gt;Free space in directories is never reclaimed through compaction, only eventually re-used if a new name happens to fit.&lt;/p&gt;
&lt;h3 id=&#34;symlinks&#34;&gt;
    &lt;a href=&#34;#symlinks&#34;&gt;
	Symlinks
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;The traditional filesystem allowed a file to have multiple names, using the &lt;code&gt;link()&lt;/code&gt; system call and the hardlink mechanism.
Hardlinks are limited in number (a &lt;code&gt;short&lt;/code&gt;, so 64 K names).&lt;/p&gt;
&lt;p&gt;They can be lost accidentally, for example, by saving a hardlinked file with certain editors.
If the editor does write a file as &lt;code&gt;filename.new&lt;/code&gt;, then unlinks the old &lt;code&gt;filename&lt;/code&gt; and moves the new file into place, the hardlinked nature of the file will be modified.&lt;/p&gt;
&lt;p&gt;Hardlinks also reference the original inode of the file multiple times, so they cannot span filesystem boundaries.&lt;/p&gt;
&lt;p&gt;BSD introduces a new filetype (&lt;code&gt;l&lt;/code&gt;, symlink), and places a &amp;ldquo;replacement filename&amp;rdquo; in the linked file, which determines the link target location.
It can be an absolute or relative name (relative to the location of the symlink file).&lt;/p&gt;
&lt;p&gt;This creates a &amp;ldquo;soft&amp;rdquo; or &amp;ldquo;symbolic link.
Trying to access a symlink will kick off a reinterpretation of the filename in &lt;code&gt;namei()&lt;/code&gt; using the replacement filename,
resulting in the attempted &lt;code&gt;open()&lt;/code&gt; system call being deflected to the link target location.&lt;/p&gt;
&lt;p&gt;Since the deflection happens in &lt;code&gt;namei()&lt;/code&gt;, which can traverse filesystem boundaries, the new link type is not subject to the single filesystem limitation.
It is also not counting towards any link count limits.&lt;/p&gt;
&lt;h3 id=&#34;rename-system-call&#34;&gt;
    &lt;a href=&#34;#rename-system-call&#34;&gt;
	Rename System Call
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;BSD introduces the &lt;code&gt;rename()&lt;/code&gt; system call, which previously needed to be implemented as a library function using calls to &lt;code&gt;unlink()&lt;/code&gt; and &lt;code&gt;link()&lt;/code&gt;.
Since this uses more than one system call, the operation is not atomic:
It is subject to partial execution, and it is subject to malicious interferences, because it is a multistep process.&lt;/p&gt;
&lt;h3 id=&#34;quotas&#34;&gt;
    &lt;a href=&#34;#quotas&#34;&gt;
	Quotas
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;BSD also introduces the idea of filesystem usage quotas:
These are soft and hard limits on the number of files and the amount of disk space that a user or a group can use.&lt;/p&gt;
&lt;p&gt;In order to implement them in a useful way, the behavior of the filesystem had to be modified:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is now a privileged operation to change the owner of a file away from oneself.
Without that, it is possible to create a directory that is only accessible for oneself, and then gift all files in it to another user.
The files would then count against that user&amp;rsquo;s quota.&lt;/li&gt;
&lt;li&gt;Similarly, it is now no longer possible to change the group membership of files to just any group.
Instead, only groups from the user&amp;rsquo;s group set can be used.&lt;/li&gt;
&lt;li&gt;And finally, new directories and files inherit their group from their parent directory, not from a users primary group.
That way, project directories would contain files counting against a project&amp;rsquo;s quota, not a user&amp;rsquo;s primary group quota.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advisory-locking&#34;&gt;
    &lt;a href=&#34;#advisory-locking&#34;&gt;
	Advisory Locking
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Advisory file locking is already introduced in 4.2BSD.
For this, the new &lt;code&gt;flock()&lt;/code&gt; syscall has been implemented.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Locks can be shared (read locks) or exclusive (write locks).&lt;/li&gt;
&lt;li&gt;They always apply to the entire file, and not to byte ranges.&lt;/li&gt;
&lt;li&gt;No deadlock detection is attempted.&lt;/li&gt;
&lt;li&gt;They are tied to a file descriptor.
So when a process dies, its file-handles are automatically closed, which also automatically releases all locks held.
This is very robust, until &lt;code&gt;dup()&lt;/code&gt; and &lt;code&gt;fork()&lt;/code&gt; are coming into play.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Posix later tried to improve on this, introducing a second, completely different system of locks, using &lt;code&gt;fcntl()&lt;/code&gt;.
This is flawed in different ways, but can do byte-ranges, and it implements some rudimentary deadlock detection.&lt;/p&gt;
&lt;p&gt;Kernels that implement both systems such as Linux now have two different,
incompatible file locking implementations that do not know of each other.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://loonytek.com/2015/01/15/advisory-file-locking-differences-between-posix-and-bsd-locks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This article&lt;/a&gt;

 discusses all of this some more,
and has example programs.&lt;/p&gt;
&lt;h2 id=&#34;performance&#34;&gt;
    &lt;a href=&#34;#performance&#34;&gt;
	Performance
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The authors note the following advantages in their paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ls&lt;/code&gt; and &lt;code&gt;ls -l&lt;/code&gt; are fast, because the inodes of the files in a single directory are within the same cylinder group.
Hence, reading and listing a directory is very low on seeks, and on seek distance (except for subdirectories, which are guaranteed to be far away).
They measure a 8x speedup for directories without subdirectories.&lt;/li&gt;
&lt;li&gt;Utilization of the theoretical maximal bandwidth increased from 3% in the traditional filesystem to 22% or even 47%, depending on the controller hardware used.
The authors are very proud of the results because they have been achieved on an actual production system with real user production data being layouted,
and not on a synthetic benchmark layout. Throughput is stable over the lifetime of the filesystem, as its file population changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This solves the main drivers for the improvements: Better throughput and a stable layout that does not degrade performance over time.&lt;/p&gt;
&lt;p&gt;Additionally, a number of quality-of-life enhancements have been made, enabling more comfortable working in groups, and unlocking new functionality.&lt;/p&gt;
&lt;p&gt;While Linux contains no BSD code, the ext2 filesystem is pretty much an implementation-blind rewrite of the BSD FFS for Linux,
recreating the features as described in the literature without using any BSD code.&lt;/p&gt;
&lt;p&gt;Both BSD FFS and Linux ext2 are still non-logging filesystems that require a filesystem check after a crash.
They also cannot deal well with directories with many entries, and deal only slightly better with deep directory hierarchies.
Additional changes are required to enable truly large filesystems in order to keep up with increasing storage sizes.&lt;/p&gt;
&lt;p&gt;Also, other limitations of more hidden nature still apply:
Several places in the filesystem code are guarded by locks that make scaling certain operations hard on systems with high concurrency.&lt;/p&gt;
&lt;p&gt;It would take another ten years, until 1994, for SGI&amp;rsquo;s XFS to tackle these things.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>50 years in filesystems: 1974</title>
      <link>https://blog.koehntopp.info/2023/05/05/50-years-in-filesystems-1974.html</link>
      <pubDate>Fri, 05 May 2023 12:13:14 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2023/05/05/50-years-in-filesystems-1974.html</guid>
      <description>&lt;p&gt;Progress is sometimes hard to see, especially when you have been part of it or otherwise lived through it.
Often, it is easier to see if you compare modern educational material, and the problems discussed with older material.
And then look for the research papers and sources that fueled the change.&lt;/p&gt;
&lt;p&gt;In Linux (and Unix in general), this is easy.&lt;/p&gt;
&lt;h1 id=&#34;1974---unix-v7-file-system&#34;&gt;
    &lt;a href=&#34;#1974---unix-v7-file-system&#34;&gt;
	1974 - Unix V7 File System
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;We find the Unix Version 7 Research Release in Diomidis Spinellis &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;unix-history-repo&lt;/code&gt;&lt;/a&gt;

.
If we are reading
&lt;a href=&#34;https://www.amazon.de/Design-UNIX-Operating-System-Prentice/dp/0132017997&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Design of the Unix Operating System&lt;/a&gt;


by Maurice J. Bach
&lt;a href=&#34;https://www.pdfdrive.com/the-design-of-the-unix-operating-system-maurice-bach-e25830714.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;,&lt;/a&gt;


we would want to look at the
&lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/tree/Research-V7-Snapshot-Development&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Research V7 Snapshot&lt;/a&gt;


branch of that Repository.&lt;/p&gt;
&lt;h2 id=&#34;machines&#34;&gt;
    &lt;a href=&#34;#machines&#34;&gt;
	Machines
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;It is 1974.
Computers have a single &amp;ldquo;core&amp;rdquo;, the central processing unit.
In some computers, this is no longer a device with parts, such as boards for the arithmetic logic unit, registers, sequencers and microcode memory, but a single integrated chip.
The new devices are called microcomputers, as opposed to the older generation of minicomputers.
These new CPUs sometimes have thousands of transistors on a single chip.&lt;/p&gt;
&lt;h2 id=&#34;kernels&#34;&gt;
    &lt;a href=&#34;#kernels&#34;&gt;
	Kernels
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;In Unix, we are dealing with system resources as configured in a header file.
&lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/h/param.h&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Default values&lt;/a&gt;


are shown here, and the data structures are arrays, with the values shown being the respective array sizes.
To change them, you edit the file, recompile and relink the kernel, and then reboot.&lt;/p&gt;
&lt;p&gt;We have a file system buffer cache using &lt;code&gt;NBUF&lt;/code&gt; (29) disk blocks of 512 bytes.
We have an inode array of &lt;code&gt;NINODE&lt;/code&gt; (200) entries, and we can mount up to &lt;code&gt;NMOUNT&lt;/code&gt; (8) filesystems concurrently.
A user can have &lt;code&gt;MAXUPRC&lt;/code&gt; (25) processes running, for a total of &lt;code&gt;NPROC&lt;/code&gt; (150) system processes.
Each process can have up to &lt;code&gt;NOFILE&lt;/code&gt; (20) files open.&lt;/p&gt;
&lt;p&gt;Reading Bach and the original V7 sources is interesting, despite the fact that things are completely outdated, because a lot of core concepts are much clearer,
and a lot of structures are a lot simpler.
Sometimes even archaic.
But this is what defines the behavior of Unix File Systems, to this day, because the accidental behavior of V7 Unix became immortalized in the POSIX standard,
and every file system after had to conform to it.
Check &lt;a href=&#34;https://blog.koehntopp.info/2018/11/29/but-is-it-atomic.html#source-dive-why-are-writes-atomic&#34;&gt;But Is It Atomic?&lt;/a&gt;

 for an example.&lt;/p&gt;
&lt;h1 id=&#34;core-concepts&#34;&gt;
    &lt;a href=&#34;#core-concepts&#34;&gt;
	Core Concepts
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;The basic concepts and structures of Unix Filesystems are from this time, and from this system.
Some of them exist even in modern systems.&lt;/p&gt;
&lt;p&gt;The disk is an array of blocks. It begins at block 0, and stretches to block n.
At the beginning of the filesystem we find the &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/h/filsys.h&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;superblock&lt;/a&gt;

.
It is located &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/h/param.h#L89&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at block number 1&lt;/a&gt;

 of the filesystem.
The &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/sys3.c#L128-L192&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mount system call&lt;/a&gt;

 finds an empty &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/h/mount.h#L6-L11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mount&lt;/a&gt;

 structure, reads the superblock off disk and keeps it as part of the mount structure.&lt;/p&gt;
&lt;h2 id=&#34;inode&#34;&gt;
    &lt;a href=&#34;#inode&#34;&gt;
	Inode
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The in-memory superblock has fields for an array of inodes (a &lt;code&gt;short&lt;/code&gt;) on disk.
An &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/h/ino.h&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;inode&lt;/a&gt;

 is a structure that describes a file as a variable length array of blocks, and some metadata.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dinode&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;unsigned&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;short&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;di_mode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;  &lt;span class=&#34;cm&#34;&gt;/* mode and type of file */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;short&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;di_nlink&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;    	      &lt;span class=&#34;cm&#34;&gt;/* number of links to file */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;short&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;di_uid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;      	      &lt;span class=&#34;cm&#34;&gt;/* owner&amp;#39;s user id */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;short&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;di_gid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;      	      &lt;span class=&#34;cm&#34;&gt;/* owner&amp;#39;s group id */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;off_t&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;di_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;     	      &lt;span class=&#34;cm&#34;&gt;/* number of bytes in file */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;  	&lt;span class=&#34;n&#34;&gt;di_addr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;40&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;	  &lt;span class=&#34;cm&#34;&gt;/* disk block addresses */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;time_t&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;di_atime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;   	  &lt;span class=&#34;cm&#34;&gt;/* time last accessed */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;time_t&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;di_mtime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;   	  &lt;span class=&#34;cm&#34;&gt;/* time last modified */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;time_t&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;di_ctime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;   	  &lt;span class=&#34;cm&#34;&gt;/* time created */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#define	INOPB	8	&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;/* 8 inodes per block */&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;/*
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt; * the 40 address bytes:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt; *	39 used; 13 addresses
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt; *	of 3 bytes each.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt; */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;The inode as it appears on disk. 8 inodes fit into a 512-byte disk block, so they are aligned at 64 byte boundaries.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The inode array on the filesystem has a &lt;code&gt;short&lt;/code&gt; count, so there can be up to 65535 inodes in a filesystem.
As each file requires an inode, there can only be that many files per filesystem.&lt;/p&gt;
&lt;p&gt;Each file has some fixed properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(2 bytes) a &lt;code&gt;mode&lt;/code&gt; (the file type and access permissions combined).&lt;/li&gt;
&lt;li&gt;(2 bytes) a link count (&lt;code&gt;nlink&lt;/code&gt;), the number of names this file has.&lt;/li&gt;
&lt;li&gt;(2 bytes) a &lt;code&gt;uid&lt;/code&gt;, the owner.&lt;/li&gt;
&lt;li&gt;(2 bytes) a &lt;code&gt;gid&lt;/code&gt;, the owner&amp;rsquo;s group id.&lt;/li&gt;
&lt;li&gt;(4 bytes) a &lt;code&gt;size&lt;/code&gt;, the length of the file in bytes (defined as an &lt;code&gt;off_t&lt;/code&gt;, a &lt;code&gt;long&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;(40 bytes) an &lt;code&gt;addr&lt;/code&gt; array of disk block addresses&lt;/li&gt;
&lt;li&gt;(3x 4 bytes) three times, an &lt;code&gt;atime&lt;/code&gt; (access time), &lt;code&gt;mtime&lt;/code&gt; (modification time) and &lt;code&gt;ctime&lt;/code&gt; (supposedly create time, but really the time of the last inode change).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;for a total size of 64 bytes.&lt;/p&gt;
&lt;h2 id=&#34;bmap&#34;&gt;
    &lt;a href=&#34;#bmap&#34;&gt;
	bmap()
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;addr&lt;/code&gt; array contains 40 bytes, but it stores 13 disk block addresses, each using 3 bytes.
This is good for 24 bits, or 16 megablocks of 512 bytes, each, for a total filesystem size of 8M kilobytes, or 8 GB.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2023/05/rl02-front.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;


&lt;em&gt;Front panel of a PDP-11 RL02 disk drive, from &lt;a href=&#34;https://www.pdp-11.nl/peripherals/disk/rl-info.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pdp-11.nl&lt;/a&gt;

&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For comparison, a &lt;a href=&#34;https://www.pdp-11.nl/peripherals/disk/rl-info.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDP-11 RL02K disk cartridge&lt;/a&gt;

 held 10.4 MB,
but the newer &lt;a href=&#34;https://lastin.dti.supsi.ch/VET/disks/RA92/EK-ORA90-UG.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RA92&lt;/a&gt;

 could store 1.5 GB.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;addr&lt;/code&gt; array is being used in the &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/subr.c#L9-L120&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bmap() function&lt;/a&gt;

.
The function consumes an inode (&lt;code&gt;ip&lt;/code&gt;) and a logical block number &lt;code&gt;bn&lt;/code&gt; and returns a physical block number.
That is, it maps a block in a file to a block on a disk, hence the name.&lt;/p&gt;
&lt;p&gt;The first 10-block pointers are stored directly in the inode.
That is, to access for example block 0, &lt;code&gt;bmap()&lt;/code&gt; &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/subr.c#L40&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;will look up&lt;/a&gt;

 &lt;code&gt;di_addr[0]&lt;/code&gt; in the inode and return this block number.&lt;/p&gt;
&lt;p&gt;Additional blocks are stored in an indirect block, and the indirect block is stored in the inode.
For even larger files, a double indirect block is allocated, and points to more indirect blocks, and finally very large files need even triple indirect blocks.&lt;/p&gt;
&lt;p&gt;The code &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/subr.c#L60-L73&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first determines the number of indirections&lt;/a&gt;

,
grab the &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/subr.c#L78&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;appropriate indirect block&lt;/a&gt;

,
and then &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/subr.c#L91-L112&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resolve the indirection&lt;/a&gt;

 the appropriate number of times.&lt;/p&gt;
&lt;p&gt;This results in the following famous picture:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/1994/02/filestructure.gif&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Original Unix file structure with increasing numbers of indirect accesses for increasingly larger files.
This forms a compressed array, where short files can be accessed directly with data from the inode, whereas larger files are using increasingly indirect access.
For performance, it is crucial to keep indirect blocks in the file system buffer cache.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How this scales is dependent on the block size (512 bytes back then, 4096 bytes these days), and the size of a block number in bytes (originally 3 bytes, later 4 or even 8 bytes).&lt;/p&gt;
&lt;h2 id=&#34;atomic-writes&#34;&gt;
    &lt;a href=&#34;#atomic-writes&#34;&gt;
	Atomic writes
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Writes to files happen under a lock, so they are always atomic.
This is true even for long writes, which span multiple block boundaries, and is discussed at length in
&lt;a href=&#34;https://blog.koehntopp.info/2018/11/29/but-is-it-atomic.html#source-dive-why-are-writes-atomic&#34;&gt;But Is It Atomic?&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;This also means that even with multiple writer processes, on a single file there can be only ever one disk write active at any point in time.
This is very inconvenient for authors of database systems.&lt;/p&gt;
&lt;h2 id=&#34;naming-files&#34;&gt;
    &lt;a href=&#34;#naming-files&#34;&gt;
	Naming files
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;A directory is a file with a special type (directory), and a &lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/h/dir.h&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fixed record structure&lt;/a&gt;

.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#ifndef	DIRSIZ
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#define	DIRSIZ	14
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#endif
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;direct&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;kt&#34;&gt;ino_t&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;d_ino&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt;	&lt;span class=&#34;n&#34;&gt;d_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DIRSIZ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;A directory entry contains an inode number (an &lt;code&gt;unsigned int&lt;/code&gt;), and a filename which can be up to 14 bytes long. This fits 32 directory entries into a disk block, and 320 directory entries into the 10 disks blocks that can being referenced by the direct blocks of a directory file.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The lower filesystem is a sea of files.
Files have no names, only numbers.&lt;/p&gt;
&lt;p&gt;The upper filesystem uses a special type of file, with a simple 16-byte record structure,
to assign a name of up to 14 characters to a file.
A special function, &lt;code&gt;namei()&lt;/code&gt;
&lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/nami.c#L9-L200&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;converts a filename into an inode number&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;Pathnames passed to &lt;code&gt;namei()&lt;/code&gt; are hierarchical:
they can contain &lt;code&gt;/&lt;/code&gt; as a path separator, and they are being terminated by &lt;code&gt;\0 (nul)&lt;/code&gt;.
&lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/nami.c#L37-L41&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pathnames&lt;/a&gt;

 either start with &lt;code&gt;/&lt;/code&gt;,
in which case the traversal begins at the filesystem root, making the filename absolute.
Or they do not, in which case traversal starts at &lt;code&gt;u.u_cdir&lt;/code&gt;, the current directory.&lt;/p&gt;
&lt;p&gt;The function then consumes pathname component after component,
using the currently active directory and searching linearly for the name of the current component in that directory.
It ends when the last pathname component is found, or if at any stage a component is not found.
It also ends,
if at any point in time, for any directory in the path,
&lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/nami.c#L91&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;we have no x-permission&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dspinellis/unix-history-repo/blob/Research-V7-Snapshot-Development/usr/sys/sys/nami.c#L179&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Some entries are magical&lt;/a&gt;

:
They are mountpoints.
When we encounter them, we change from the directory entry of the current node and filesystem to the root inode of the mounted filesystem.
This makes all filesystems in Unix appear as a single tree, and &amp;ldquo;drives are changed&amp;rdquo; by simply going to a different directory.&lt;/p&gt;
&lt;p&gt;The function ultimately returns a pointer to the inode for the given pathname, creating (or deleting) the inode (and directory entry) if necessary and desired.
It is a centralized point for directory traversal and access permission checks.&lt;/p&gt;
&lt;h1 id=&#34;novel-ideas-and-limits&#34;&gt;
    &lt;a href=&#34;#novel-ideas-and-limits&#34;&gt;
	Novel ideas and Limits
    &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;This very early Unix filesystem has a number of very nice properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It presents multiple filesystems as one single unified tree.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Files are structureless arrays of bytes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;These arrays are stored internally in a variable depth dynamic array, using a system of increasingly deeply nested indirect blocks.
This allows O(1) disk seeks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lower filesystem (creating files) and upper filesystem (structuring files into a tree) are clearly separated.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathname traversal is the only way to get an inode, and along the way permissions are always checked.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are very few characters in filenames that are special, &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;\0 (nul)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also have clear limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Files can only have 16M blocks.&lt;/li&gt;
&lt;li&gt;Filesystems can only have 65535 inodes, which is very limited.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And there are a number of annoying limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There can be only one writer active per file, which kills concurrency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Directory lookups are linear scans, so they become very slow for large directories (more than 320 entries).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There is no system for mandatory file locking.
There are several systems for advisory file locking.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And a few quirks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There is no &lt;code&gt;delete()&lt;/code&gt; system call.
We have &lt;code&gt;unlink()&lt;/code&gt;, which removes a file name,
and files that have zero names and zero open file handles are being automatically collected.
This has a few unusual consequences,
for example, disk space is only freed if a completely unlinked file is also completely closed.
Generations of Unix sysadmins have asked where their disk space is,
when a deleted log file in &lt;code&gt;/var/log&lt;/code&gt; was still kept open by some forgotten process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Initially there is no &lt;code&gt;mkdir()&lt;/code&gt; and &lt;code&gt;rmdir()&lt;/code&gt; system call, which leads to exploitable race conditions.
This is fixed in later versions of Unix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are a few operations that are accidentally atomic (like the write(2) system call), or have been made atomic after they have been exploited (&lt;code&gt;mknod(2)&lt;/code&gt; and &lt;code&gt;mkdir(2)&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Structurally, it is annoying that the inode table and free maps for blocks and inodes are at the beginning of the filesystem, and disk space is allocated linearly from the front of the disk, too.
This leads to a seek intense structure, and enables filesystem fragmentation (in which files are being stored in non-adjacent blocks).&lt;/p&gt;
&lt;p&gt;Traversing a directory structure means reading a directories inode at the beginning of the disk,
going to the data blocks further back,
then reading the next inode of the next pathname component from the beginning of the disk,
and going back the data blocks in the back.
This goes back and forth, once for each pathname component, and is not necessarily fast.&lt;/p&gt;
&lt;h2 id=&#34;today-and-improvements&#34;&gt;
    &lt;a href=&#34;#today-and-improvements&#34;&gt;
	Today, and Improvements
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The PDP-11 V7 Unix filesystem got a faithful reimplementation as the  &lt;code&gt;minix&lt;/code&gt; filesystem, with all its limitations.
In modern Linux, it has been removed from the kernel source tree because it is no longer useful.&lt;/p&gt;
&lt;p&gt;We will see in a later article about the BSD fast filesystem, how the data can be better layouted on disk,
how we can implement longer filenames, more inodes, and how we can speed things up a bit by taking physical properties of the disk into account.&lt;/p&gt;
&lt;p&gt;Only even newer filesystems will be dealing with linear directory lookup times, single writers or limited file metadata.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What are the problems with POSIX?</title>
      <link>https://blog.koehntopp.info/2020/10/05/what-are-the-problems-with-posix.html</link>
      <pubDate>Mon, 05 Oct 2020 17:13:30 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2020/10/05/what-are-the-problems-with-posix.html</guid>
      <description>&lt;p&gt;Every once in a while there is the IT news article that kind of triggers me. This time it was &lt;a href=&#34;https://www.golem.de/news/object-storage-object-storage-protokoll-koennte-posix-abloesen-2010-151294.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Object-Storage-Protokoll könnte Posix ablösen&amp;rdquo;&lt;/a&gt;

 in german computer news site &lt;a href=&#34;https://golem.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Golem&lt;/a&gt;

. The article speaks about mmap(), NVMEoF and object storage and how it could revolutionize or complete object storages, but does not link to an original article, names no persons and no paper. Also, what do these things - mmap, NVMEoF, object storage and Posix, even have in common? It is not explained anywhere in the article.&lt;/p&gt;
&lt;p&gt;In another article &lt;a href=&#34;https://blog.koehntopp.info/2018/11/29/but-is-it-atomic.html&#34;&gt;But is it atomic?&lt;/a&gt;

 we dive into the internals of the old UNIX V7 kernel, and how design decisions and technical realities from the late 1970ies became immortialized as technical requirements for filesystems in the Posix standard.&lt;/p&gt;
&lt;p&gt;So Unix is historically grown, and a lot of things have changed since that time. The original file system (for Linux users: &amp;ldquo;mkfs -t minix&amp;rdquo; is the closest thing to that which you might have) is not even used any more anywhere in a modern system. But it is basically what Posix means when we speak about &amp;ldquo;Posix File System Semantics&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The basic design is good - I mean, it has held up almost 50 years by now, and that is a fantastic achievement. It even has value in modern systems. But it has been designed for local systems, and it is not scalable to distributed systems very well.&lt;/p&gt;
&lt;p&gt;So everybody is either cheating or slow. There is &lt;a href=&#34;https://en.wikipedia.org/wiki/GFS2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GFS2&lt;/a&gt;

 as an example for slow. There is NFS as an example for hidden cheating (I am looking at you, &lt;code&gt;actimeo&lt;/code&gt;, and at you, &lt;code&gt;lockd&lt;/code&gt;). And there is &amp;ldquo;Object Storage&amp;rdquo;, whatever that specifically means, as an example for doing things completely differently.&lt;/p&gt;
&lt;h2 id=&#34;mutability-as-the-root-of-all-evil&#34;&gt;
    &lt;a href=&#34;#mutability-as-the-root-of-all-evil&#34;&gt;
	Mutability as The Root of All Evil
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The biggest baggage Posix has, in terms of distributed systems, is mutable files. You can open a file, fseek() to an offset of 1MB and overwrite a 3MB segment in a 10MB file. I am choosing these sizes and offsets to make it abundantly clear that writes can be large, larger than a network packet, a disk block or other allocation units, and that they can take measureable, non-zero time.&lt;/p&gt;
&lt;h3 id=&#34;one-writer-per-inode-per-time&#34;&gt;
    &lt;a href=&#34;#one-writer-per-inode-per-time&#34;&gt;
	One writer per Inode, per time
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Posix demands that writes are atomic (see link above), and it implemented that originally by locking the (only copy) of the file with a central lock at the in-memory inode of the file. This has multiple effects:&lt;/p&gt;
&lt;p&gt;There can be only one write active at a time for a file.&lt;/p&gt;
&lt;p&gt;This already is a problem for local filesystems, when you have a database that wants to write out many concurrent changes to different non-overlapping records in a database file.&lt;/p&gt;
&lt;p&gt;Databases work around the problem either by defining tablespaces from many small sized &amp;ldquo;.DBF&amp;rdquo; files (Oracle) - 1GB to 4 GB seem to be popular. Or they work around it by  foregoing the use of filesystems entirely - using raw disks, at a terrible toil cost for sysadmins and DBAs. Only XFS in O_DIRECT mode will handle this better, by not blocking concurrent writes as long as they are non-overlapping.&lt;/p&gt;
&lt;p&gt;Reads never happen concurrently with writes, too. They are either completely before or after the write instead. If they happen after the write, they will never return stale data, but always the most recently written fresh data.&lt;/p&gt;
&lt;p&gt;In distributed systems, this is very expensive: Imagine a storage cluster with dozens or hundreds of storage nodes and hundreds or thousands of clients. Parts of the file we are about to write to may have been cached anywhere in any cluster node or client to avoid excessive network transfers. These cached copies are now all invalid, because they contain data that has been overwritten with potentially different values.&lt;/p&gt;
&lt;p&gt;Because reads must not return stale data under Posix, we need to invalidate all of these copies, and - worse - we need to this under a lock to ensure atomicity and fresh reads. So each write (write, not commit/fsync, which can happen much less often) has to inform all relevant nodes with a copy of the data that there is a new write for an extent (offset, length) incoming and wait for the acknowledgement. And then let the write proceed, to wait again to collect the complete acknowledgement of all data nodes that they have the data, and all caches that the have destroyed or updated their cached copy. That&amp;rsquo;s a lot of waiting.&lt;/p&gt;
&lt;p&gt;This is unimaginably slow, and even worse, should the cluster topology change during  any operation - a node taking part in our operation leaving the cluster or similar.&lt;/p&gt;
&lt;p&gt;If you happen to have two concurrent overlapping writes (write A to node 1 at offset 0, length 3MB, and write B to node 12 at offset 1MB, length 3MB), Posix also demands ordering between these writes - A either happens in total before or after B. You will never end up with an ABABABAB block pattern in the overlap, and again, this can only be guaranteed by locking.&lt;/p&gt;
&lt;p&gt;Nothing &amp;ldquo;made&amp;rdquo; Posix specify this, this is just the natural behavior in a single core 1970ies computer when you lock interrupts (&lt;code&gt;spl()&lt;/code&gt;) or &lt;code&gt;plock()&lt;/code&gt; on an in-memory inode, which then was later officially documented and bless into a standard when the Posix specificationw as written. Our modern computers do not have these properties any more, so we need to make them wait to simulate this.&lt;/p&gt;
&lt;p&gt;In any case: The combination of Mutability and Strong Read Consistency/Atomic Write Behavior creates a cluster wide locking problem.&lt;/p&gt;
&lt;h3 id=&#34;fixed-metadata-and-synchronous-metadata-updates&#34;&gt;
    &lt;a href=&#34;#fixed-metadata-and-synchronous-metadata-updates&#34;&gt;
	Fixed metadata, and synchronous metadata updates
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Things get even worse when we look at metadata. The set of metadata in a Unix Inode is fixed, and documented in Posix, too. We have the typical Unix access rights (ugo times rwx) and the three second-resolution times (a/m/ctime), and a file length - that is basically it.&lt;/p&gt;
&lt;p&gt;There is an attempt to extend this, using Posix File ACLs (ACL) and Posix Extended Attributes (xattr), but for example mapping NTFS access permissions to Posix ACLs is complicated - just look at what Samba has to do to get this approximately right. The fact that the Unix notion of a user identity is scoped to a local machine does not help at all - your userid 0 is different from my user id 0 in meaning, and if I am UID 0 on my box I probably should not be UID 0 on yours.&lt;/p&gt;
&lt;p&gt;Posix demands synchronus updates to metadata updates. When you extend a file, this should be visible immediately in the file length, and when you access a file, the atime should update, too. Even on local filesystems this is no longer doable - everybody mounts their Linux disks with a variant of &lt;code&gt;noatime&lt;/code&gt; or the other.&lt;/p&gt;
&lt;p&gt;But while data writes distribute themselves across the cluster, metadata updates do not. They all have to hit the relatively small number of metadata nodes, and this load often crushed metadata servers.&lt;/p&gt;
&lt;h3 id=&#34;the-key-idea-immutability&#34;&gt;
    &lt;a href=&#34;#the-key-idea-immutability&#34;&gt;
	The key idea: Immutability
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Note that all the things we spoke about are lower file system problems - they happen at the level of individual files already, not even going into upper file system, directory hierarchies and path names. That&amp;rsquo;s not even necessary, because as long as the lower file system is not in shape looking up is kind of pointless.&lt;/p&gt;
&lt;p&gt;Object Stores have one single core idea: Files are immutable, maybe appendable (S3 is not, Google GFS and Hadoop HDFS are). The payoff is that we can cache this stuff, because the cache never becomes invalid. Maybe the end of the file is not up to date, but it will be eventually consistent.&lt;/p&gt;
&lt;p&gt;The file has a Key-Value store with metadata, which can even grow to considerable size, and which is also eventually consistent. With the upper file system we do the same: The entire namespace is one single KV store, in which the key is a binary string that cosplays a path name, and the value is the actual file data (and metadata).&lt;/p&gt;
&lt;p&gt;This has far reaching consequences: We can talk about &amp;ldquo;Log Structured Merge Trees&amp;rdquo;, why they became popular in database land, and how that plays nicely with Object Storages. Or we can talk about how &amp;ldquo;Event Sourcing&amp;rdquo; favors &amp;ldquo;append only data structures&amp;rdquo;, and how that plays nicely with Object Storages.&lt;/p&gt;
&lt;h2 id=&#34;there-is-a-timeline-and-there-is-progress&#34;&gt;
    &lt;a href=&#34;#there-is-a-timeline-and-there-is-progress&#34;&gt;
	There is a timeline, and there is progress
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The original Unix File System is, for the sake of illustration, basically the state of the art as seen in 1974.&lt;/p&gt;
&lt;p&gt;There are a lot of elementary improvements to this initial idea that we find reflected in BSD FFS, from 1984.&lt;/p&gt;
&lt;p&gt;The culmination point of these ideas, and structures, can be found in Silicon Graphics XFS, from 1994.&lt;/p&gt;
&lt;p&gt;A completely different way of thinking can be found in &lt;a href=&#34;https://en.wikipedia.org/wiki/Log-structured_file_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Log Structured File Systems&lt;/a&gt;

, first seen in Ousterhout&amp;rsquo;s  Sprite, in the early 1990. But performancewise the original implementation that was a failure: the first successful and performant implementation can be found  - made by Oursterhout&amp;rsquo;s students - in Solaris ZFS, and at the same time in NetApps WAFL, which promptly engaged in a legal battle over patents. In parallel, many ZFS ideas are being reimplemented about half a decade later in Btrfs. All of that would put us roughly into 2004, in terms of commercial availability.&lt;/p&gt;
&lt;p&gt;Now, widespread adoption of Object Storages in persistence products, using Log Structured Merge (LSM) Trees, has taken place. We find them in Elastic Search, in Cassandra, in RocksDB and its many applications, and many more. In a timeline, this would put us into 2014-land.&lt;/p&gt;
&lt;p&gt;LFS-like non-overwriting structures are also at the foundation of all of our storage, deep down in the block-simulation layer of flash storage devices, again around 2014.&lt;/p&gt;
&lt;h2 id=&#34;other-developments&#34;&gt;
    &lt;a href=&#34;#other-developments&#34;&gt;
	Other developments
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Flash Storage originally appeared as a simulated hard disk, SSD. So we have a PCI bus, on which there is a SATA controller, which connects to the simulated hard disks&amp;rsquo; flash controller, which talks to the flash.&lt;/p&gt;
&lt;h3 id=&#34;ssd-nvme-roce-and-nvmeof&#34;&gt;
    &lt;a href=&#34;#ssd-nvme-roce-and-nvmeof&#34;&gt;
	SSD, NVME, RoCE, and NVMEoF
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;NVME is what we get when we ask the question &amp;ldquo;What purpose does the SATA controller have in this setup, anyway?&amp;rdquo; - except slowing everything down, forcing us to use antiquated SCSI commands and serializing access to the highly parallel flash.&lt;/p&gt;
&lt;p&gt;NVME therefore is a PCI bus, which talks to flash controllers, which talk to the flash, and a revised command set for this. The great innovation is to put away the single disk queue and allow many of these. Which allows us to utilize around 800k IOPS per device, even if a single operation can take as long as 1/20.000th of a second - just go 40-way wide, if you can.&lt;/p&gt;
&lt;p&gt;Now, what if we had a way to speak to a NVME device on a different computers PCI bus as if it were local? That&amp;rsquo;s what remote DMA (RDMA) would enable us to do. RoCE (&amp;ldquo;rocky&amp;rdquo;, &amp;ldquo;RDMA over Converged Ethernet&amp;rdquo;) initially was painful - RoCE V1 was unrouteable raw Ethernet with its own Ethertype. RoCE V2 fixed that, and put the entire stuff into UDP instead, so it was routeable, at least within a single data center.&lt;/p&gt;
&lt;p&gt;Turns out, the value in RDMA is more in the Remote than in the DMA, so when RoCE became &amp;ldquo;NVME over Fabric over TCP&amp;rdquo; (NVMEoF/TCP), it was valuable and useful to have, even if you had an ultracheap Broadcom that did not properly DMA the way RoCE needs it. The resulting access to a remote NVME still was as fast as to a local SSD, or even faster.&lt;/p&gt;
&lt;h3 id=&#34;mmap&#34;&gt;
    &lt;a href=&#34;#mmap&#34;&gt;
	mmap()
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;And finally, &lt;code&gt;mmap()&lt;/code&gt; is a Unix system call that allows you to map a file into the address space of a process: when you access a memory page, this memory page is appropriately filled with the content of the file, automatically. Think &amp;ldquo;swap from any file into memory, not just from a swap file&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;For mutable files, and the kings of mutability, relational databases, mmap() is of little use and has many complexities. Databases need extending files, controlled write-out to persistent storage, not overwriting data because MVCC. Also, different data formats on disk and in memory are actually important to people who transact, and hence mmap() does help very little for most databases. We can talk about &amp;ldquo;Optane&amp;rdquo;, &amp;ldquo;PMEM&amp;rdquo;, and &amp;ldquo;transactional memory&amp;rdquo;, but that&amp;rsquo;s a different lecture for another day.&lt;/p&gt;
&lt;p&gt;Making an &lt;em&gt;immutable&lt;/em&gt; file available &lt;em&gt;read-only&lt;/em&gt; in memory has none of these complexities, and is actually very useful and efficient. This is what mmap() is really, really good at - you get clean, discardable memory pages, none of the file extension problems, none of the &amp;ldquo;which pages are written to disk at what point in time&amp;rdquo; sychronisation problems.&lt;/p&gt;
&lt;p&gt;The idea is so good that Bryan Cantrill actually had it first, and foreshadowed &amp;ldquo;Serverless&amp;rdquo; in &lt;a href=&#34;https://github.com/joyent/manta&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Manta&lt;/a&gt;

. Basically, Joyent starts a &amp;ldquo;Container&amp;rdquo;, actually a BSD &amp;ldquo;jail&amp;rdquo; or Solaris &amp;ldquo;zone&amp;rdquo;, because containers weren&amp;rsquo;t fully invented, yet. It then maps a kind of not-quite-S3 object into it and your code can  access this, and emit output results into a new object in the same or a different bucket.&lt;/p&gt;
&lt;p&gt;The mmap() in this is a nice sugar on top of it, as it makes reads automatic,  en-passant and makes I/O minimal. The important fact is that Input and Output are distinguished and the input file is still immutable. Because it is, the munmap() is unimportant - exit() is also a munmap() and since the object is immutable it is unimportant if it stays mapped.&lt;/p&gt;
&lt;h2 id=&#34;putting-it-all-together&#34;&gt;
    &lt;a href=&#34;#putting-it-all-together&#34;&gt;
	Putting it all together
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;So you have a page fault and have to get a page from local storage.&lt;/p&gt;
&lt;p&gt;So why not get it from Ethernet, via RDMA, remotely, from any disk anywhere in your data center? Can we have this as a kernel extension and - more important - standardized in the NVME spec?&lt;/p&gt;
&lt;p&gt;Ah, now we&amp;rsquo;re talking. &amp;ldquo;We have standardized remote page fault reads over NVMEoF&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s the article - at least, that is what I think it is, because it does not say so, and it references nothing.&lt;/p&gt;
&lt;p&gt;But even so, it needs 50 years of Unix as a context to be understandable.&lt;/p&gt;
&lt;p&gt;I still don&amp;rsquo;t know what the original article was, but there is stuff such as &lt;a href=&#34;https://www.usenix.org/conference/nsdi20/presentation/yang&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FileMR: Rethinking RDMA Networking for Scalable Persistent Memory&lt;/a&gt;

 that is being worked on.&lt;/p&gt;
&lt;p&gt;Based on &lt;a href=&#34;https://twitter.com/isotopp/status/1313084116569645057&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a twitter tread&lt;/a&gt;

.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>When a file changes, do a thing</title>
      <link>https://blog.koehntopp.info/2019/11/19/when-a-file-changes-do-a-thing.html</link>
      <pubDate>Tue, 19 Nov 2019 12:49:53 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2019/11/19/when-a-file-changes-do-a-thing.html</guid>
      <description>&lt;p&gt;When developing there is often an edit-compile-test cycle, or an
edit-distribute-changes cycle or a similar repetetive task. You
could poll changes, for example with cron every minute or
similarly, but that is wasteful and slow.&lt;/p&gt;
&lt;p&gt;All modern operating systems have mechanisms for processes to
subscribe to file or directory changes. In MacOS, we do have the
&lt;a href=&#34;https://developer.apple.com/library/archive/documentation/Darwin/Conceptual/FSEvents_ProgGuide/TechnologyOverview/TechnologyOverview.html#//apple_ref/doc/uid/TP40005289-CH3-SW1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;File System Events&lt;/a&gt;


API since 10.5, in Linux we got three different implementations
(as described in &lt;a href=&#34;https://lwn.net/Articles/604686/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LWN&lt;/a&gt;

): The
original dnotify, its replacement inotify and the even more
recent fanotify (which got its own &lt;a href=&#34;https://lwn.net/Articles/605128/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LWN
article&lt;/a&gt;

). BSD has kqueue.&lt;/p&gt;
&lt;p&gt;The idea is that you subscribe to a directory and get notified
for change/create/delete/rename events inside that directory
and/or all events recursively beneath that starting point (a
&amp;lsquo;root&amp;rsquo;). You would be interested into the type of change and the
name of the file path that changes, and you would probably want
to be able to retrieve lists of these changes in batch.&lt;/p&gt;
&lt;p&gt;To make that useful, you would need a shell interface to this,
and there are quite a few by now.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The most convenient seems to be
&lt;a href=&#34;https://github.com/clibs/entr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;entr&lt;/a&gt;

, because it works most
closely with shell programs.&lt;/li&gt;
&lt;li&gt;There is also &lt;a href=&#34;https://facebook.github.io/watchman/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;watchman&lt;/a&gt;

,
but this requires submitting jobs and processing results in
Javascript to fully use its potential.&lt;/li&gt;
&lt;li&gt;One of the first programs to use filesystem subscriptions is
&lt;a href=&#34;https://github.com/emcrisostomo/fswatch/wiki/How-to-Use-fswatch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fswatch&lt;/a&gt;

,
but while highly portable, it is cumbersome to use. Instead of
running commands, it just reports filenames to feed into a
pipe to handle.&lt;/li&gt;
&lt;li&gt;Ruby seems to have a library called
&lt;a href=&#34;https://github.com/guard/guard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Guard&lt;/a&gt;

 that also comes with
an interface to shell, but can also being used as a ruby gem.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://hackage.haskell.org/package/spy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spy&lt;/a&gt;

 is a weird piece
of Haskell that produces a small binary that can run commands
on file system changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Python seems to come with a bunch of modules and interfaces in
various states of disrepair,
&lt;a href=&#34;https://github.com/rvoicilas/inotify-tools&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;inotify-tools&lt;/a&gt;

, the
very tiny wrapper &lt;a href=&#34;https://pypi.org/project/inotify_simple/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;inotify_simple&lt;/a&gt;


(the simple here refers to the fact that it is a tiny
wrapper around the C library, not simple to use), the more
convenient &lt;a href=&#34;https://pypi.org/project/inotify/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;inotify&lt;/a&gt;

 and the
high level wrapper
&lt;a href=&#34;https://pythonhosted.org/watchdog/quickstart.html#a-simple-example&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;watchdog&lt;/a&gt;

.&lt;/p&gt;
&lt;h2 id=&#34;a-test-scenario&#34;&gt;
    &lt;a href=&#34;#a-test-scenario&#34;&gt;
	A test scenario
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;As a test scenario I have a &lt;code&gt;ship-to-kvm&lt;/code&gt; command that I want to
run on every file change. It looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;rsync -e ssh -t -v --delete --delete-excluded --exclude=&amp;#39;.git&amp;#39; -r \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;  ~/git_tree/myproject \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;  devuser@devbox.example.com:myproject
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;when I save my local file from my local editor so that the tree
myproject is made available on my devbox.&lt;/p&gt;
&lt;h2 id=&#34;entr&#34;&gt;
    &lt;a href=&#34;#entr&#34;&gt;
	entr
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;With &lt;a href=&#34;https://github.com/clibs/entr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;entr&lt;/a&gt;

, that is rather
simple. The package &lt;code&gt;entr&lt;/code&gt; is available in Homebrew on MacOS
(&lt;code&gt;brew install entr&lt;/code&gt;) or as a package in Linux (&lt;code&gt;yum install -y entr&lt;/code&gt;, &lt;code&gt;apt install entr&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;You ask entr to watch a list of files or a directory, and when
things change to run a command. You can hit space to force
execution even when nothing changed, or &lt;code&gt;q&lt;/code&gt; to end the command.&lt;/p&gt;
&lt;p&gt;Various ways to handle changes are provided:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;$&lt;/span&gt; ls *.js &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; entr -r node myproject.js
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;-r&lt;/code&gt; option here will SIGTERM the node instance, wait for it
to complete and then restart it.&lt;/p&gt;
&lt;p&gt;To get notification of new and deleted files, you need to watch
directories, which are inferred from a file list. This is done
with the &lt;code&gt;-d&lt;/code&gt; option and in fact the command terminates so you
need to wrap it in a loop:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; :&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;&amp;gt;&lt;/span&gt;   ls &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; entr -d ship-to-kvm
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are a few other options, but these two should cover the
most common use cases.&lt;/p&gt;
&lt;h2 id=&#34;watchman&#34;&gt;
    &lt;a href=&#34;#watchman&#34;&gt;
	watchman
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://facebook.github.io/watchman/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;watchman&lt;/a&gt;

 is the facebook
take on things. It consists of a daemon that is automatically
started when you are using the frontend command, and a frontend
command that actually does not expose all the functionality
unless you feed it JSON job files. All command results are also
JSON.&lt;/p&gt;
&lt;p&gt;watchman has the concept of roots, filesystem subtrees that are
being watched, and then triggers that are attached to roots or
subtrees of roots, and are being run on change. A simple
predicate language and a selection of regex libraries can be
used to formulate conditions for triggers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;$&lt;/span&gt; watchman watch ~/git_tree/myproject &lt;span class=&#34;c1&#34;&gt;# this will start the daemon&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;$&lt;/span&gt; watchman -j ship-to-kvm.json        &lt;span class=&#34;c1&#34;&gt;# this defines the job&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and the actual job definition is then something like&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;trigger&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;~/git_tree/myproject&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ship-to-kvm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;expression&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;pcre&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;^[a-zA-Z0-9]&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;command&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ship-to-kvm&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This may look nicer to developers, but I seem to prefer the entr
way of doing things.&lt;/p&gt;
&lt;h2 id=&#34;python-watchdog&#34;&gt;
    &lt;a href=&#34;#python-watchdog&#34;&gt;
	Python watchdog
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The Python library
&lt;a href=&#34;https://pythonhosted.org/watchdog/quickstart.html#a-simple-example&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;watchdog&lt;/a&gt;


provides a convenient programmatic interface to inotify and friends
by defining an Observer class and scheduling operations to the
observer when there are events outstanding.&lt;/p&gt;
&lt;p&gt;The example from the manual looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ch&#34;&gt;#! /usr/bin/env python&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sys&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;time&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;logging&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;watchdog.observers&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Observer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;watchdog.events&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LoggingEventHandler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;logging&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;basicConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;level&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logging&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;INFO&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%(asctime)s&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; - &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%(message)s&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;datefmt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;%Y-%m-&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%d&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; %H:%M:%S&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sys&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;argv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sys&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;argv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;event_handler&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LoggingEventHandler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;observer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Observer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;observer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;schedule&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;event_handler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;recursive&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;observer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sleep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;KeyboardInterrupt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;observer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;observer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and when run does things like this for a &lt;code&gt;touch keks; sleep 1; rm keks&lt;/code&gt; in a secondary shell:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2019-11-19 14:36:40 - Modified directory: ./.git
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2019-11-19 14:36:44 - Created file: ./keks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2019-11-19 14:36:44 - Modified directory: .
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2019-11-19 14:36:44 - Modified directory: ./.git
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2019-11-19 14:36:53 - Deleted file: ./keks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2019-11-19 14:36:53 - Modified directory: .
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2019-11-19 14:36:53 - Modified directory: ./.git
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The actual observer selection allows a rich palette of event
classes and filters, so dispatching and filtering events is easy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An abundance of IOPS and Zero Jitter</title>
      <link>https://blog.koehntopp.info/2017/07/26/an-abundance-of-iops-and-zero-jitter.html</link>
      <pubDate>Wed, 26 Jul 2017 14:51:55 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2017/07/26/an-abundance-of-iops-and-zero-jitter.html</guid>
      <description>&lt;p&gt;Two weeks ago, I wrote about
&lt;a href=&#34;https://blog.koehntopp.info/2017/07/07/the-data-center-in-the-age-of-abundance.html&#34;&gt;The Data Center in the Age of Abundance&lt;/a&gt;


and claimed that IOPS are - among other things - a solved problem.&lt;/p&gt;
&lt;p&gt;What does a solved problem look like?&lt;/p&gt;
&lt;p&gt;Here is a benchmark running 100k random writes of 4K per second, with zero
Jitter, at 350µs end-to-end write latency across six switches. Databases
really like reliably timed writes like these. Maximum queue depth would be
48, the system is not touching
that.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2017/07/pure-storage1.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;and here is iostat on the iSCSI client running the test&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2017/07/pure-storage2-1024x238.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;100k random writes, 4k write size, inside a 2 TB linux file of random data,
on a 15 TB filesystem with XFS, on an LVM2 volume provided by iSCSI over a
single 10 GBit/s interface, with six switch hops between the linux client
and the array.&lt;/p&gt;
&lt;p&gt;The array claims 150µs latency, on the linux we measure around 350µs. Out of
that, there are less than 50µs from the switches and 150µs or more from the
Linux storage stack (and that is increasingly becoming an issue).&lt;/p&gt;
&lt;p&gt;Tested product was a &lt;a href=&#34;https://www.purestorage.com/products/flasharray-x.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Purestore Flasharray-X&lt;/a&gt;

,
client was Dell PowerEdge R630, 2x E5-2620v4, 128G, 10GBit/s networking.&lt;/p&gt;
&lt;p&gt;Thanks, Peter Buschman!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neue Ideen in Dateisystemen (oder: BTRFS in Fedora 11)</title>
      <link>https://blog.koehntopp.info/2009/02/06/neue-ideen-in-dateisystemen-oder-btrfs-in-fedora-11.html</link>
      <pubDate>Fri, 06 Feb 2009 16:34:20 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2009/02/06/neue-ideen-in-dateisystemen-oder-btrfs-in-fedora-11.html</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;There are two kinds of fool. One says, “This is old, and therefore good..” And one says “This is new, and therefore better..”	—John Brunner, in The Shockwave Rider&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Neue Ideen in Dateisystemen sind so eine Sache. Es handelt sich bei einem Dateisystem ja um Infrastrukturcode par excellence, und so reden die meisten Leute gerne von ihrem letzten Datenverlust, wenn man sie nach Dateisystemen befragt. Das ist nicht neu, ich habe in &lt;a href=&#34;https://blog.koehntopp.info/2008/05/30/the-importance-of-fail.html&#34;&gt;The Importance Of FAIL&lt;/a&gt;

 das Thema ja schon mal angeschnitten.&lt;/p&gt;
&lt;p&gt;Neue Ideen in Dateisystemen sind auch langsam. 1984 bis 1992 gab es das
&lt;a href=&#34;http://en.wikipedia.org/wiki/Sprite_operating_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sprite Projekt&lt;/a&gt;

 in Berkeley, bei dem es um die Entwicklung eines rechnerübergreifenden Betriebssystems ging. Teil von Sprite war auch etwas, das sich &lt;a href=&#34;http://en.wikipedia.org/wiki/Log-structured_file_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LFS (Log Structured Filesystem)&lt;/a&gt;

 nannte.&lt;/p&gt;
&lt;p&gt;LFS basiert auf der Idee, daß nur noch Writes auf eine Platte übrig bleiben, wenn man nur genug RAM zum Cachen aller Reads hat. Also quasi die Situation, in der Google jetzt grad ist. Wenn das aber so ist, so geht die Überlegung weiter, dann muß man Daten auf der Platte nicht zum Lesen optimiert ablegen, sondern das Schreiben optimieren.&lt;/p&gt;
&lt;h2 id=&#34;ein-exkurs-in-fragmentierung&#34;&gt;
    &lt;a href=&#34;#ein-exkurs-in-fragmentierung&#34;&gt;
	Ein Exkurs in Fragmentierung
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Gegeben sei MS-DOS, also ein System mit nur einem Thread, der Read-Requests generieren kann und quasi ohne Read-Cache, der solche Reads wegoptimieren kann. Dann ist das Dateisystem dann für viele Anwendungen zum Lesen optimiert, wenn alle Dateien defragmentiert abspeichert sind, d.h die Blocknummern der physikalischen Blöcke jeder Datei unmittelbar aufeinanderfolgend sind.&lt;/p&gt;
&lt;p&gt;Gegeben sei ein System mit unendlich viel Speicher, das schon unendlich lange läuft. Dann wiederum sind alle Daten im RAM gecached, und die physikalische Anordnung der Daten auf der Platte ist aus der Sicht der Lesezugriffe total schnurz.&lt;/p&gt;
&lt;p&gt;Die meisten realen Systeme liegen irgendwo dazwischen - je mehr RAM und je besser die Caches vorgeglüht, desto mehr ist es egal, wie die Daten auf der Platte angeordnet sind.&lt;/p&gt;
&lt;p&gt;Die meisten realen Systeme haben heutzutage auch mehr als einen Thread, der Requests erzeugen kann und schon von daher ist das statisch lineare Layout von Dateien auf der Platte nicht mehr unmittelbar ein Garant für dynamisch lineare Lesezugriffe.&lt;/p&gt;
&lt;h2 id=&#34;lfs-dreht-den-spieß-um&#34;&gt;
    &lt;a href=&#34;#lfs-dreht-den-spie%c3%9f-um&#34;&gt;
	LFS dreht den Spieß um
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;LFS nimmt nun ein solches System mit unendlich viel Speicher und unendlich langer Laufzeit an, d.h, kümmert sich nicht um die Linearisierung von Reads, sondern lediglich noch um die Linearisierung von Writes: Das Dateisystem &lt;em&gt;hat&lt;/em&gt; kein Log, es &lt;em&gt;ist&lt;/em&gt; ein Log - ein großer Ringpuffer von Daten, bei dem die Platte von vorne nach hinten beschrieben wird und wenn man am Ende der Platte angekommen ist, fängt man von vorne an.&lt;/p&gt;
&lt;p&gt;Snapshots bekommt man bei einem solchen System gratis (Aber
&lt;a href=&#34;http://en.wikipedia.org/wiki/Log-structured_File_System_%28BSD%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BSD hat es nicht implementiert&lt;/a&gt;

): Am Anfang der Platte hat man keinen Superblock, sondern einen Zeiger auf die letzten paar Superblöcke, die geschrieben wurden. Jede Operation kann als Transaktion geschrieben werden: Blöcke werden dabei nicht überschrieben, sondern geänderte Versionen des neuen Blockes werden linear rausgeschrieben.&lt;/p&gt;
&lt;p&gt;Beispiel: Eine Datei besteht aus einem Verzeichniseintrag, einer Inode und in der Inode aus Zeigern auf Datenblöcke. Überschreibt man nun das letzte KB der Datei und verlängert sie nun auch noch um ein KB, dann werden zunächst der &amp;ldquo;überschriebene&amp;rdquo; Datenblock und der neue Datenblock am Ende des Schreibpuffers neu geschrieben, dann wird die geänderte Inode der Datei dahinter neu geschrieben und dann der Block, der auf die aktuelle Version dieser Inode zeigt neu geschrieben und am Ende ein neuer Superblock.&lt;/p&gt;
&lt;p&gt;Beim Lesen folgt man dem neusten Superblock, findet man die neuste Version der Inode, und damit die geänderte und verlängerte Datei. Folgt man der älteren Kopie des Superblocks, findet man eine ältere Version der Inode derselben Datei und die dazu gehörenden älteren Versionen der Datenblöcke, also eine alte Version derselben Datei. Alle Blöcke, die zwischen beiden Versionen der Datei unverändert bleiben, sind beiden Versionen gemeinsam und nur einmal auf der Platte vorhanden.&lt;/p&gt;
&lt;h2 id=&#34;lfs-performance-stinkt&#34;&gt;
    &lt;a href=&#34;#lfs-performance-stinkt&#34;&gt;
	LFS Performance stinkt
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Nun ist es so, daß außerhalb des Googleplex RAM endlich und die Laufzeiten von Computern begrenzt sind. Daher ist es auch so, daß es zu Situationen kommen kann, in denen der File System Buffer Cache das aus Lesesicht absolut pessimale Layout von LFS nicht abschirmen kann. In solchen Fällen - die besonders von Datenbanken gerne provoziert werden - ist die Performance von LFS nur mit geologischen Fachbegriffen zu erfassen.&lt;/p&gt;
&lt;p&gt;Schon LFS auf Sprite, und sein späterer Port auf BSD Unix haben daher einen Repacker gehabt. Das ist ein Prozeß der Idlezeiten der Platte nutzt und die Daten auf dem Medium ein wenig read-freundlicher anordnet. Man kann sich das wie eine dauernd laufende Defragmentierung im Hintergrund vorstellen. Auch ReiserFS 4, das auf ähnlichen Ideen basiert hat einen solchen Repacker.&lt;/p&gt;
&lt;h2 id=&#34;in-with-the-out-old-with-the-new&#34;&gt;
    &lt;a href=&#34;#in-with-the-out-old-with-the-new&#34;&gt;
	In with the out, old with the new
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Die Idee, Daten niemals zu überschreiben ist sicherlich gut. Sie läßt sich jedoch auch mit Dateisystemen implementieren, die Daten gleich beim ersten Schreiben sinnvoll auf der Platte layouten ohne dabei eine Ringpuffer-Struktur zu erzeugen.&lt;/p&gt;
&lt;p&gt;Dateisysteme wie
&lt;a href=&#34;http://en.wikipedia.org/wiki/Write_Anywhere_File_Layout&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WAFL&lt;/a&gt;

 auf einer NetAPP sind ein erster Schritt in diese Richtung, Suns
&lt;a href=&#34;http://en.wikipedia.org/wiki/ZFS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ZFS&lt;/a&gt;

 geht ihn noch konsequenter. Beide Dateisysteme stammen aus Projekten, in denen Leute arbeiten, die vorher mit verschiedenen Versionen von LFS gearbeitet haben. Insofern ist es auch nicht weiter verwunderlich, daß solche Ideen in diesen Dateisystemen auftauchen - &lt;a href=&#34;http://www.sun.com/lawsuit/zfs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;außer man ist Patentanwalt&lt;/a&gt;

 und wundert sich aus beruflichen Gründen.&lt;/p&gt;
&lt;p&gt;Solche Dateisysteme, die nie überschreiben (und daher laufend snapshotten), aber die Platte nicht als Ringpuffer betrachten sondern schon noch layouten, nennt man Copy-On-Write Dateisysteme (COW-FS).&lt;/p&gt;
&lt;h2 id=&#34;weitere-gute-ideen-importieren&#34;&gt;
    &lt;a href=&#34;#weitere-gute-ideen-importieren&#34;&gt;
	Weitere gute Ideen importieren
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;ZFS ist in diesem Zusammenhang besonders interessant, weil es noch weitere gute Ideen von anderswo importiert. &lt;a href=&#34;http://en.wikipedia.org/wiki/Episode_filesystem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Episode&lt;/a&gt;

 zum Beispiel ist das Dateisystem der halb vergessenen DCE-Initiative, und vielen Leuten in der Geschmacksrichtung &lt;a href=&#34;http://en.wikipedia.org/wiki/AdvFS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AdvFS&lt;/a&gt;

 von DEC bekannt.&lt;/p&gt;
&lt;p&gt;AdvFS integriert das Storagemangement, das sonst von einem Logical Volume Manager erledigt wird und das Platzmanagement des Dateisystems ineinander. Eine File Domain kann man sich dabei wie eine Volume Group vorstellen - eine Art Kiste, in der die Blöcke enthalten sind, die beschrieben werden können.&lt;/p&gt;
&lt;p&gt;In der File Domain sind File Sets vorhanden, Dinge, die man anderswo Dateisysteme nennt. File Sets ist dabei nicht zwingend eine feste Größe zugewiesen - man kann sie sich wie Luftballons vorstellen, die in der Kiste sind und die nach bedarf aufgeblasen und verkleinert werden können. Mit einem Quota-System kann man einem File Set einen Mindestplatzbedarf und einen Maximalbedarf zuordnen und so den Platz in der File Domain verwalten. Der &lt;a href=&#34;http://advfs.sf.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AdvFS-Source&lt;/a&gt;

 ist seit 2008 GPLed, aber er interessiert kaum noch jemanden.&lt;/p&gt;
&lt;p&gt;ZFS hat wie AdvFS diese Integration von Volume Management in das Dateisystem übernommen - etwas, das auf den Linux-Kernel-Mailinglisten von einigen Personen als &lt;a href=&#34;http://www.google.de/search?q=blatant&amp;#43;layering&amp;#43;violation&amp;amp;ie=utf-8&amp;amp;oe=utf-8&amp;amp;aq=t&amp;amp;rls=org.mozilla:en-US:official&amp;amp;client=firefox-a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blatant layering violation&lt;/a&gt;

 angesehen wurde - die Redewendung hat es zu einigem Google-Karma gebracht. Tatsächlich handelt es sich um eine andere Sicht auf den Stack, aber eine, die durchaus von anderen geteilt wird, die wie man an AdvFS sehen kann und die durchaus interne Struktur hat.&lt;/p&gt;
&lt;p&gt;Eine andere gute Idee, die ZFS aus der Datenbankwelt übernommen hat, sind Prüfsummen überall. Auf diese Weise - und nur auf diese Weise - ist es möglich, die Integrität des Dateisystems Ende-zu-Ende sicherzustellen und vor allen Dingen auch integre Teile des Systems zur Verfügung zu stellen, während man beschädigte Teile isoliert und abtrennt. Der ZFS-Code kann es nicht, aber grundsätzlich ist es möglich aus jedem defekten ZFS ein integres Sub-ZFS raus zu extrahieren und zu publizieren, während der Rest anderweitig recovered wird - die Datenstrukturen geben das her.&lt;/p&gt;
&lt;h2 id=&#34;open-source-management&#34;&gt;
    &lt;a href=&#34;#open-source-management&#34;&gt;
	Open Source Management
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Nun hat Sun ein sehr sauberes und durchaus &lt;a href=&#34;http://en.wikipedia.org/wiki/CDDL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;politisches-strategisches Intellectual Property Management&lt;/a&gt;

. Das bewirkt, daß der Source von ZFS unter der &lt;a href=&#34;http://en.wikipedia.org/wiki/CDDL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CDDL&lt;/a&gt;

 (&amp;ldquo;cuddle&amp;rdquo;, eine modifizierte MPL 1.1) verfügbar ist - er ist Teil von Opensolaris, FreeBSD, MacOS X und einigen anderen Systemen. Die CDDL verletzt aber die additional restrictions clause der GPL und ist mit der GPL nicht kompatibel - es wäre zwar technisch möglich einen Linux-Kernel zu bauen, der ZFS enthielte, aber es ist keinem Distributionshersteller juristisch möglich so etwas zu verteilen. ZFS läuft daher unter Linux nur als Userland-Prozeß als Teil von &lt;a href=&#34;http://en.wikipedia.org/wiki/Filesystem_in_Userspace&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FUSE&lt;/a&gt;

.&lt;/p&gt;
&lt;h2 id=&#34;cow-fs-in-linux---btrfs&#34;&gt;
    &lt;a href=&#34;#cow-fs-in-linux---btrfs&#34;&gt;
	COW-FS in Linux - BTRFS
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Die Linux-Crowd ficht das nicht an. Im Linux-Kernelspace entwickelt man seit einiger Zeit an einem Nachfolger für die extX-Reihe von Dateisystemen, und einer der Kandidaten für eine solche Nachfolge ist &lt;a href=&#34;http://btrfs.wiki.kernel.org/index.php/Main_Page&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BTRFS&lt;/a&gt;

 (&amp;lsquo;ButterFS&amp;rsquo;, nicht etwa &amp;lsquo;BetterFS&amp;rsquo;). Die Featureliste von BTRFS liest sich wie eine Shoppingliste in den Stammbäumen der oben genannten Ahnherrschaften: Extent-Based wie ext4 und XFS statt Bitmaps wie ext3 und ZFS, Tail-Packing wie bei Reiser3 und Reiser4, Verzeichnisse als Bäume wie inzwischen allgemein üblich, dynamisch erzeugte Inodes, wie sich fast zwingend aus COW-FS-Erfordernissen ergibt, writeable-snapshots, subvolumes (File Sets aus AdvFS), Object Level Mirroring und Striping, Checksums on Everything, Compression (und sicher auch Encryption), Integrated Multiple Device Support (besagte blatant layering violation), Online Filesystem Check (o.a. Teilvalidierung) und Online System Defragmentation (ein Repacker und die Möglichkeit des Online Filesystem Checks machen das leicht).&lt;/p&gt;
&lt;p&gt;Die BTRFS-Leute ziehen dabei ein paar echt eklige, aber vollkommen legale Stunts ab. In &lt;a href=&#34;http://btrfs.wiki.kernel.org/index.php/Conversion_from_Ext3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conversion from ext3&lt;/a&gt;

 wird gezeigt, wie die Metadatenstrukturen von BTRFS in ein existierendes ext3 reingemalt werden können ohne das ext3 zu beschädigen und wie kurzzeitig beide Systeme parallel existieren und die Datenblöcke teilen können. Das erlaubt eine Konvertierung von extX-Dateisystemen in BTRFS ohne Neuformatierung - ein echtes Killerfeature für Leute mit einem Arsch voll Linux-Daten.&lt;/p&gt;
&lt;p&gt;Einen schönen Überblick über die BTRFS-Strukturen findet man im &lt;a href=&#34;http://btrfs.wiki.kernel.org/index.php/Btrfs_design&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Design Dokument&lt;/a&gt;

. Zum Thema &lt;a href=&#34;http://btrfs.wiki.kernel.org/index.php/Multiple_Device_Support&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multiple Device Support&lt;/a&gt;

 gibt es ebenfalls Seiten. Und eine Einführung in den Code gibt es auch - &lt;a href=&#34;http://btrfs.wiki.kernel.org/index.php/Code_documentation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hier&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;BTRFS ist unter der GPL lizensiert, kompatibel mit dem Linux-Kernel und Bestandteil aktueller Standardkernel. Die gestern freigegebene &lt;a href=&#34;http://btrfs.wiki.kernel.org/index.php/Code_documentation#Sample_Item_Insertion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fedora 11 Alpha&lt;/a&gt;

 enthält BTRFS als experimentelles Dateisystem - gut genug zum Spielen und Testen, aber noch nicht gut genug für Wirkdaten.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Was bringt ext4?</title>
      <link>https://blog.koehntopp.info/2009/01/23/was-bringt-ext4.html</link>
      <pubDate>Fri, 23 Jan 2009 19:14:35 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2009/01/23/was-bringt-ext4.html</guid>
      <description>&lt;p&gt;Der Linux-Kernel 2.6.28 enthält das ext4-Dateisystem standardmäßig und sowohl Fedora als auch Ubuntu werden es unterstützen. Was bringt ext4 an Änderungen?&lt;/p&gt;
&lt;p&gt;Ein Dateisystem ist für die meisten Benutzer eine quasi unsichtbare Sache. Es sind halt Dateien da und wenn man auf diese zugreift hat man halt Daten. So sind Dateisystem-Features für die meisten Leute also eine sehr unspektakuläre Sache. Die folgende Übersicht ist also etwas geekzentrisch.&lt;/p&gt;
&lt;h2 id=&#34;extents&#34;&gt;
    &lt;a href=&#34;#extents&#34;&gt;
	Extents
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;ext2 und ext3 sind sehr traditionelle Dateisysteme, die intern im Grunde auf Technik von 1984 basieren. ext2 ist eine minimal verbessere Nachprogrammierung des BSD ffs (bei Sun und MacOS X: ufs), ext3 fügt dem lediglich das Journal zur schnelleren Wiederherstellung nach Systemcrashes zu. Beide Dateisysteme speichern die Metadaten von Dateien in einer Inode ab und merken sich die Lage der Datenblöcke in einer Datei in einem (in sogenannten Indirect-Blocks gefalteten) Array von Blocknummern. In einem Artikel von 1994 habe ich das mal &lt;a href=&#34;http://kris.koehntopp.de/artikel/dateisysteme/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;länger ausgeführt&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;Da mit einigem Glück die meisten Dateien nicht fragmentiert gespeichert sind, besteht dieses Array also bei vielen Dateien aus einer langen Folge von unmittelbar aufeinander folgenden Blocknummern. Das ist unglaublich ineffizient.&lt;/p&gt;
&lt;p&gt;XFS, also Technik von 1994, ist eines der ersten Dateisysteme gewesen, das stattdessen mit Extents arbeitet, also Blockfolgen durch Run Length Encoding komprimiert: Statt einer Folge von Blocknummern wird die Startnummer der Folge und ihre Länge als ein Paar gespeichert. 15 Jahre später führt man dieses Detail auch in ext4 endlich ein.&lt;/p&gt;
&lt;h2 id=&#34;große-dateisysteme&#34;&gt;
    &lt;a href=&#34;#gro%c3%9fe-dateisysteme&#34;&gt;
	Große Dateisysteme
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Eine andere Technik von XFS kopiert man dabei nicht: Kompression von Blocknummern. ext4 unterstützt große Dateisysteme durch den Einsatz von 64 Bit großen Blocknummern. Da die Kernel-Infrastruktur noch nicht entsprechend mitgewachsen ist, sind derzeit 48 Bit große Blocknummern nutzbar, was für immerhin schon Exabyte-große Dateisysteme ausreicht. Anders als XFS, das ebenfalls ein 64 Bit-Dateisystem ist, speichert ext4 jedoch immer ganze 8 Byte große Blocknummern, während XFS auch relative Blockadressen zum Beginn jeweils einer Zone verwenden kann und so an vielen Stellen mit 4 Byte langen Zahlen auskommt, auch wenn es 64 Bit adressieren kann.&lt;/p&gt;
&lt;p&gt;Weil ext2 und ext3 im Grunde saubere Rewrites von FFS waren, haben sie auch die 16 Bit große Linkcount-Zahl von diesem geerbt und konnten so bis zu 32767 Links pro Datei verwalten. Da Unterverzeichnisse durch den &amp;ldquo;..&amp;quot;-Eintrag den Linkcount des Elternverzeichnisses erhöhen, war man so auf 32765 Unterverzeichnisse pro Directory beschränkt. ext4 geht hier anders vor und das Limit existiert nicht mehr.&lt;/p&gt;
&lt;h2 id=&#34;neues-blockbelegungsschema&#34;&gt;
    &lt;a href=&#34;#neues-blockbelegungsschema&#34;&gt;
	Neues Blockbelegungsschema
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Jedes Dateisystem hat Funktionen mit denen es entscheidet auf welchen physischen Blöcken der Platte eine Datei zu liegen kommt. Ziel diese Allokators ist es dabei, die Fragmentierung von Dateien zu verhindern und die Dateien so anzuordnen, daß schnell lesen darauf zugegriffen werden kann.&lt;/p&gt;
&lt;p&gt;Der Allkokator von ext2 und ext3 ist dabei notorisch schlecht. Zum einen beherrschen diese Dateisyteme keine verzögerte Blockzuweisung: Jeder Block einer Datei muß physikalisch angeordnet werden sobald der Kernel Platz für den Block im File System Buffer Cache belegt. Wenn also eine große Datei linear geschrieben wird bedeutet das, daß das Dateisystem schon versucht den ersten Block der Datei zu positionieren ohne abzuwarten ob und wie viele weitere Blöcke noch folgen werden.&lt;/p&gt;
&lt;p&gt;XFS hatte schon 1994 eine bessere Strategie: Blöcke werden im File System Buffer Cache auch ohne physikalische Positionsinformation gecached. Dadurch kann etwa ein linearer Write einer ganzen Datei erst einmal im Cache abgelegt werden und erst am Ende, wenn die Datei geschlossen und geflushed wird, muß eine Layoutentscheidung getroffen werden. Diese kennt dann aber schon die Gesamtgröße der Datei und das Dateisystem kann versuchen die Datei am Stück zu schreiben. ext4 kann nun endlich auch solche delayed allocation und reiht sich so neben XFS, ZFS, btrfs und Reiser4 ein.&lt;/p&gt;
&lt;p&gt;ext2 und ext3 belegten dabei den Platz auf der Platte einzelblockweise (in 4KB großen Blöcken) und haben dabei maximal 8 Blocks in Folge im Voraus belegt. Wenn man also ein Verzeichnis hat, in dem zwei Dateien gleichzeitig offen sind und verlängert werden (etwa: /var/log), dann entstehen so Zonen von jeweils 32 KB großen Dateistummeln, die sich gegenseitig im Weg stehen. Die Dateien sind maximal fragmentiert. ext4 fixt das dann auch in der ext?-Serie von Dateisystemen endlich.&lt;/p&gt;
&lt;p&gt;ext4 bekommt außerdem ein Feature, das sich &amp;ldquo;persistent preallocation&amp;rdquo; nennt und das es Anwendungen erlaubt, dem Dateisystem schon vorab Hinweise darauf zu geben wie groß Dateien am Ende sein werden wenn die Anwendung mit ihnen fertig sein wird. Die Anwendungen dafür sind vielfältig: Mit ein wenig Management ließe sich so zum Beispiel das Guaranteed Rate I/O von XFS nachprogrammieren und ext4 kennt mit diesem Feature und ein wenig weiterer Magie auch Online-Defragmentierung.&lt;/p&gt;
&lt;h2 id=&#34;schnelleres-fsck-und-prüfsummen&#34;&gt;
    &lt;a href=&#34;#schnelleres-fsck-und-pr%c3%bcfsummen&#34;&gt;
	Schnelleres fsck und Prüfsummen
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;So wie ZFS und InnoDB es vorgemacht haben führt auch ext4 nun endlich Prüfsummen ein: Das Journal und Teile der Inode-Infrastruktur bekommen nun Prüfsummen, mit denen die Integrität der Daten nach einem Crash schneller gecheckt werden kann, sodaß der fsck optimiert werden kann und schneller abläuft wenn er dennoch einmal notwendig werden sollte. Dadurch kann auch der Journal-Commit selbst optimiert werden.&lt;/p&gt;
&lt;p&gt;Von einer durchgehenden Prüfsummen-Infrastruktur wie in ZFS und einer Online-Prüfung von Checksummen ist man in ext4 jedoch noch weit entfernt.&lt;/p&gt;
&lt;h2 id=&#34;inode-basteleien&#34;&gt;
    &lt;a href=&#34;#inode-basteleien&#34;&gt;
	Inode-Basteleien
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Die Größe einer Inode ist in ext4 gewachsen: 256 Byte Minimumgröße statt bisher 128 Bytes. ext4 macht mit dem zusätzlichen Platz sinnvolle Sachen: Dateien bekommen Timestamps, die in Nanosekunden statt wie bisher in Sekunden rechnen, time_t stirbt und der verbleibende Platz kann verwendet werden um Extentlisten oder erweiterte Dateiattribute inline zu speichern.&lt;/p&gt;
&lt;p&gt;Außerdem kann ext4 Inodes reservieren und dynamisch verwalten. Dadurch werden auch Metadata-Operationen sehr viel schneller und layouten sich besser physikalisch auf der Platte.&lt;/p&gt;
&lt;h2 id=&#34;barrier-writes&#34;&gt;
    &lt;a href=&#34;#barrier-writes&#34;&gt;
	Barrier Writes
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Schließlich erzeugt auch ext4 nun ohne Journal=Data immer konsistente Datenstrukturen auf der Platte, indem Barrier Writes verwendet werden.&lt;/p&gt;
&lt;h2 id=&#34;live-upgrade&#34;&gt;
    &lt;a href=&#34;#live-upgrade&#34;&gt;
	Live Upgrade
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;ext4 kann ext3-Dateisysteme lesen. Es ist also möglich, ein ext3-Dateisystem als ext4 zu mounten. Die meisten Features, die ein ext4 einem ext3 voraus hat, sind dabei jedoch nicht nutzbar. Auch ein&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;tune2fs -O extents,uninit_bg,dir_index /dev/DEV
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;fsck -pf /dev/DEV
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;schaltet diese Features nur für neue Dateien um, baut aber die vorhandenen Dateien und Verzeichnisse nicht auf das neue Format um. Die volle Effizienzsteigerung erlangt man also nur durch ein Umkopieren der Daten auf ein neue angelegtes, leeres ext4-Dateisystem.&lt;/p&gt;
&lt;h2 id=&#34;was-fehlt-noch&#34;&gt;
    &lt;a href=&#34;#was-fehlt-noch&#34;&gt;
	Was fehlt noch?
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Grub unterstützt ext4 bisher noch nicht (d.h. die Grubs, die das können, haben es noch in keine Distro geschafft).&lt;/p&gt;
&lt;p&gt;(siehe auch:
&lt;a href=&#34;http://kernelnewbies.org/Ext4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kernelnewbies: ext4&lt;/a&gt;

)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Filesysteme für theclaw (30 Jahre Unix Source)</title>
      <link>https://blog.koehntopp.info/2006/12/26/filesysteme-fuer-theclaw-30-jahre-unix-source.html</link>
      <pubDate>Tue, 26 Dec 2006 19:09:59 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2006/12/26/filesysteme-fuer-theclaw-30-jahre-unix-source.html</guid>
      <description>&lt;p&gt;&amp;mdash; Log opened Di Dez 26 15:52:09 2006&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Hey :] &lt;a href=&#34;https://blog.koehntopp.info/2006/05/08/fragmentierung-fuer-jannik.html&#34;&gt;Spitze erklaerung zu ext2.&lt;/a&gt;

&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Danke&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Bist du Kerneldeveloper?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Nein. Mysql Consultant.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Hmm. Hab da was nicht verstanden bei der Erklärung. Und zwar: Was sind Datenblockzeiger?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Die Blockadressen von Datenblöcken einer Datei.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ich paste mal was&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;(0-11):9711-9722, (IND):9723, (12-267):9724-9979, (DIND):9980, (IND):9981, (268-523):9982-10237, (IND):10238, (524-779):10239-10494, (IND):10495, (780-1035):10496-10751, (IND):10752, (1036-1291):10753-11008, (IND):11009, (1292-1547):11010-11265, (IND):11266, (1548-1795):11267-11514
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Isotopp&amp;gt; Habs im
&lt;a href=&#34;%28/2006/05/08/fragmentierung-fuer-jannik.html%29&#34;&gt;Originalartikel&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; &lt;code&gt;__le32 i_block[EXT2_N_BLOCKS];&lt;/code&gt; Das ist das, was unter BLOCKS bei debugfs steht?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt;
&lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2_fs.h#L211&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/include/linux/ext2_fs.h#L211&lt;/a&gt;

: Das ist was auf der Platte steht.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Okay mal durchdenken.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Die Definition steht in
&lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2_fs.h#L165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/include/linux/ext2_fs.h#L165&lt;/a&gt;

. Es kommen also &lt;code&gt;EXT2_NDIR_BLOCKS&lt;/code&gt; direkt, also in der Inode selbst. Das sind (0-11):9711-9722.&lt;/p&gt;
&lt;p&gt;Dann kommt ein &lt;code&gt;EXT2_IND_BLOCK&lt;/code&gt;, (IND):9723. Der steht auch in der Inode, aber der zeigt nicht auf Daten, sondern auf einen Indirect Block. Der enthält die Blocknummern der Datenblöcke, (12-267):9724-9979.&lt;/p&gt;
&lt;p&gt;Dann kommt &lt;code&gt;EXT2_DIND_BLOCK&lt;/code&gt;. Der wiederum enthält keine Blocknummern von Datenblöcken, sondern die Blocknummern von Indirect Blocks, die wiederum Blocknummern von Datenblöcken enthalten. Daher
&lt;a href=&#34;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; &lt;em&gt;anseh&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; In der inode steht nur (DIND):9980. In DIND steht dann (IND):9981 und (IND):10238 und so weiter. Und in (IND):9981 stehen dann 9982-10237, in (IND):10238 dann 10239-10494.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; DIR steht für &amp;ldquo;direct&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; ja&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Muss das jetzt mal kurz mit debugfs ausprobieren.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Sieh mal
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L665&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L665&lt;/a&gt;

. Das &lt;code&gt;ext2_bmap&lt;/code&gt; geht über &lt;code&gt;generic_block_bmap&lt;/code&gt; nach
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L547&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L547&lt;/a&gt;

. Und das wiederum benutzt
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L196&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L196&lt;/a&gt;

. Und da siehst du den lookup.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; okay langsam kommts&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Wenn &lt;code&gt;i_block&lt;/code&gt;&amp;lt;0 -&amp;gt; error. Wenn &lt;code&gt;i_block&lt;/code&gt;&amp;lt;&lt;code&gt;direct_blocks&lt;/code&gt;, dann direkt. Sonst IND, sonst DIND, sonst TIND. Sonst bumm.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Wart mal, nicht so schnell. Ich kann das nicht alles gleichzeitig aufnehmen. Also, die ersten zwölf Blöcke sind -direkt- in der Inode?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja. Blocknummern. Nicht Blöcke.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; &lt;code&gt;i_block&lt;/code&gt; ist schon ein element aus &lt;code&gt;i_block[]&lt;/code&gt; oder? ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Wo bist du gerade? Also in welcher zeile?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; &lt;code&gt;static int ext2_block_to_path&lt;/code&gt;. Bei der Definition da.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Kannst du eine lxr url geben bitte? Sonst wird das schwer hier. Ah, hier:
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L196&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L196&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; ja&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; &lt;code&gt;i_block&lt;/code&gt; ist der 2. Parameter der Funktion, der Aufruf steht in
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L547&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L547&lt;/a&gt;

. Da ist es &lt;code&gt;iblock&lt;/code&gt;,
das wird durchgereicht vom 2. Parameter von &lt;code&gt;ext2_get_block&lt;/code&gt; &lt;code&gt;iblock&lt;/code&gt;. Das wiederum ist
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L665&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L665&lt;/a&gt;

, der das über den Umweg von
&lt;a href=&#34;http://lxr.linux.no/source/fs/buffer.c#L2759&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/buffer.c#L2759&lt;/a&gt;

 aufruft.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Nicht gerade trivial.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Für den Kernel schon. Das geht da überall so, inzwischen. Man gewöhnt sich dran, das Lesen zu können. Die
Alternative ist Code Duplication, und das nervt noch mehr. Anyway, &lt;code&gt;sector_t&lt;/code&gt; ist ein unsigned 64 bit
(long long, 8 byte) in i386. Eine Blocknummer.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; In
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L556&lt;/a&gt;

, das ist die wichtige Stelle, hast du die &lt;code&gt;inode&lt;/code&gt; und den &lt;code&gt;iblock&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Die &lt;code&gt;inode&lt;/code&gt; hat das 12-Elemente direct block array usw im Speicher und &lt;code&gt;iblock&lt;/code&gt; ist der Offset. Die Frage,
die in
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L556&lt;/a&gt;

 geklärt werden muss
ist: wie tief müssen wir runter steigen - für die Blöcke 0-11 gar nicht, für die Blöcke 12- einen Level und so weiter.
Das klärt &lt;code&gt;ext2_block_to_path&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Den Abstieg sehen wir dann in
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L562&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L562&lt;/a&gt;

.
Und der Abstieg klappt entweder, weil das File schon einen Block hat an der Stelle &lt;code&gt;iblock&lt;/code&gt;
(&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L564&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L564&lt;/a&gt;

),
oder es klappt nicht und wir müssen Blöcke beschaffen
(nach &lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L575&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L575&lt;/a&gt;

).&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Sekunde. Bin kein Kernelmensch ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Aber das ist doch nur gewöhnliches C.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Na ja, trotzdem komplex (für mich). Erstmal eine Frage. Man hat ein FS, und will die Inode nummer 23, wie wird die gefunden?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Über die Verzeichnisse. Wir wissen, / hat die inode 2. Das ist definiert in
&lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2_fs.h#L60&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/include/linux/ext2_fs.h#L60&lt;/a&gt;

,
also lesen wir das File mit der inode 2 durch, und parsen es als
&lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2_fs.h#L510&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/include/linux/ext2_fs.h#L510&lt;/a&gt;

 Strukturen.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Sind die indodes nicht nacheinander abgepeichert in den BGs?
Also die erste BG enthält die ersten X Inodes, die zweite BG die zweiten X usw&amp;hellip;&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Aeh, ah. Ich verstehe. Userland kann nichts mit Inodes machen, nur mit Filenamen. Es gibt kein &lt;code&gt;openi()&lt;/code&gt;.
Also müssen alle Funktionen im Userland immer Namen angeben, und du kommst dann vom Namen zur Inode über das kernel-interne &lt;code&gt;namei()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ja, klar. Aber der Kernel will ja Inode X irgendwie kriegen können.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja, intern. Das weiß er, weil im Superblock ja steht, wie viele Inodes pro bg vorhanden sind, und er dann aus
der Inodenummer / inodes_per_bg sofort die bg nummer ausrechnen kann, und dann sofort weiß, wo die inode
stehen muss auf der Platte. Eine Inodenummer ist also implizit auch die Blockadresse der Inode auf der Platte.&lt;/p&gt;
&lt;p&gt;Hier ist der Superblock:
&lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2_fs.h#L341&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/include/linux/ext2_fs.h#L341&lt;/a&gt;

,
und &lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2_fs.h#L352&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/include/linux/ext2_fs.h#L352&lt;/a&gt;

 ist
die &lt;code&gt;s_inodes_per_group&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Und die Inode wird dann in
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L998&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L998&lt;/a&gt;

 gelesen.&lt;/p&gt;
&lt;p&gt;Meine Rechnung von eben ist hier
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L1012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L1012&lt;/a&gt;

.  &lt;code&gt;(ino - 1) / EXT2_INODES_PER_GROUP(sb);&lt;/code&gt;
und &lt;code&gt;((ino - 1) % EXT2_INODES_PER_GROUP(sb)) * EXT2_INODE_SIZE(sb);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Nix für ungut aber für mich ist der Code grad ned so hilfreich.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Was ist das Problem?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Bin grad bisschen überfordert.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Du hast eine Inode Nr 23. Du weisst, pro bg hast Du sagen wir 8192 Inodes. Und (23-1) / ext2_indes_per_group(sb) = 0.
Also ist inode 23 in bg 0.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; &lt;em&gt;wartmal&lt;/em&gt; 8192?!&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; 8192 bei 1 kb blockgroesse, 32768 bei 4kb&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Wie gross ist eine Inode nochmal?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; 128 bytes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;linux/ext2_fs.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;stdio.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ext2_inode&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nf&#34;&gt;printf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;%d&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;und&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kris@linux:~&amp;gt; make probe
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;make: &amp;#34;probe&amp;#34; ist bereits aktualisiert.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;kris@linux:~&amp;gt; ./probe
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;128
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;theclaw&amp;gt; Also pro BG ist allein 1 MB bzw 4 MB an Inodes reserviert?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja. Eine bg ist 8 MB oder 128 MB gross. Schau, hast du ein ext2 da?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; ja&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Dann mach mal ein &lt;code&gt;debugfs /dev/...&lt;/code&gt; da drauf. Ist read only, macht also nix kaputt. Dann mach mal &lt;code&gt;show_super_stats&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Inodes per group:         2008
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Inode count:              26104
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;für ein &lt;code&gt;/dev/sda5     ext2     99M  6.7M   87M   8% /boot&lt;/code&gt;
und &lt;code&gt;26104*128/1024 = 3263&lt;/code&gt;, also 3263 KB oder 3.2M für alle Inodes.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Aber meine Frage ist eine Andere.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; 21:40 Isotopp&amp;gt; und das wichtigste in &lt;code&gt;struct: __le32 i_block[EXT2_N_BLOCKS];&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; &lt;code&gt;i_block[12]&lt;/code&gt; ist ein indirekter Block? &lt;em&gt;aaargh&lt;/em&gt; Die Adresse eines indirekten Blockes?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja.
&lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2_fs.h#L165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/include/linux/ext2_fs.h#L165&lt;/a&gt;

. Dort ist
&lt;code&gt;#define EXT2_IND_BLOCK EXT2_NDIR_BLOCKS&lt;/code&gt; und weiter ist
&lt;code&gt;#define EXT2_NDIR_BLOCKS 12&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Also 15 &lt;code&gt;EXT2_N_BLOCKS&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Also ist &lt;code&gt;i_block[0]&lt;/code&gt; bis &lt;code&gt;i_block[11]&lt;/code&gt; direct, &lt;code&gt;i_block[12]&lt;/code&gt; indirect, und &lt;code&gt;i_block[13]&lt;/code&gt; DIND und &lt;code&gt;i_block[14]&lt;/code&gt; TIND. Alles in allem also 15.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Sind das die Faktoren die die max. Dateigröße in ext2 bestimmen?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Das sind die Faktoren, die die maximale Blocknummer bestimmen. Dateigröße ist Blockgröße mal maximale Blocknummer.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Also ja ;) Indirekt halt.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; 9711-9722: Sind das die &amp;ldquo;Adressen&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Blocknummern, ja, Adressen auf der Platte. In
&lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2_fs.h#L234&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/include/linux/ext2_fs.h#L234&lt;/a&gt;

 siehst du als Typ uebrigens &lt;code&gt;__le32&lt;/code&gt;. Das ist definiert in
&lt;a href=&#34;http://lxr.linux.no/source/include/linux/types.h#L172&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/include/linux/types.h#L172&lt;/a&gt;

 und endet als &lt;code&gt;__u32&lt;/code&gt;, also unsigned 32 bit. Also 2^32 Blöcke. Bei 4 KB Blöcken sind das 17592186044416 Bytes, oder 16 TB, bei 1 KB Blöcken nur 4 TB.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Diese Blöcke haben aber nix mit den Blöcken bei ext2 zu tun? Oder doch?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; &lt;code&gt;show_super_stats&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; &lt;code&gt;Block size:               1024&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; In meinem Fall also auch maximale Dateigröße 4 TB. 4 Gigablocks.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Okay, dann noch eine Frage:&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; TOTAL: 1804 und Blockcount: 3608, huh? Warum *2?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Hmm, da rechnet jemand mit 512 Byte Hardwaresektoren, warum auch immer.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:~ # ls -lsi /boot/vmlinuz-2.6.13-15-default
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; 28 1513 -rw-r--r--  1 root root 1541719 Sep 13  2005 /boot/vmlinuz-2.6.13-15-default
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Isotopp&amp;gt; Inode 28, 1513 Blöcke auf der Platte, Dateilaenge 15411719 Bytes. Rechnerisch ist  &lt;code&gt;1541719/1024 = 1505.5849&lt;/code&gt;. 7 Blöcke Verwaltungsoverhead. Und zwar&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;(0-11):11515-11526, 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;(IND):11527, (12-267):11528-11783, 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;(DIND):11784, (IND):11785, (268-523):11786-12041, 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;              (IND):12042, (524-779):12043-12298, 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;              (IND):12299, (780-1035):12300-12555, 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;              (IND):12556, (1036-1291):12557-12812, 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;              (IND):12813, (1292-1505):12814-13027
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;TOTAL: 1513
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Isotopp&amp;gt; (IND):11527, (DIND):11784, (IND):11785, (IND):12299, (IND):12556, (IND):12813 &amp;lt;- das sind 6.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; 1541719/1024 = 1505.5849 sind 1506. Plus 6 sind 1512.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Er meint total sei 1513. Wieso?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ich versteh das sowieso nicht, warum da 1513 angezeigt wird. 1292-1505 ist das letzte und dann total 1513. Evtl noch die Metainfos dazu?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Nein, aber die DIND und IND Blocks. Für die Blöcke 12-267 wird ja ein IND gebraucht, für die Blöcke 268-1505 wird ein DIND und vier IND gebraucht. 6 blocks Extra. Siehe noch einmal
&lt;a href=&#34;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&lt;/a&gt;

. Das rechts sind die Daten. In der Inode stehen die ersten paar Datenblocknummern direkt, in der zeichnung 10, in ext2 sind es 12. Dann steht in der Inode die Nummer vom IND, und im IND die Blocknummern der Datenblöcke, hier 12-267. Das ist also 1 block overhead, wenn das file mehr als 12 blocks lang wird. Dann ein DIND, wenn der 268&amp;rsquo;te block gebraucht wird und für jeweils 256 Blocks ein IND dazu.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Du erklaerst so schnell.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # dd if=/dev/zero of=kris bs=1k count=12
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # ls -ls kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;12 -rw-r--r--  1 root root 12288 Dec 26 16:58 kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;12 Blöcke, 12288 Bytes Länge. Und nun:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # dd if=/dev/zero of=kris bs=1k count=13
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # ls -ls kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;14 -rw-r--r--  1 root root 13312 Dec 26 16:58 kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Ein 1k länger, 14 Blocks statt 12.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Wie findet man die Größen der BGs eines dateisystems heraus?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Lies &lt;code&gt;show_super_stats&lt;/code&gt; von &lt;code&gt;debugfs&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # export DEBUGFS_PAGER=cat
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # debugfs /dev/sda5
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;debugfs 1.38 (30-Jun-2005)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;debugfs:  show_super_stats
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Inode count:              26104
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Block count:              104388
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Block size:               1024
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Blocks per group:         8192
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Inodes per group:         2008
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Inode blocks per group:   251
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Isotopp&amp;gt; Wird einiges klarer?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ein bisschen. Also wenn ich z.B. block nummer X habe, dann ist (nummer X)/(blocks per group) die BG nummer gell?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja, aber das interessiert nicht. Du redest ja von Blöcken.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Der n-te Block der Datei x kann irgendwo liegen. Wo, das sagt dir die Inode.  Normal hast du ja ein File, und eine Position in einem File.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ich kann doch einfach von der Adresse auf der Platte X * bytes_per_block lesen?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Na ja, als root schon. Sonst nicht. debugfs macht das ja, die Disk als raw device auf und dann direkt auf die Blöcke klettern. Niemand sonst tut so etwas ausser debugfs und fsck. Alle anderen machen FILES auf und lesen dann am OFFSET in dem File. Punkt ist, dass du normal mit Files arbeitest und nicht mit Blöcken. Der Kernel arbeitet mit Blöcken. Und er muss irgendwie vom File + Offset auf den Block kommen.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ja Klar&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Unser ext2 hier hat 1 KB Blocksize. Wir lesen das File &lt;code&gt;/boot/vmlinuz-2.6.13-15-default&lt;/code&gt; (inode 36). Und zwar am Offset 1000000 (1 mio). Der wievielte Block im File ist das?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; &lt;em&gt;denk&lt;/em&gt;. Erstmal hat man ja nur den Dateinamen.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja, das kümmert uns gerade noch nicht. Offset 1 mio &amp;ndash; welcher block? 1000000/1024 = 976.5625. Also Bytes 0-1023 sind Block 0, Bytes 1024-2047 sind Block 1 und so weiter.  In unserem fall also block 976. 976*1024=999424, 1000000-999424=576. Byte 1 000 000 steht also in Block 976, an Position 576 in diesem Block.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Okay. Stop mal. Hab da gleich ne Frage dazu:&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; 21:40 Isotopp&amp;gt; und das Wichtigste in &lt;code&gt;struct: __le32 i_block[EXT2_N_BLOCKS];&lt;/code&gt;
theclaw&amp;gt; dieser kontext: &lt;code&gt;i_block[976]&lt;/code&gt; brauchen wir da also. Ack? Und dazu noch das offset dazu?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja, aber den kriegen wir nicht so.
&lt;a href=&#34;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ja, das wollte ich grad sagen. :-P Blocks 0-11 kriegen wir so. Muss man sich halt den Weg durchhangeln.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; blocks 12-267 kriegen wir über den IND (single indirect block). Und blocks 268- kriegen wir über den DIND und den passenden IND.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; 976-12=964, 964-256=708. 708/256=2.7656. Also müssen wir über den 2. IND des DIND gehen.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ich weiss nicht ganz was du da rechnest.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Naja, 1 KB blockgroesse, 4 byte pro blocknummer, also 256 blockadressen pro Block. 976ter Block ist gefragt.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; ay&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; 12 direct blocks, also 964 blocks dahinter.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Also mit einem &amp;ldquo;indirekten block&amp;rdquo; kann man 256 andere Blöcke adressieren, wie pointer in C&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ich hab noch ganz grundlegende Fragen. Was wir wirklich wollen, ist doch das mapping logische Ext2block-Adresse der Datei → physische Blockadresse. Richtig?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; In diesem speziellen Fall: ja. Die allgemeine Formulierung lautet so: Wir haben ein Quadrupel (major number, minor number, inode number, offset in bytes), das ist ein Device, eine Partition (maj, min), und in dem Device ein File (inode), und in dem File eine Byteposition. Und wir wollen ein Tripel (maj, min, blockno), also in der partition (maj, min) den zu dieser Datei gehörenden physikalischen Block.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; jo&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Weil (maj, min) bei dieser Abbildung konstant sind (wir arbeiten immer innerhalb derselben partition), vergessen wir maj und min und reden von einer Funktion die (ino, offset) auf (phy block) abbildet. Das nennt man ein Mapping. Und zwar ein Mapping für Datenblöcke. Daher heisst die funktion &lt;code&gt;bmap&lt;/code&gt;. Jedes Dateisystem hat so eine Funktion, daher reden wir hier über die bmap funktion von ext2, die heisst also sinnigerweise &lt;code&gt;ext2_bmap&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Schonmal sauhilfreich. ext2_bmap: Jetzt kann ich mir was darunter vorstellen. Danke. &lt;em&gt;codesuch&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Es ist eine diskrete Funktion.&lt;/p&gt;
&lt;p&gt;y = mx+k. Das sind kontinuierliche Funktionen von R-&amp;gt;R.&lt;/p&gt;
&lt;p&gt;Wir arbeiten hier mit diskreten, endlichen Funktionen. Die werden in der Regel als Lookuptable realisiert.&lt;/p&gt;
&lt;p&gt;Es gibt also eine Wertetabelle, die jedem (ino, offset) ein (phy block) zuordnet.&lt;/p&gt;
&lt;p&gt;Die Wertetabelle &lt;em&gt;ist&lt;/em&gt; die Inode. Eine Inode ist also ein Array von Blocknummern.&lt;/p&gt;
&lt;p&gt;Wenn es ein naives Array waere, dann waere die Inode variabel groß und für große Dateien sehr, sehr gross. Das ist wenig effizient.&lt;/p&gt;
&lt;p&gt;Daher hat man die Inode komprimiert, für kleine dateien (bis 12 blocks) speichert man die Wertetabelle tatsächlich &lt;em&gt;in&lt;/em&gt; der Inode (i_blocks[0-11]),
aber stell Dir dieses Verfahren mal für 1000 Blocks vor. Das wäre doof.&lt;/p&gt;
&lt;p&gt;Also speichert man die Wertetabelle für die Blöcke 12-267 nicht in der Inode, sondern in einem für diesen Zweck bestellten block, indem indirect block und in der Inode nur den einen Eintrag für diesen Block.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ich habs soweit gecheckt.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Das kann man beweisen.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # dd if=/dev/zero of=kris bs=1k count=12
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;12 blocks a 1 KB (ich hab ja ein ext2 mit 1 KB blocks).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # ls -ls kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;12** -rw-r--r--  1 root root 12288 Dec 26 17:30 kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;12288 bytes lang 12 blocks belegt. Nun mal 13 KB.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # dd if=/dev/zero of=kris bs=1k count=13
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # ls -ls kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;14 -rw-r--r--  1 root root 13312 Dec 26 17:30 kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;13312 bytes, aber 14 blocks! Da ist er, der IND.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; &lt;em&gt;selbstausprobier&lt;/em&gt; Ist das die Anzahl der Blöcke für das Inode inklusive den Daten?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Das ist die Anzahl der Blöcke OHNE die inode selber (Die belegt 128 byte in der Inodetable), also Daten + IND + DIND + TIND. Kann man auch beweisen.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # dd if=/dev/zero of=kris bs=1k count=0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # ls -ls kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;0 -rw-r--r--  1 root root 0 Dec 26 17:33 kris
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Isotopp&amp;gt; File mit 0 Byte belegt 0 Blocks, Inode wird also nicht gezählt.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; okay. Das war hilfreich die Erklaerung, danke.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Also wir haben in der Inode das Lookup Array für eine diskrete Funktion, eine Wertetabelle,
und die Speicherung des Array ist ulkig. Und wir haben deswegen overhead, weil wir die
mit 1, 2 und 3 markierten Blöcke in
&lt;a href=&#34;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&lt;/a&gt;


irgendwann belegen müssen. Und deswegen siehst du die Sprünge - kein File hat jemals 13 Blocks.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Moment, aber das ist ja sau umständlich eine ganze Datei zu lesen dann? :)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ah! Jetzt dämmert es langsam. Ist ja nicht so, dass ext2 GUT wäre.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; So, jetzt gehen wir noch mal in den Code&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Das ist
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L665&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ext2_bmap&lt;/a&gt;

, sehr kurz. Du erinnerst dich: JEDES Dateisystem hat ein bmap.
Darum ist &lt;code&gt;ext2_bmap&lt;/code&gt; sehr kurz, es ruft &lt;code&gt;generic_block_bmap&lt;/code&gt; auf. Das wiederum ruft dann allerdings
&lt;code&gt;ext2_get_block&lt;/code&gt; auf, das die Arbeit für &lt;code&gt;generic_block_bmap&lt;/code&gt; macht. &lt;code&gt;generic_block_bmap&lt;/code&gt; kriegt also
einen Callback nach &lt;code&gt;ext2_get_block&lt;/code&gt; mitgegeben. Wir landen also in
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L547&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ext2_get_block&lt;/a&gt;

. So weit so klar?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ja warte. Ich schau mir den Code gerade an.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Tut nicht not. Noch nicht. Erst mal ist nur wichtig, wie wir zu &lt;code&gt;ext2_get_block&lt;/code&gt;
kommen und wieso da ein Umweg über das &lt;code&gt;generic_block_bmap&lt;/code&gt; gemacht wird.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Nicht klar. Warte. Wo bei &lt;code&gt;ext2_get_block()&lt;/code&gt; ist das Offset?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; in &lt;code&gt;iblock&lt;/code&gt; (2. parameter), ist schon umgerechnet in eine blocknummer.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ah klar, der n. block eines inodes.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Wir sind also in
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L547&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L547&lt;/a&gt;

 und sollen &lt;code&gt;iblock&lt;/code&gt;
aus &lt;code&gt;inode&lt;/code&gt; (1. parameter) fischen. Also block 976 aus file 36. Wir müssen ja nun je nach
Blockoffset unterschiedlich kompliziert die Lookuptable runterklettern. Bei blocks 0-11 wäre
alles ganz einfach, bei 12-267 kommt der IND dazu und bei den folgenden Blöcken der DIND.
Soweit das Verfahren grundsätzlich klar?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Ich schau mir das .gif nochmal an.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Es wird leichter, wenn du
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L196&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L196&lt;/a&gt;

 liest.
&lt;code&gt;i_block&lt;/code&gt; ist also 976. Dann schau mal in die Zeile 196. Zeile 201: &lt;code&gt;direct_blocks&lt;/code&gt; ist 12. &lt;code&gt;indirect_blocks&lt;/code&gt; ist ptrs, also 256.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; &lt;code&gt;ext2_block_to_path&lt;/code&gt;. Das &lt;code&gt;path&lt;/code&gt; hat nix mit dem Dateisystempfad zu tun, sondern mit dem Pfad, wie man zum Block kommt. Ahh.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Es geht um den Path in
&lt;a href=&#34;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&lt;/a&gt;

, ja.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Warte, ich hab eine Frage zu dem GIF. Da z.b. &amp;ldquo;1&amp;rdquo;, also der erste indirekte Block. Der koennte auf 256 weitere zeigen?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja. In unserem Beispiel ist das so, 4 byte pro blocknummer und 1 kb pro block. Bei anderen
Größen (8 byte pro blocknr, und 4 kb pro block) ist das anders. 4096/8 = 512 pro IND z.B.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; hm. Sorry, ich dachte ne Blocknummer ist 32bit?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja, in unserem Beispiel ist das so. Aber es waere ja möglich, das alles mit anderen Sizes zu compilen.
Und dann soll es auch noch funktionieren. Also coden wir das alles nicht hart rein, sondern speichern die
Rahmendaten im Superblock des Filesystems und schreiben den Code ordentlich. Soweit so klar?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; jo&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Drum auch der Code in
&lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L196&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L196&lt;/a&gt;

, Zeilen 199 bis 203.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Den ich mir grad anschaue. ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Der fragt den Superblock sb nach den Anzahl der Adressen pro Block, und bestimmt dann die &lt;code&gt;direct_blocks&lt;/code&gt;,
die Anzahl der Blockadressen pro indirect Block in &lt;code&gt;indirect_block&lt;/code&gt; und die Anzahl der Blockadressen pro Double Indirect Block.&lt;/p&gt;
&lt;p&gt;Der macht das ein wenig komisch. Erst mal &lt;code&gt;direct_blocks&lt;/code&gt;. Das ist leicht, da nimmt er nur den #define.
&lt;code&gt;indirect_blocks&lt;/code&gt; ist auch leicht, das ist ptrs, also &lt;code&gt;EXT2_ADDR_PER_BLOCK(...sb)&lt;/code&gt;, also mal im Superblock nachschlagen.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[#define EXT2_ADDR_PER_BLOCK(s)          (EXT2_BLOCK_SIZE(s) / sizeof (__u32))](http://lxr.linux.no/source/include/linux/ext2_fs.h#L100)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Block size bei uns 1024 und &lt;code&gt;sizeof __u32&lt;/code&gt; ist immer 4. Also ist mein beispiel mit 8 derzeit hypothetisch.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ein double block kann ebenfalls ptrs viele Blockadressen enthalten, also 256 Stück. Jede von denen ist ein indirect block, der 256 Datenblöcke enthält.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; also 256^2 und ein TIND für 256^3.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Nun will er das aus irgendeinem Grund nicht so rechnen, sondern mit bit shifts, also macht er 1 &amp;laquo; (ptrs_bits * 2).&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Also warte, kann man 256+256^2+256^3 Blöcke adressieren, d.h. kann ne Datei so groß sein?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja, das ist auch noch ein Limit. Aber da wir nur 2^32 viele Blocknummern haben, ist bei 4 Gigablocks Schluss, also bei 4 TB (1 kb Blöcke) oder 16 TB (4 kb Blöcke).&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Okay, irgendwie so ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Du müßtest schon die Blocknummern länger machen als ein &lt;code&gt;__u32&lt;/code&gt;, damit mehr geht, dann passen aber weniger direct_blocks in eine Inode, oder die Inode wird größer.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Aber wir wollen mal weiter im Code. Schau in zeile 207. Was machen die da?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; &lt;code&gt; if (i_block &amp;lt; 0) {&lt;/code&gt;
Isotopp&amp;gt; geht das überhaupt?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Warte&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Nah, nur durch nen Programmierfehler evtl.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Welchen typ hat &lt;code&gt;i_block&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; unsigned? :) Ja, okay. Das geht überhaupt nicht.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Steht oben in der Funktion - da ist es ein LONG!&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; ah, doch ein signed :)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Aber eine Blocknummer, das haben wir vorher gesehen, ist ein &lt;code&gt;__u32&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Macht das sinn?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Nein, das ist FAHALSCH!&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Sag das doch!&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Du hast gerade deinen ersten Kernelfehler gefunden.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Geil! ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Der Fehler sieht Blocknummern als signed, daher ist also nun schon bei 2 Gigablocks zu, also 2 TB und 8 TB Filesize (für 1 und 4 kb Blocksize). Aber weiter im Text, Zeile 209.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Weiter zu 212. &lt;code&gt;((i_block -= direct_blocks) &amp;lt; indirect_blocks)&lt;/code&gt; ist dir auch klar? Wir zermatschen &lt;code&gt;i_blocks&lt;/code&gt; hier als Seiteneffekt.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; ja bin ein C-ler. Daran scheitert die Erklaerung nicht ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Also sind wir in 216. Nun ist &lt;code&gt;i_block&lt;/code&gt; also 964 und wir ziehen 256 (&lt;code&gt;indirect_blocks&lt;/code&gt;) ab. Das sind 708. Und &lt;code&gt;double_blocks&lt;/code&gt; ist 256^2. Also true. Also speichern wir in 217: lese &lt;code&gt;EXT2_DIND_BLOCK&lt;/code&gt;, dann in 218: lese &lt;code&gt;i_block/256&lt;/code&gt; (&lt;code&gt;i_block &amp;gt;&amp;gt; ptrs_bits&lt;/code&gt;), und in 219: lese &lt;code&gt;i_block % 256&lt;/code&gt; (&lt;code&gt;i_block &amp;amp; ( ptrs - 1)&lt;/code&gt;). Dann sind wir fertig.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Wir sind wieder in &lt;a href=&#34;http://lxr.linux.no/source/fs/ext2/inode.c#L547&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lxr.linux.no/source/fs/ext2/inode.c#L547&lt;/a&gt;

, line 557 nun, soweit klar?&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Kleinen Moment, das Bitshifting finde ich verwirrend.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; ja&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; ich weiss jetzt, was &lt;code&gt;ext2_block_to_path&lt;/code&gt; macht.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Also warte mal. Lass mich mal zusammenfassen.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Naja, nicht mal so einfach zu beschreiben.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Doch schon. Dir fehlen nur die Worte.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; :] jo&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Wir haben nun offset[0], offset[1] und offset[2]. In offset[0] steht welches Feld aus der Inode wir nehmen (das DIND feld),  Wir haben dann einen Block mit 256 Feldern, und nehmen das Feld offset[1] da draus, lesen den Block und nehmen das Feld offset[2] da draus.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Naja, das sind ja Details. Mich interessiert aber eher das Design als die Implementation ;), Das geht mir schon zu sehr in die Tiefe ehrlich gesagt.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Allgemeiner: wir haben ein Array, das nicht linear ist, sondern quadratisch steigend durch Indirektion komprimiert wird, und bei unseren Randparametern ist die schrittweite 8 bit (256 entries) pro Block, also 256, 256^2, 256^3, &amp;hellip; und das ist genau die Zeichnung
&lt;a href=&#34;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://kris.koehntopp.de/artikel/dateisysteme/filestructure.gif&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Okay. Jetzt sind meine Fragen geklärt oder? Dieses ganze Detailwissen erschlägt mich ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Der Rest sind tatsaechlich Kerneldetails. Das hier war die Logik. Der Punkt ist, dass du in &lt;code&gt;block_to_path&lt;/code&gt; durch die Faltlogik geklettert bist. Also die, die das mit DIR, IND, DIND und TIND analysiert und entscheidet.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Naja, Faltlogik?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ja, erst 12 direkt, dann 256 einmal gefaltet, dann 256*256 zweimal gefaltet, dann 256*256*256 dreimal gefaltet statt eines einzigen linearen Arrays das zum groessten Teil leer waere.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Hm, ich versteh zwar das System, aber nicht was das mit Falten zu tun hat ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Naja, statt eines Array mit 2 Gigaentries (Kernelbug!) hast du ein Array mit 15 Eintraegen, bei dem die ersten 12 Eintraege für sich selber stehen, der Eintrag 13 für 256 Eintraege, der Eintrag 14 für 256 Eintraege, die für 256 Eintraege stehen, steht, und der Eintrag 15 für 256 Eintraege die für 256 Eintraege, die für 256 Eintraege stehen steht. Also einmal falten, zweimal falten, dreimal falten.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Aber &amp;ldquo;falten&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; So in etwa:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;   ___
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;___\ /___
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;theclaw&amp;gt; Was stellt das dar?&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Ein Eintrag, der für viele steht. Ein Blatt Papier mit zwei Knicks, ein Eintrag (der zwischen \ /) steht für Drei (___)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Und zum Schluß
&lt;a href=&#34;http://www.tamacom.com/tour.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.tamacom.com/tour.html&lt;/a&gt;

,
&lt;a href=&#34;http://www.tamacom.com/tour/kernel/unix/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.tamacom.com/tour/kernel/unix/&lt;/a&gt;

,
&lt;a href=&#34;http://www.tamacom.com/tour/kernel/unix/S/97.html#L18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.tamacom.com/tour/kernel/unix/S/97.html#L18&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;Das, mein Freund, ist der Urvater aller bmaps, bmap in V7 Unix.&lt;/p&gt;
&lt;p&gt;Rein gehen eine &lt;code&gt;struct inode&lt;/code&gt;, die inode,  eine &lt;code&gt;daddr_t bn&lt;/code&gt;, eine blocknummer und ein &lt;code&gt;rwflag&lt;/code&gt;, das ist aber Wurst. Raus geht eine &lt;code&gt;daddr_t&lt;/code&gt; blocknummer.&lt;/p&gt;
&lt;p&gt;Also (ino, block_in_file) -&amp;gt; (phys blocknr). NADDR ist die Anzahl der Eintraege in der Inode. Also sind 0-&amp;gt; NADDR-4 die direct blocks, NADDR-3 der IND,  NADDR-2 der DIND  und NADDR-1 der TIND.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Aber den Code will ich mir jetzt nicht genauer ansehen, sorry ;)&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Das ist derselbe Code, nur noch verquaster. Der ist ja auch 30 Jahre alt.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Den Link bookmarke ich mal, das könnte noch interessant werden.&lt;/p&gt;
&lt;p&gt;Isotopp&amp;gt; Und weil es so schön ist, sind da auch die FreeBSD, NetBSD, OpenBSD und Hurd Versionen von demselben Zeug. Und da kannst du dann sehen wie fundamental das ist, was Du da gerade anfasst. Und wie sich C-Style im Kernel in den letzten 30 Jahren so entwickelt hat. Weil das V7 Zeugs da sind etwa 30 Jahre von hier, das 4.3BSD sind ca. 20 Jahre von hier und das Linux-Zeugs ist von jetzt.&lt;/p&gt;
&lt;p&gt;theclaw&amp;gt; Wenn ich das so seh fällt mir grad auf wie sinnvoll man seine Zeit nützen könnte ;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fragmentierung (für Jannik)</title>
      <link>https://blog.koehntopp.info/2006/05/08/fragmentierung-fuer-jannik.html</link>
      <pubDate>Mon, 08 May 2006 20:45:51 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2006/05/08/fragmentierung-fuer-jannik.html</guid>
      <description>&lt;p&gt;ircnet, #lug-kiel, am 7. und 8. Mai.&lt;/p&gt;
&lt;p&gt;Jannik wundert sich über fsck&amp;rsquo;s Meldung&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;/: 130834/6553600 Dateien (1.1% nicht zusammenhängend), 1007700/13107200 Blöcke
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Ist das schlimm?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
hmm.. /:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;130834/6553600 Dateien (1.1% nicht zusammenhängend), 1007700/13107200 Blöcke
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Sieht so aus als wäre mein Dateisystem fragmentiert gewesen o.O?
Ich dachte, so etwas kennt Linux nicht.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tholle:&lt;/strong&gt;
Linux oder das Dateisystem? ;-)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Seufz. &amp;ldquo;1.1% nicht zusammenhängend&amp;rdquo; Natürlich können Dateien in
nicht fortlaufenden Blocknummern gespeichert sein. Warum meinst
du, daß das ein Problem sei? Oder anders herum, warum glaubst
Du, daß es gut ist, wenn eine Datei in fortlaufenden
Blocknummern gespeichert ist?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
???&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Erklär mal was Du denkst, was die Ausgabe bedeutet, die Du da
bekommen hast?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Isotopp: Öhm. Ei, jetzt kommt hier ne Lehrstunde :D
Ja, vielleicht daß die Daten nicht zusammenhängen (in Blöcken
&amp;ldquo;nebeneinander&amp;rdquo; liegen) Also eine Datei zum Beispiel.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Ja, genau. Eine Datei ist zusammenhängend, wenn sie in
fortlaufenden Blocknummern gespeichert ist. Wenn in den
Blocknummern einer Datei lücken sind, ist sie fragmentiert.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ich dachte sowas nennt man dann fragmentiert sein, so wie man
das bei Windows kennt. Ach so, gut.
Also kann das Dateisystem defragmentiert werden irgendwie?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Wenn das z.B. ein ext2 oder ext3 ist, dann besteht das
Dateisystem aus Blockgruppen, In BSD ufs nennt man das cylinder
groups, das ist im Kern dasselbe. In einer bg befinden sich eine
Superblockkopie, die Inode-Bitmaps, die Block-Bitmap, die Inodes
der bg und dann die Datenblöcke. Jede bg ist also ein kleines
Mini-Dateisystem für sich.&lt;/p&gt;
&lt;p&gt;In ext2/ext3 kann eine Bitmap nur einen Block groß sein.&lt;/p&gt;
&lt;p&gt;Früher war ein Block 1 kb groß. das sind also 8192 Bit in einem
Block. Also konnte eine bg 8192 Blöcke groß sein.&lt;/p&gt;
&lt;p&gt;Heute hat ext2/ext3 4 kb blöcke. also sind 32768 Bit in einem
Bitmap block, und 32768 Blöcke in einer bg.&lt;/p&gt;
&lt;p&gt;Früher waren also 8192 1 kb blocks = 8 MB eine bg. Heute sind
32768 4 kb blocks = 128 MB eine bg.&lt;/p&gt;
&lt;p&gt;Was passiert, wenn du eine Datei mit 200 MB speichern willst?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Isotopp: dann wird eine bg belegt und eine 2. nur zum Teil.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Ja, genau. Ist die Datei dann fragmentiert?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Nicht wirklich.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Naja, sind die Blocknummern der Datei zusammenhängend oder nicht?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ähm…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Sie können nicht, denn irgendwo fängt ja die neue bg an.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Genau.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Also liegt irgendwo Mitten in der Datei der Superblock und die
Bitmaps und die Inodes der neuen bg. Die Datei ist also
fragmentiert. In ext2/ext3 ist es nicht möglich eine Datei zu
speichern ohne sie zu fragmentieren, wenn sie größer als eine bg
ist.&lt;/p&gt;
&lt;p&gt;Wenn eine 200 MB Datei eine bg komplett voll macht, dann sind ja
nur die Datenblöcke voll, die Inodes in der bg sind noch leer.
(bis auf eine, die von der 200 MB Datei) Jetzt wird eine zweite
Inode in der bg belegt, in der die Datenblöcke voll sind. Wo
werden die Datenblöcke der Datei abgelegt?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ähm, woanders?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Ja, weit weg von der Inode. Das ist aber doof, denn um die Datei zu lesen,
muß man die Inode lesen und dann die Daten finden und lesen.
Das dauert.&lt;/p&gt;
&lt;p&gt;Die 200 MB Datei ist doch aber sowieso fragmentiert. wenn wir sie
schneller (früher) fragmentiert hätten, dann wäre noch Platz
frei in der 1. bg frei und die 2. Datei könnte ihre Daten näher
an der Inode speichern. Aber wieso glauben wir, daß
Fragmentierung was schlechtes ist? Wieso denken wir, daß
zusammenhängende Blocknummern an einer Datei gut sind?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Das glaube ich mittlerweile gar nicht mal mehr.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Jannik: Du hast eine Festplatte. Wie lange dauert es denn bei
Deiner Platte, den Kopf von einer Inode zu den Datenblöcken in
einer anderen bg zu bewegen? Ungefähr?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Kein Plan. Nicht lang. Vermutlich keine Millisekunde.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Kennt jemand die Zeit von seiner Festplatte hier?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Isotopp hat eine Platte mit einer Seek Time von 8ms in seinem Laptop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Jannik: Festplatten haben Seek times zwischen 5 und 10ms.
Das heißt, die können 100-200 Seeks pro Sekunde machen&lt;/p&gt;
&lt;p&gt;Damit haben wir einen Grund, warum ein Interesse besteht, Daten
auf einer Platte möglichst dicht beeinander zu speichern (Seeks
sind teuer), aber wenn wir sie &lt;em&gt;zu&lt;/em&gt; dicht beieinander speichern,
dann kommen neue und alte Daten einander in die Quere.
Daher wirst du auf jedem Dateisystem mit grossen Dateien immer
fragmentierte Dateien finden.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Isotopp: Gut. aber Dateien werden trotzdem wie bei Windows
fragmentiert? Sodaß man Defragmentieren muss? Wie ist das, wenn
der Datenträger voll ist? Dann ist das System vermutlich
gezwungen zu fragmentieren?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Ja, natürlich. Wenn die Platte fast voll ist, muss das
Dateisystem nehmen was da ist. ufs sagt das an.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;changing optimization from time to space&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;oder so ähnlich steht dann im syslog.
In ext2/ext3 sieht man das nicht, aber kein Dateisystem sollte mehr als 80% gefuellt werden, sonst wird es mit der
Allokation schwierig.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Gut.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wir wissen nun: Dateien sollten dicht beeinander stehen, damit Seeks vermieden werden. aber wenn wir das tun, etwa eine bg voll machen,
dann bauen wir uns die bg fuer alle nachfolgenden Dateien zu, und bekommen die Seeks dann da.&lt;/p&gt;
&lt;p&gt;ufs vermeidetet das etwa, indem es &amp;ldquo;long Seeks&amp;rdquo; erzwingt. Es fragmentiert Dateien &lt;em&gt;absichtlich&lt;/em&gt; etwa alle 1mb oder so, indem es in eine andere bg wechselt.&lt;/p&gt;
&lt;p&gt;Wenn du also eine Datei in ufs speicherst, wird das erste MB Daten in dieselbe bg geschrieben, in der die Inode der Datei steht, und dann wird ein long Seek erzwungen und in einer anderen bg weiter geschrieben. Dadurch bleiben Datenblöcke in der bg frei, sodaß die nächste Datei, die angelegt wird
Datenblöcke frei vorfindet in derselben bg wie ihre Inode.&lt;/p&gt;
&lt;p&gt;In diesem fall ist es messbar vorteilhaft, die Datei zu fragmentieren als die unfragmentiert zu lassen. Cool was?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Noch am lesen. Weiß nicht ob ich das genau verstanden habe. Nochmal lesen ;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Vielleicht so: eine bg ist ein kleines Dateisystemchen mit Inodes und Datenblöcken. Wir wollen gerne, dass die Datenblöcke einer Datei dicht an der Inode stehen, sonst Seek und Seek ist langsam bei Platten.&lt;/p&gt;
&lt;p&gt;Wenn wir eine grosse Datei abspeichern, belegen wir eine Inode in einer bg und dann alle Datenblöcke in dieser bg, weil es ja eine grosse Datei ist.&lt;/p&gt;
&lt;p&gt;Wenn wir eine zweite Datei in derselben bg speichern wollen, finden
wir noch eine freie Inode (war ja erst eine belegt) und dann aber
keine Datenblöcke mehr,  denn die lange erste Datei hat die ja alle belegt.&lt;/p&gt;
&lt;p&gt;ßWir müssen also die Datenblöcke von Datei 2 weit entfernt von der Inode von Datei 2 abspeichern,
da die Datenblöcke in der Datei wo die Inode steht belegt sind.
Das ist doof.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Klar das hab ich noch verstanden. &lt;em&gt;Müssen&lt;/em&gt; wir?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wenn du die Inodes in dieser Blockgroup nicht ganz aufgeben willst - ja.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ach so, ja klar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Über die Regeln, wie man Inodes auswählt, reden wir noch.&lt;/p&gt;
&lt;p&gt;Der Punkt ist, dass wir Inode 2 beschreiben wollen in derselben bg wie Inode 1, aber keine Datenblöcke in dieser bg mehr frei sind.
Also können wir nur die Inode nehmen, müssen die Datenblöcke fuer Inode 2 aber weit weg in einer anderen bg abspeichern.&lt;/p&gt;
&lt;p&gt;ufs versucht das zu vermeiden,
genauer versucht ufs zu vermeiden, dass eine einzige grosse Datei alle Datenblöcke in einer bg belegt.&lt;/p&gt;
&lt;p&gt;ufs geht also so vor:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wenn Datei 1 geschrieben wird,
wird eine Inode in der bg belegt und die daten zu dieser Inode 1 erst mal in dieselbe bg wie die Inode geschrieben&lt;/li&gt;
&lt;li&gt;nach einem MB oder so (konfigurierbar) macht ufs dann aber einen long Seek, erzwingt einen Wechsel der bg.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Das erste MB der langen Datei steht also in der ersten bg, der Rest dann in einer anderen, nach einem weiteren MB dann in einer noch anderen und so weiter.
ufs verteilt die grosse Datei also über die Datenblöcke in allen Blockgroups, in 1 MB grossen fragmenten.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Dadurch wird sie fragmentiert, aber ist das nicht schlecht für sie? Wird das nicht langsamer?
Effektiver wäre es vielleicht, wenn man das über mehrere Festplatten machen würde, mit LVM und so vielleicht?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Ein wenig. &lt;em&gt;Aber:&lt;/em&gt;
in der ersten bg sind nun noch Datenblöcke frei, und das erste MB von Datei 2 kann nun ebenfalls dicht bei der Inode von Datei 2 stehen.
Und ja, Raid 0, Raid 10 oder andere Dinge können hier helfen. Mehr Spindeln = mehr Möglichkeiten pro Sekunde, einen Seek zu machen.
Aber wenn du die Anzahl der spindeln als konstant (1, oder meinetwegen 6) ansetzt,
brauchst du trotzdem eine Optimierungsstrategie.
Man kann ja nicht jedes Problem mit beliebig viel Geld bewerfen.
Aber Dein Denkansatz ist vollkommen korrekt - mehr spindeln = mehr Seeks = weniger probleme.
Der Punkt ist - mit long Seeks stehen die Dateianfänge aller Dateien dicht bei den Inodes dieser Dateien.
Stell dir etwa mal ein &lt;code&gt;file *&lt;/code&gt; auf ein Verzeichnis vor.
Oder &lt;code&gt;konqueror file:/home/jannik&lt;/code&gt; - da müssen ja Previews gerendert werden, Dateitypen bestimmt und so weiter.
Da ist es schon ausgesprochen nützlich, wenn man die Dateianfaenge (die ersten MB oder so) dicht an den Inodes hat.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Steht das Wichtigste über eine Datei nicht im Anfang? Dateityp, Berechtigungen etc.?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wenn ein Dateiformat schlau überlegt ist, dann steht es am Anfang.
Aber metadaten wie Berechtigungen, Zeitstempel und alles andere stehen in der Inode.
Schau mal in &lt;code&gt;/usr/src/linux/include/linux/ext2_fs.h&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Wenn man mehrere Festplatten hätte, dann wäre es doch ziemlich cool die Dateien darüber zu streuen…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Und suche da nach &lt;code&gt;struct ext2_Inode&lt;/code&gt;.
Das ist eine Inode auf der Platte und was da drin steht.
Der &lt;code&gt;struct ext2_Inode&lt;/code&gt; ist 128 Byte gross, wenn man nachzählt.
In einen 1024 Byte Block passen also 8 davon.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Den Pfad gibts bei mir nicht.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Dann hast du keine Kernelsources oder Kernelincludes installiert.
Guckst Du bei &lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2%5c_fs.h#L211&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lxr&lt;/a&gt;


Das ist ein struct, eine Datenstruktur in C.
Das ist eine ext2/ext3 Inode auf der Platte.
Die daten da drin kannst du auslesen. Hast du eine ext2 oder ext3 Partition irgendwo wo du root bist?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ja. Ähm, lvm is ja egal oder?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Es geht nur um das Dateisystem, nicht um die Art der Partition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Gut. Ja, dann hab ich.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Hast du ein &lt;code&gt;/boot&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
&lt;code&gt;/boot&lt;/code&gt; ist ext2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wie heisst die Partition bei Dir? Bei mir ist &lt;code&gt;/boot&lt;/code&gt; auf &lt;code&gt;/dev/sda5&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;#&lt;/span&gt; mount &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;grep boot
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;dev/hdb4 on /boot type ext2 (rw)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:~ # debugfs /dev/sda5
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;debugfs 1.38 (30-Jun-2005)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;debugfs:  ls
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;   2  (12) .        2  (12) ..      11  (20) lost+found    40  (16) message
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;6025  (24) grub    29  (12) boot    31  (16) vmlinuz       13  (76) initrd
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;debugfs arbeitet ja auf Dateisystemen, deswegen siehst du nur Sachen in /dev/hdb4 und nicht Zeugs da drüber
Du kannst nun mal &lt;code&gt;stat&lt;/code&gt; auf einen Kernel machen.
In meinem Fall&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;debugfs:  stat vmlinux-2.6.13-15-default.gz
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Inode: 27   Type: regular    Mode:  0644   Flags: 0x0   Generation: 1675033442
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;User:     0   Group:     0   Size: 1838899
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;File ACL: 0    Directory ACL: 0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Links: 1   Blockcount: 3608
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;Fragment:  Address: 0    Number: 0    Size: 0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;ctime: 0x43f86651 -- Sun Feb 19 13:36:33 2006
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;atime: 0x4419638b -- Thu Mar 16 14:09:31 2006
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;mtime: 0x4327102c -- Tue Sep 13 19:45:16 2005
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;BLOCKS:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;0-11):9711-9722, (IND):9723, (12-267):9724-9979, (DIND):9980, (IND):9981, (268-
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;523):9982-10237, (IND):10238, (524-779):10239-10494, (IND):10495, (780-1035):104
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;96-10751, (IND):10752, (1036-1291):10753-11008, (IND):11009, (1292-1547):11010-1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;1265, (IND):11266, (1548-1795):11267-11514
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;TOTAL: 1804
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Vergleiche das mit &lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2%5c_fs.h#L211&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LXR&lt;/a&gt;


vmlinux-&amp;hellip; ist Inodenummer 27, ein Regular File.&lt;/p&gt;
&lt;p&gt;In der Inode stehen&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;der Mode (hier: 0644),&lt;/li&gt;
&lt;li&gt;die Flags (keine),&lt;/li&gt;
&lt;li&gt;die ACL Nummern (keine)&lt;/li&gt;
&lt;li&gt;der Owner (root, 0) und&lt;/li&gt;
&lt;li&gt;die Group (0, root)&lt;/li&gt;
&lt;li&gt;die Datei hat einen Namen (Linkcount 1), und&lt;/li&gt;
&lt;li&gt;3608 blocks&lt;/li&gt;
&lt;li&gt;die hat eine ctime, atime, mtime&lt;/li&gt;
&lt;li&gt;und dann kommen die Blocknummern der Datenblöcke und der Datenblockzeiger (INDirect blocks)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Und im struct siehst du genau das: Platzhalter für den Mode (16 bit),
eine uid, die Größe der Datei in Byte, die atime, ctime, mtime und dtime (nur gesetzt, wenn die Datei deleted wird)
die Gruppennummer, der Linkcounter
die Anzahl der Blöcke
und das Flags Feld.
und das Wichtigste: &lt;code&gt;struct: __le32 i_block[EXT2_N_BLOCKS];&lt;/code&gt;
die Blockadressen der Datenblöcke.&lt;/p&gt;
&lt;p&gt;In der Inode steht also alles über die Datei - ihre Eigenschaften und die Zeiger auf die eigentlichen daten, nur eine Sache fehlt.&lt;/p&gt;
&lt;p&gt;Weißt du, welche Sache das ist?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Die Daten.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Richtig, die stehen in den Datenblöcken.
Ich meinte aber was anderes, eigentlich.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Oder &lt;em&gt;was&lt;/em&gt; es für eine Datei es ist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Das steht in den oberen 4 Bit von &lt;code&gt;i_mode&lt;/code&gt;.
Der Filemode ist ja &lt;code&gt;sstrwxrwxrwx&lt;/code&gt; - das sind nur 12 Bit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Steht da ob mp3 oder .wav? Das meinte ich.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Das ist ja eine Extension, also Teil des Dateinamens.
Aber jetzt bist du auf einer Spur. Wo steht der Dateiname?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
In der Datei. Also in den Datenblöcken.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Nein, da stehen nur die Daten.
Jannik, mach mal &lt;code&gt;cd $HOME; touch lall; ln lall laber&lt;/code&gt;
und dann &lt;code&gt;ls -li lall laber&lt;/code&gt;
und paste den output.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;#&lt;/span&gt; ls -li lall laber
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;870099 -rw-r--r-- 2 root root 0 2006-05-08 21:43 laber
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;870099 -rw-r--r-- 2 root root 0 2006-05-08 21:43 lall
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wie lang ist die Datei?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Genau das hat uellue mir letztens erklärt
mit den Hardlinks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
wie lang ist die Datei?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
870099?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Nein, das ist die Inodenummer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ah, so. Ähm…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
ja&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
In der Datei steht der Name nicht,
denn die Datei ist ja 0 byte lang.
Sie &lt;em&gt;hat&lt;/em&gt; gar keine Datenblöcke.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Suchte grad nach ner Zahl &lt;em&gt;g&lt;/em&gt; Ach so, ja genau, &lt;code&gt;touch&lt;/code&gt; erstellt nur eine Inode?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
ja&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Und jetzt hat man 2 Inodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
In der Inode kann der Name auch nicht stehen. Denn beide Dateien haben ja dieselbe Inodenummer, aber verschiedene Namen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Die auf das Gleiche verweisen würden?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
So ist es. Hast du 2 Inodes? Nein. Die Zahlen sind gleich am Anfang.&lt;/p&gt;
&lt;p&gt;Wir lernen: der Name einer Datei steht &lt;em&gt;nicht&lt;/em&gt; in den Daten (0 bytes, keine Datenblöcke),
und auch nicht in der Inode (2 Namen, eine Inode, wie wir beweisen können).
Was bleibt?&lt;/p&gt;
&lt;p&gt;Der Name einer Datei steht in einem Verzeichnis. Verzeichnisse sind auch Dateien und in denen speichert Linux paare von (Name, Inodenummer) ab.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2%5c_fs.h#L501&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LXR&lt;/a&gt;

 zeigt wie.&lt;/p&gt;
&lt;p&gt;Ein &lt;code&gt;ext2_dir_entry&lt;/code&gt; ist eine Inodenummer (32 bit) und dann ein Record von &lt;code&gt;rec_len&lt;/code&gt; Bytes Länge, in dem &lt;code&gt;name_len&lt;/code&gt; viele Bytes verwendet werden.
und dann natürlich der name, &lt;code&gt;name_len&lt;/code&gt; bytes lang in ASCII.
Mit &lt;code&gt;touch lall; ln lall laber&lt;/code&gt;
machst du also einen eintrag (870099, lall) und einen Eintrag (870099, laber) in &lt;code&gt;.&lt;/code&gt;
(dem aktuellen Verzeichnis)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ja.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Hey, welche Inode hat /boot? &lt;code&gt;ls -dlsi /boot&lt;/code&gt; sagt es dir?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;#&lt;/span&gt; ls -dlsi /boot
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2 2 drwxr-xr-x 4 root root 2048 2006-05-02 00:37 /boot
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Und welche Inode hat &lt;code&gt;/&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;#&lt;/span&gt; ls -dlsi /
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;2 4 drwxr-xr-x 22 root root 4096 2006-04-26 23:31 /
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Die erste Zahl ist die Inodenummer, die 2. Zahl ist die Länge in Blöcken.
Beide Dateien &lt;code&gt;/&lt;/code&gt; und &lt;code&gt;/boot&lt;/code&gt; haben als die Inode 2. Wie kann das sein?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ähm. Beide sind in &lt;code&gt;/&lt;/code&gt; eingehängt?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Naja, &lt;code&gt;/&lt;/code&gt; ist nirgendwo eingehaengt, &lt;code&gt;/&lt;/code&gt; ist &lt;code&gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ja, ok. Deshalb? Was hätte dann die erste Inode?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Du bist auf der richtigen Spur. Die root-Inode in ext2 hat immer die Nummer 2.
Und da &lt;code&gt;/&lt;/code&gt; und &lt;code&gt;/boot&lt;/code&gt; jeweils verschiedene Dateisysteme sind,
haben beide root-Inodes,
also haben beide die Inode 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ach ja&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wie kann Linux die beiden auseinander halten?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Durch die Verzeichnisstruktur?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Nun eines ist auf &lt;code&gt;/dev/hdb4&lt;/code&gt;, das andere auf &lt;code&gt;/dev/hdbsonstwas&lt;/code&gt;.
Und wenn du &lt;code&gt;ls -l /dev/hdb*&lt;/code&gt; machst
siehst du, daß jedes Device da intern andere Gerätenummern hat.
Die Inodenummer in Unix ist also nicht eindeutig, aber auf jedem System ist die Kombination &lt;code&gt;(major number, minor number, Inode number)&lt;/code&gt;
zu jedem Zeitpunkt eindeutig.
(maj, min) bezeichnen die Partition, und in einer Partition ist (ino) eindeutig
Schau mal hier -&amp;gt; &lt;a href=&#34;http://lxr.linux.no/source/include/linux/ext2%5c_fs.h#L56&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LXR&lt;/a&gt;

.
Das mit der Inode 2 denke ich mir ja nicht aus,
das muss ja irgendwo im source stehen
und da steht es.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
cool&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#define EXT2_BAD_INO             1      &lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;/* Bad blocks Inode */&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Was soll das dann heißen?
Das wird einfach ausgelassen?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Inode 1 ist eine Datei, die in der Regel keinen Namen hat und die nur aus kaputten Blöcken besteht.&lt;/p&gt;
&lt;p&gt;Wenn du eine alte Platte hast, noch ohne Bad Blocks Management,
dann kannst du ein Dateisystem mit &lt;code&gt;mke2fs -c /dev/...&lt;/code&gt; anlegen.
Das dauert sehr lange,
denn dabei wird jeder Block gelesen.
Zwangsläufig sind auf jeder HDD immer einige Blöcke im Eimer, -c findet
die und fügt sie dann in die Datei Inode 1 ein.
Dadurch sind sie belegt und können nicht Bestandteil einer anderen Datei werden.&lt;/p&gt;
&lt;p&gt;Bei Disketten mit ext2 hat man das sehr, sehr oft gebraucht
(Ich nehme an, Du hast noch mit Disketten gearbeitet, irgendwann mal).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Mach ich immer noch , aber noch nicht mit Linux. Habe es noch nicht gebraucht.
Doch, ich habe mal meinen grub auf einer Diskette gehabt, in einem externen diskettenlaufwerk :D
mit usb, hat er geschluckt. Fand ich cool.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Die erste Datei, die auf einem ext2 angelegt wird, noch beim Erstellen, ist &lt;code&gt;lost+found&lt;/code&gt;.
Die hat dann immer Inode 11, denn &lt;code&gt;EXT2_GOOD_OLD_FIRST_INO&lt;/code&gt; ist 11.
Wenn du also mal ein &lt;code&gt;ls -li / /boot&lt;/code&gt; machst, wirst du sehen, dass&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lost+found 11 ist&lt;/li&gt;
&lt;li&gt;alle Inodenummern ausser &lt;code&gt;.&lt;/code&gt; und &lt;code&gt;..&lt;/code&gt; größer als 11 sind.
(vorausgesetzt, alle betrachteten Dateisysteme sind ext2/ext3, denn bei reiser ist alles anders.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;       2 drwxr-xr-x   4 root root  2048 2006-05-02 00:37 boot
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Hat wieder kleinere Inode ;P&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wieso ist das so?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ja, weil das wieder ein eigenes Dateisystem ist ;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Genau!
Wir wissen nun:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verzeichnisse sind Dateien.&lt;/li&gt;
&lt;li&gt;Verzeichnisse sind Listen von Namen und Inodenummern&lt;/li&gt;
&lt;li&gt;die /-Inode jedes ext2-Dateisystems ist 2&lt;/li&gt;
&lt;li&gt;es kann mehr als eine Inode 2 pro system geben, und an der (maj, min) des Devices auf dem sie liegt können wir sie unterscheiden.&lt;/li&gt;
&lt;li&gt;in der Inode stehen alle Informationen über eine Datei, insbesondere der Mode, die Times, und die Datenblockzeiger&lt;/li&gt;
&lt;li&gt;in der Inode steht NICHT der name der Datei, der steht im Verzeichnis&lt;/li&gt;
&lt;li&gt;in den Datenblöcken stehen nur die Daten&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Auch wenn ich da jetzt noch nicht sehe, wie er das unterscheidet von &lt;code&gt;/&lt;/code&gt; &amp;hellip;
Wie kann ich &amp;ldquo;(maj, min) des Devices auf dem sie liegt&amp;rdquo; herrausfinden?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
jannik: mit &lt;code&gt;mount&lt;/code&gt;
oder genauer, mit &lt;code&gt;cat /proc/mounts&lt;/code&gt;, denn da holt &lt;code&gt;mount&lt;/code&gt; das her, und diese Liste ordnet Devices und Namens-Stummel einander zu.
Linux weiß dann: wegen &lt;code&gt;/dev/sda5 /boot ext2 rw 0 0&lt;/code&gt;
muß ich ab &lt;code&gt;/boot&lt;/code&gt; neu zu zählen anfangen, und zwar&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/proc # ls -l /dev/sda5
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;brw-r-----  1 root disk 8, 5 May  6 22:44 /dev/sda5
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;auf (8, 5) statt auf (8, 1), bzw in meinem Fall statt auf (8, 5) auf (253, 1)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/proc # ls -lL /dev/system/root
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;brw-r-----  1 root disk 253, 1 May  6 22:44 /dev/system/root
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wir wollten über Dateisysteme und Layoutstrategien reden.&lt;/p&gt;
&lt;p&gt;Wir wollten ja, dass die Platte möglichst wenig Seeken muss.
Und wir hatten gelernt: es ist gut, Fragmentierung zu vermeiden und &amp;ldquo;dicht&amp;rdquo; zu packen, aber nicht zu dicht,
sonst nehmen wir Platz fuer die folgenden Dateien weg.&lt;/p&gt;
&lt;p&gt;Daher machen wir nach &lt;em&gt;großen&lt;/em&gt; Fragmenten von ca. 1 MB oder so eine absichtliche Fragmentierung um Platz fuer neue Dateien zu lassen.
Platten können Seeks machen, ab und zu. Kleine fragmente sind es, die uns langsam machen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ja.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wenn ich nun ein Verzeichnis anlege in ext2, und darin Dateien anlege, dann wird ext2 die Dateien alle in dieselbe bg tun wie das Verzeichnis.
Ich kann das an den Inode-nummern sehen.
Die Inode-nummern aller Dateien (nicht-Verzeichnisse) in &lt;code&gt;/&lt;/code&gt; sollten in etwa gleich groß sein.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ja&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Aber wenn ext2 ein Verzeichnis anlegt, dann tut es das neue Verzeichnis in einer &lt;em&gt;andere&lt;/em&gt; bg als das Elternverzeichnis.
Die Inode-nummern aller Verzeichnisse in / sollten sich von der Inode-nummer von / sehr unterscheiden.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot # ls -li
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;     2 drwxr-xr-x   5 root root    2048 Mar 26 11:00 .
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;    28 -rw-r--r--   1 root root 1541719 Sep 13  2005 vmlinuz-2.6.13-15-default
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;    40 -rw-r--r--   1 root root  133120 Nov  2  2005 message
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;go&#34;&gt;  6025 drwxr-xr-x   2 root root    1024 Feb 11 21:20 grub
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; 10041 drwxr-xr-x   2 root root    1024 Dec 13 09:48 mysql
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;und in grub dann:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot/grub # ls -li
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;total 285
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; 6025 drwxr-xr-x  2 root root   1024 Feb 11 21:20 .
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; 6040 -rw-r--r--  1 root root     10 May  2 14:01 default
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; 6039 -rw-------  1 root root     15 Nov  2  2005 Device.map
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; 6026 -rw-r--r--  1 root root   7540 Dec 17 06:52 e2fs_stage1_5
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Da liegt dann alles beieinander, und in einer anderen bg als /.
ext2/ext3 sortiert also Dateien &lt;em&gt;in&lt;/em&gt; einem Verzeichnis &lt;em&gt;dicht&lt;/em&gt; beieinander,
und Verzeichnisse &lt;em&gt;weit&lt;/em&gt; auseinander.
Dadurch wird die ganze Platte gleichmäßig genutzt,
aber auch das ist eine Art absichtlicher Fragmentierung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ja.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Es werden künstlich kontrolliert Seeks eingefuegt,
um die Struktur des Systems zu erhalten
und unkontrolliert entstehende kleine Fragmente zu vermeiden.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Gut.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Die abstrakte Logik: irgendwann müssen wir sowieso fragmentieren.
Das ist auch nicht schlimm, solange die Stücke einigermassen groß sind.
Also fragmentiert ext2/ext3 und auch BSD ufs absichtlich und macht
große Stücke, die nicht schaden.
Dadurch entsteht eine locker gepackte Struktur, die die ganze Platte ausnutzt,
statt sich auf einer Ecke der Platte zu ballen und sich selbst im Weg zu stehen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Hmm&amp;hellip; Ich lerne daraus, dass ich meine Platten nie überfülle (Dateisysteme).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Genau.
Und jetzt: wir erinnern uns was schlimm ist - Seeks.
Nimm an, du hast ein gut organisiertes ext2 oder auch vfat system, frisch defragmentiert.
Du loggst dich da ein und lädst dein kde,
und uellue loggt sich ebenfalls da ein und startet sein gnome,
und ich logge mich da ein und starte mein mysql.
Alle gleichzeitig.
Was wird passieren?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Die Festplatte wird springen müssen (Der Kopf)
wenn sie alles gleichzeitig machen soll&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Was nutzt uns unsere Defragmentierung nun?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Nix. In dem Fall ändert das nix.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Unfragmentierte Dateien sind Dateien, die auf der Platte mit
aufeinanderfolgenden Blocknummern stehen
aber das nutzt nix, wenn die Lesezugriffe nicht linear erfolgen.&lt;/p&gt;
&lt;p&gt;Fragmentierung oder Defragmentierung beschäftigen sich mit einem
&lt;em&gt;statischen&lt;/em&gt; Layout von Daten auf einer Platte,
aber entscheidend in einem System ist die &lt;em&gt;dynamische&lt;/em&gt; Sequenz von
Lesezugriffen über die Zeit.&lt;/p&gt;
&lt;p&gt;In einem Singleuser/Singleprocess-System wie MS-DOS
gibt es nur einen user und nur ein programm zur zeit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Willst du darauf hinaus, dass Defragmentierung auf ext2 z.b. nicht nötig ist?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Das liest dann seine Daten durch,
wodurch die &lt;em&gt;dynamische&lt;/em&gt; Lesefolge immer der &lt;em&gt;statischen&lt;/em&gt; Anordnung
von Daten auf der Platte entspricht,
da die Zugriffe niemals unterbrochen werden können.&lt;/p&gt;
&lt;p&gt;In MS-DOS bringt es also was, zu defragmentieren.&lt;/p&gt;
&lt;p&gt;MS-DOS hat auch keinen file-system cache, d.h. jeder &lt;code&gt;read()&lt;/code&gt;
endet unweigerlich &lt;em&gt;immer&lt;/em&gt; auf der Platte.&lt;/p&gt;
&lt;p&gt;In Linux ist die Situation weitaus komplizierter:
wir haben mehrere User, jeder hat mehrere Programme, die lesen können
und wir haben einen haufen RAM, den wir als Cache verwenden,
sodaß viele Reads gar nicht mehr auf die Platte gehen.&lt;/p&gt;
&lt;p&gt;Für Seeks (die sind ja das, was langsam ist) ist aber die &lt;em&gt;dynamische&lt;/em&gt;
Folge von Seeks, die die Platte wirklich sieht, relevant.&lt;/p&gt;
&lt;p&gt;Und die entspricht bei Linux nun genau nicht mehr dem statischen
Bild auf der Platte.&lt;/p&gt;
&lt;p&gt;Wenn die Daten auf der Platte defragmentiert sind, und wenn die
Lesezugriffe eines Programmes nicht durch andere Reader unterbrochen
werden und wenn der cache leer ist, &lt;em&gt;dann&lt;/em&gt; ist Defragmentierung relevant, auch bei Linux.&lt;/p&gt;
&lt;p&gt;Also&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;beim Booten&lt;/li&gt;
&lt;li&gt;beim Login nach dem Booten&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Das kann sehr viel schneller sein auf einem ordentlich defraggten System,
denn da ist der Cache leer und Programme greifen nacheinander zu.&lt;/p&gt;
&lt;p&gt;Danach - naja, ich habe&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;linux:/boot/grub # free -m
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;              total       used       free     shared    buffers     cached
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; Mem:          1011        909        101          0         82        280
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; -/+ buffers/cache:        547        464
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; Swap:         2055          3       2051
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ein Gig RAM, und davon sind 547 MB durch Programme belegt und 82+280 MB in Filesystem Caches.
Die meisten meiner Reads gehen der Platte am Arsch vorbei, denn sie kommen aus dem Cache.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gp&#34;&gt;#&lt;/span&gt; free -m
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt;              total       used       free     shared    buffers     cached
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; Mem:          1517       1484         33          0         40       1052
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; -/+ buffers/cache:        391       1126
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;go&#34;&gt; Swap:         1011        313        698
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;LOL.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Du hast 1.1 Gig im Cache.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
1 GB cached?? Wow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Wie auch immer: Fragmentierung von Linux-Dateisystemen spielt eine eher untergeordnete Rolle.
ext2 und ext3 fragmentieren &lt;em&gt;absichtlich&lt;/em&gt; und auf eine kontrollierte Weise, damit
große, schlau angeordnete Fragmente entstehen,
und auch ohne Fragmente können Seeks auf der Platte durch konkurrente Zugriffe entstehen.&lt;/p&gt;
&lt;p&gt;Wichtiger als zu Defragmentieren ist es, einen grossen Filesystem Cache zu haben.
Soweit klar?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
ja&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
1.1% fragmentation bei einem fsck sind also eine gute Sache.&lt;/p&gt;
&lt;p&gt;0.0% wären doof.
Das heisst ja, dass wir einen nicht aufgelockerten Klumpatsch von
Dateien haben (oder nur sehr viele, sehr kleine Dateien).&lt;/p&gt;
&lt;p&gt;20% fragmentierung sind auch doof.
Das kann zum beispiel passieren, wenn wir sehr viele, sehr kleine
Dateien haben und dauernd Dateien löschen und anlegen.
Etwa in einem Squid Cache oder in einem USENET News mit tradspool.&lt;/p&gt;
&lt;p&gt;Entscheidend ist aber - und die information gibt uns fsck leider nicht - daß
die stücke alle so mittelgross sind,
etwa 1/100 bis 1/10 einer Blockgroup gross.&lt;/p&gt;
&lt;p&gt;Kleiner ist von Übel, weil kleine Stücke viele Seeks bedeuten,
und größer ist von Übel, weil große Stücke die bgs zustopfen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ich schätze, ich will heute nicht mehr wissen, was reiserfs so toll macht?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
Heute nacht nicht, das schaffen wir nicht in 20 Minuten.
Aber wenn du willst kannst du dir als Hausaufgabe mal
ein ext2 directory aufmalen,
mit &lt;code&gt;(Inode, nam_len, rec_len, Dateiname)&lt;/code&gt;
und dann überlegen, wie du die Datei mit dem namen &lt;code&gt;toller name eigentlich&lt;/code&gt;
in einem  Verzeichnis mit 1 Million Einträgen findest, löscht und
dann die Datei mit dem namen &lt;code&gt;blurb&lt;/code&gt; da drin anlegst.
Und wieso das so lange dauert mit einer Million Einträgen.
Dann können wir bald mal über reiser reden und wieso das da nicht so lange dauert.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Ok. :D Erst mal die Hausaufgabe verstehen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Isotopp: das dauert so lange weil man erst alles abklappern muss, bis man auf den Namen trifft.
Muss ja alles überprüfen,
ist jede Menge Arbeit . Gute Nacht, Isotopp, und Danke :D&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
jannik: genau. Das ist effek0tiv eine &amp;ldquo;lineare Liste&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Wir schreiben Montag, den 8. Mai 2006, sie hörten den &amp;ldquo;Dienstag&amp;rdquo; zum Thema &amp;ldquo;Dateisysteme und Fragmentierung&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
und so eine Liste skaliert sich halt mit der Anzahl der Einträge im Verzeichnis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
immerhin ist die nicht fragmentiert *g*&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
oh, die kann fragmentieren.
Wenn ich &lt;code&gt;langer name eigentlich&lt;/code&gt; lösche und dann &lt;code&gt;kurz&lt;/code&gt; erzeuge, bleibt ein Stück frei.
Das ist entweder verloren oder muss gelegentlich wieder belegt werden.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jannik:&lt;/strong&gt;
Wer sorgt für die Ordnung dafür? Wie räumt man denn schön auf auf seinem Dateisystem?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Isotopp:&lt;/strong&gt;
und das alles werden wir ein andermal besprechen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Webanwendungen und der FHS</title>
      <link>https://blog.koehntopp.info/2005/06/13/webanwendungen-und-der-fhs.html</link>
      <pubDate>Mon, 13 Jun 2005 10:47:50 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2005/06/13/webanwendungen-und-der-fhs.html</guid>
      <description>&lt;p&gt;Auf der S9Y Mailingliste fragte ein zukünftiger Paketmaintainer nach, ob er Serendipity für seine Distribution packen solle und wir ihn dabei unterstützten wollen. Abgesehen von allgemeinen Überlegungen die dagegen sprechen, gibt es noch andere Gründe, die das nicht wünschenswert machen.&lt;/p&gt;
&lt;p&gt;In den Bereich der allgemeinen Überlegungen fallen zum Beispiel die Releasezyklen von Distributionen: Sie sind in der Regel sehr viel länger als die von Webanwendungen wie Serendipity. Insbesondere sehr langwellige Distributionen wie Debian verteilen mit ihren Paketen Versionen der Software, die die Entwickler von S9Y nicht mehr unterstützen können und wollen.&lt;/p&gt;
&lt;p&gt;Auch ist fraglich, welchen Gewinn ein solches Package bringen soll. Eine Anwendung wie S9Y installiert sich mit Download-Auspacken-Anklicken sowieso schon selber und ist dabei dann an keinerlei externes Packaging oder fremde Zyklen gebunden. Eine Installation mit Betriebssystem-Packages bringt da nur Nachteile und fehlende Flexibilität. Zum Beispiel ist es so nicht leicht möglich, mehr als eine Version von S9Y an mehr als einer Location im System zu installieren, denn das Packagemanagement der üblichen Distributionen unterstützt weder konkurrente Installation unterschiedlicher Paketversionen noch sind verschiebliche Pakete mit durch den Anwender bestimmter Package-Root allgemein üblich.&lt;/p&gt;
&lt;p&gt;Aber das ist nur die Spitze des Eisberges. Checkt man sich einmal
&lt;a href=&#34;https://alioth.debian.org/projects/webapps-common/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;webapps-common&lt;/a&gt;

 aus und liest, was da drin steht - oder vielmehr nicht drin steht, sieht man, daß ein solches Packaging nur Nachteile haben kann:&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&#34;21-web-applications-and-the-fhs&#34;&gt;
    &lt;a href=&#34;#21-web-applications-and-the-fhs&#34;&gt;
	2.1 Web applications and the FHS
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Web applications should follow the same guidelines as any other software. most specifically, they should not make any assumption about how the administrator has arranged the file hierarchy outside of the FHS by placing files in non-standard places such as /var/www or /usr/local. Specifically, the following table should serve as guidelines for the location of files:&lt;/p&gt;
&lt;p&gt;| type of file                           | location                        |
+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;+
| static web pages                       | /usr/share/PACKAGE/www          |
| dynamically interpreted web pages      | /usr/share/PACKAGE/www          |
| persistent application data            | /var/lib/PACKAGE                |
| dynamcially executed web pages         | /usr/lib/cgi-bin/PACKAGE        |
| application-specific libraries         | /usr/share/PACKAGE/include      |
| site configuration                     | /etc/PACKAGE                    |
| locally modifiable/overridable content | /etc/PACKAGE/templates          |
| php libraries                          | /usr/share/php/PACKAGE          |
| rrd, mrtg and other database files     | see database application policy |&lt;/p&gt;
&lt;p&gt;Fußnoten weggelassen.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Das ist so kraß konsequent an der Realität vorbei, daß es mir persönlich schon wieder Respekt abnötigt.&lt;/p&gt;
&lt;p&gt;Webanwendungen sind überall für Webhostingumgebungen entwickelt, denn dies ist bei weitem die verbreiteste Form des Deployments für Webanwendungen. Webhostingumgebungen werden aber in der Regel per Filetransfer in ein Verzeichnis unterhalb der DocumentRoot der Kundendomain beschickt. Sicherheitsmechanismen in Webhostingumgebungen berücksichtigen das: Mit chroot() oder Virtual Base Directory versuchen sie, den Kunden auf seinen Verzeichnisteilbaum zu beschränken. mit einer UID und GID pro Kunde oder pro Kundendomain versuchen sie, Betriebssystem-Zugriffsrechte als Hilfsmittel zur Kundentrennung zu benutzen.&lt;/p&gt;
&lt;p&gt;Eine Webapp-Policy, die das komplett ignoriert und Webanwendungen atomisiert, um ihre Trümmer dann einmal durch das Dateisystem zu zerstäuben hat so überhaupt nichts mit der Realität zu tun, daß man im ersten Augenblick nur sprachlos daneben stehen kann.&lt;/p&gt;
&lt;p&gt;Okay, hier ist also einmal ein Katalog von Anforderungen:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Policy muß chroot/virtual base directory jails unterstützen&lt;/li&gt;
&lt;li&gt;Policy muß UID pro Domain/GID pro Kunde oder ähnliche Setups unterstützen, die auf Apache suexec oder PHP safe_mode basieren&lt;/li&gt;
&lt;li&gt;Policy muß erlauben, daß die Anwendung mehr als einmal pro physikalischem Rechner installiert wird&lt;/li&gt;
&lt;li&gt;Policy muß erlauben, daß die Anwendung in mehr als einer Version pro physikalischem Rechner installier wird.&lt;/li&gt;
&lt;li&gt;Policy muß dem Kunden erlauben, seine Installation per ftp/scp/rsync zu sichern und zu modifizieren. Versionsmanagement muß erkennen, ob die modifizierte Anwendung danach noch automatisiert updatebar ist und ggf. einen Mergeprozeß unterstützen.&lt;/li&gt;
&lt;li&gt;Policy muß die Installation mehrerer unterschiedlicher Webanwendungen pro Vhost in unterschiedliche Directories unterhalb der DocRoot eines VHosts unterstützen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Die Policy muß also ein Framework liefern, mit dem man Vhosts schnell erzeugen kann, mit dem man in einem Vhosts Features wie modlogan, perl, python, php, chroot Jail und andere Eigenschaften enablen und disablen kann, und mit dem man pro VHost eine oder mehrere Anwendungen in unterschiedlichen Verzeichnissen installieren und möglicherweise sogar sicher aktualisieren kann. Dabei muß die Anwendung durch Kopie aus einem Shared Repository installiert werden können, durch Hardlink auf das Repository um Platz zu sparen oder sie muß, wenn die Anwendung das unterstützt, shared installs (PEAR, S9Y, &amp;hellip;) möglich machen.&lt;/p&gt;
&lt;p&gt;All das ist natürlich mit dem FHS komplett inkompatibel. Aber kompatibel mit den Anforderungen des Webgeschäfts, das nun einmal mit dem FHS erst einmal genau gar nix zu tun hat.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dateisysteme und Datenbanken</title>
      <link>https://blog.koehntopp.info/2004/06/06/dateisysteme-und-datenbanken.html</link>
      <pubDate>Sun, 06 Jun 2004 14:06:40 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2004/06/06/dateisysteme-und-datenbanken.html</guid>
      <description>&lt;p&gt;Der Artikel &lt;a href=&#34;http://matthias.leisi.net/archives/45_Filesysteme_sind_Datenbanken.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Filesysteme sind Datenbanken&lt;/a&gt;

 von Matthias Leisi regt mich an, hier mal ein paar Sachen aufzuschreiben, die ich schon länger vor mir her kullere.&lt;/p&gt;
&lt;p&gt;Die meisten Unix-Dateisysteme trennen eine &amp;ldquo;Gruppiere Blocks in Dateien&amp;rdquo;-Ebene (Blockverwaltung) und die &amp;ldquo;Gruppiere Dateien in Hierarchien&amp;rdquo;-Ebene (Namensraumverwaltung) voneinander. Die Blockverwaltung ist relativ gut verstanden und der I/O-Layer von Datenbanken überlegen. Die durch WinFS ausgelöste Diskussion findet stattdessen im Bereich Namensraumverwaltung statt.&lt;/p&gt;
&lt;h2 id=&#34;blockverwaltung-und-die-überlegenheit-der-filesystem-api&#34;&gt;
    &lt;a href=&#34;#blockverwaltung-und-die-%c3%bcberlegenheit-der-filesystem-api&#34;&gt;
	Blockverwaltung und die Überlegenheit der Filesystem API
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Die Ebene der Blockverwaltung bei Dateisystemen ist sehr hoch optimiert und muß akzeptable Performance unter extrem variablen Benutzungspatterns abliefern können. Ein einzelnes Dateisystem kann im selben Moment sehr viele kleine Dateien enthalten, wie sie zum Beispiel in einem Cyrus-Mailspool oder einem INN tradspool vorkommen, und einige wenige sehr große Dateien, wie sie zum Beispiel von einer Datenbank angelegt werden. Es muß damit zurecht kommen, daß eine Anwendung sehr viele sehr kleine Dateien in Folge öffnen will - der IMAP-Server, der durch den Mailspool tobt, und daß zeitgleich sehr viele andere Anwendungen auf einer großen Datei hin- und her seeken und zeitgleich Stücke der dieser großen Datei beschreiben und lesen wollen - die Threads der Datenbank, die auf das Datenbankfile zeitgleich einschlagen.&lt;/p&gt;
&lt;p&gt;Die API, mit der dies abgewickelt wird, ist extrem simpel - das open/read/write/seek/close/lock-Interface ist steinalt und sehr gut getestet. Es ist - inklusive von neueren Optimierungen wie readv/writev und mmap - auch sehr effizient und erlaubt es Anwendungen, Daten von der Platte zu lesen oder auf die Platte zu schreiben, ohne daß das Betriebssystem dafür Bytes umkopieren müßte. Es ist sogar so effizient, daß einige Datenbanken die Option bieten, auf BLOB-Daten mit Hilfe dieser API zuzugreifen, indem die Datenbank die Tabelle, bzw. die Objekte in der Tabelle als NFS-Dateibaum exportiert.&lt;/p&gt;
&lt;p&gt;Auf dieser Ebene des Dateisystems geht es also nicht darum, irgendwelche Queries zu beantworten oder Daten wiederzufinden, sondern nur darum, Daten mit sehr heterogener Granularität und sehr variablen Anforderungen an den Zugriff möglichst effizient in den Speicher zu schaffen bzw. auf die Platte zu befördern.&lt;/p&gt;
&lt;p&gt;Dateisysteme machen - zumindest in Unix - keine Annahmen über die Struktur von Daten. Dateien als solche sind strukturlose Blobs, und den Anwendungen steht es dann frei, eine Struktur in die Daten hinein zu interpretieren. Das System aber hält seine Finger da heraus, und manipuliert Dateien entweder als Klotz, oder Stücke davon als Byteranges.&lt;/p&gt;
&lt;p&gt;Das ist eine bewußte Entscheidung der Entwickler von Unix gewesen, nachdem sie Erfahrungen mit anderen Systemen hatten und wußten, wieviel mehr Komplexität auf Systemebene notwendig ist, damit Dateien mit Satzstrukturen behandelt werden können. Andere Systeme zu dieser Zeit haben Dateien mit Satzstrukturen implementiert (ISAM-Files und ähnliches), und sind damit eher weniger gut gefahren: Datenbanken als Userland-Anwendungen sind leichter zu implementieren und zu optimieren als Datenbanken als Teil des Kernels, und Datenbanken als Teil des Kernels bieten keine Performance-Vorteile.&lt;/p&gt;
&lt;p&gt;Wir nehmen die Folgende Liste der Vorteile auf der Ebene der Blockverwaltung mit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keine Annahmen über die Struktur erlaubt Freiheit in der Definition der Datenstrukturen und Freiheit in der Strukturierung der Zugriffe.&lt;/li&gt;
&lt;li&gt;Zero Copy I/O&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;namensraumverwaltung&#34;&gt;
    &lt;a href=&#34;#namensraumverwaltung&#34;&gt;
	Namensraumverwaltung
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Man kann sich die Blockverwaltungsschicht wie ein einziges großes Verzeichnis ohne Unterverzeichnisse vorstellen, in dem alle Dateien eines Dateisystems enthalten sind, und zwar mit ihrer Inode-Nummer als Name. Die Aufgabe der Namensraumverwaltung ist nun, die Pfadnamen, die im Userland gebräuchlich sind, in Inodenummern zu übersetzen. Auf der Ebene der Blockverwaltung arbeitet Unix dann immer nur mit den Inodenummern und den daran hängenden Verwaltungsstrukturen, den Inodes. Es gibt keine Funktion openi() (&amp;ldquo;Open File by Inode number&amp;rdquo;) in der Kernel-API.&lt;/p&gt;
&lt;p&gt;Über dieser Schicht liegt die Namensraumverwaltung. Die ist in unixoiden Systemen traditionell hierarchisch strukturiert und der Zugriff auf Daten erfolgt immer über Pfadnamen relativ zum aktuellen Verzeichnis eines Prozesses oder zur aktuellen Wurzel des Dateisystems eines Prozesses. Dies ist ein sehr simpler Mechanismus der Strukturbildung auf dem flachen See der Inodenummern, der viele Jahre ausgereicht hat, um Dateien zu sortieren und wiederzufinden.&lt;/p&gt;
&lt;p&gt;Zur Zeit geschieht dies immer intern im Kernel. Die Funktion, die dies macht, bekommt entweder das aktuelle Wurzelverzeichnis des aufrufenden Prozesses (wenn der Dateiname mit / anfängt) oder das aktuelle Arbeitsverzeichnis (in allen anderen Fällen) als Startwert und übersetzt den Pfadnamen dann Schritt für Schritt rekursiv in eine Folge-Inodenummer und einen Pfadrest.&lt;/p&gt;
&lt;p&gt;Wir nehmen an Gedanken aus dieser Sektion mit: Die Ansteuerung von Dateien erfolgt in Unix traditionell über den Pfad und nicht über eine ID wie die Inode-Nummer. Das Auflösen des Pfadnamens in eine Inode-Nummer erfordert weitere, über die Rechte an der Datei hinausgehende Zugriffsrechte, die vom verwendeten Pfadnamen abhängen. Für ein modernes Rechtesystem ist das weder notwendig noch wünschenswert, und es sollte möglich sein, Dateien über eine ID - die Inode-Nummer oder eine Datei-UUID anzusprechen. Windows- und Apple-Dateisysteme erlauben dies sogar schon zum Teil.&lt;/p&gt;
&lt;h2 id=&#34;andere-strukturbildner&#34;&gt;
    &lt;a href=&#34;#andere-strukturbildner&#34;&gt;
	Andere Strukturbildner
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Einige Artikel diskutieren nun andere Strukturbildner für ein Dateisystem. El Reg hat zum Beispiel ein &lt;a href=&#34;http://www.theregister.co.uk/2002/03/29/windows_on_a_database_sliced/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interview&lt;/a&gt;

 mit den Entwicklern von BeOS über die Probleme, die sie bei der Entwicklung der BeOS &amp;ldquo;Dateisystem&amp;rdquo;-Datenbank gehabt haben. Dem Interview kann man entnehmen, daß eine Datenbank im Userland zwar keine Performance-Vorteile hat, aber die Synchronisation zwischen Datenbank und Dateisystem leichter zu realisieren ist, wenn entsprechende Synchronisationsprimitive existieren und das Interface zu diesen Funktionen ist leicher zu realisieren, wenn die Datenbank Teil des Kernels ist.&lt;/p&gt;
&lt;p&gt;Die Beispiele, die für BeOS gegeben werden, gehen von einer traditionellen Datenbank mit Feldern und Werten aus. Der Artikel mit dem langatmigen Titel &lt;a href=&#34;http://www.namesys.com/whitepaper.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Namespaces As Tools for Integration the Operating System Rather As Ends in Themselves&lt;/a&gt;

 schildert die Sicht von Hans Reiser auf dieses Problem. In dem Artikel versucht Hans Reiser zu erläutern, warum er eine feste Tabellenstruktur oder RDF-Syntax für eine Abfragesprache in Dateisystemen eher für sinnlos halt. Seine Beweisführung streift schon fast das Argument &amp;ldquo;Google für Dateisysteme&amp;rdquo;. Wenn er dann jedoch beginnt, eine Abfragesprache mit einer &amp;ldquo;dateinamenähnlichen&amp;rdquo; Syntax herzuleiten, erkennt man wieder Strukturen mit einer &amp;ldquo;Name=Wert&amp;rdquo;-Syntax.&lt;/p&gt;
&lt;p&gt;Reiser geht davon aus, daß es Dateisystem-Plugins gibt, die er Klassifikatoren nennt. Diese Plugins generieren Schlüsselwerte, unter denen die Datei zu finden ist. In einem traditionellen Unix-Dateisystem stellt der Benutzer die Schlüssel bereit, indem er eine Datei unter einem Namen in einem Verzeichnis ablegt (und durch Links kann eine Datei unter mehr als einem Schlüssel abgelegt sein). In Reisers Modell werden weitere Schlüssel automatisch generiert - abhängig vom Typ der Datei. Das kann etwa ein Volltextindex sein - eine Datei ist dann unter allen in der Datei vorkommenden Worten zu finden, oder es können Schlüssel-Schlüsselwert Paare sein wie &amp;ldquo;mime-type: audio/mp3&amp;rdquo;, &amp;ldquo;subject: strike&amp;rdquo; oder &amp;ldquo;from: santa&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;In seiner Syntax ist &amp;ldquo;/etc/passwd&amp;rdquo; die Datei passwd in der &amp;ldquo;etc&amp;rdquo;-Gruppierung, während &amp;ldquo;[dragon gandalf bilbo]&amp;rdquo; die Datei ist, die den drei genannten Schlüsselwerten zugleich genügt. Später verallgemeinert er dann den &amp;ldquo;/&amp;quot;-Operator zu einer &amp;ldquo;Syntax Barrier&amp;rdquo; und kommt so zu Queries mit &amp;ldquo;Funktionen&amp;rdquo; wie &amp;ldquo;case-insensitve/[computer privacy laws]&amp;rdquo;, zu Security Barriers wie &amp;ldquo;[my secrets]/[love letter susan]&amp;rdquo; und zu einer Attributsyntax wie &amp;ldquo;[subject/[illegal strike] to/elves from/santa document-type/RFC822 ultimatum]&amp;rdquo;. Er weist darauf hin, daß man zu einer quasi-relationalen Query gelangt, wenn man das vorhergehende Beispiel zu &amp;ldquo;[subject/strike to/elves from/santa document-type/RFC822]&amp;rdquo; vereinfacht.&lt;/p&gt;
&lt;h2 id=&#34;objektklassen-und-attributsyntax&#34;&gt;
    &lt;a href=&#34;#objektklassen-und-attributsyntax&#34;&gt;
	Objektklassen und Attributsyntax
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Und an dieser Stelle landet man dann bei weiterem Nachdenken bei LDAP. Während LDAP eine stark denormalisierte Datenbankstruktur ist (LDAP ist nicht einmal in erster Normalform), besteht der Wert von LDAP paradoxerweise darin, ein sehr rigides Datenmodell zu haben. LDAP definiert einige Datentypen und die für diese Datentypen zugelassenen Attribute sowie ein Vererbungssystem für Typen. Alle LDAP-Anwendungen verwenden diese Datentypen oder erben von ihnen, sodaß unterschiedliche LDAP-Storages untereinander kompatibel sind.&lt;/p&gt;
&lt;p&gt;Der Wert von LDAP besteht also nicht im Storage oder der Abfragesprache (beide sind kaputt bzw. unvollständig), sondern im Schema und im Wire-Protokoll, die beide extrem interoperabel sind und die interoperabel erweiterbar sind. Der Wert von LDAP liegt also in der Normierung.&lt;/p&gt;
&lt;p&gt;Diese Überlegung ist auch auf eine Dateisystem-Abfragesprache anwendbar: Betrachtet man die Beispiele von Reiser genauer, findet man im Grunde zwei Szenarien, die Reiser unterscheidet.&lt;/p&gt;
&lt;p&gt;Das eine Szenario sind Anfragen nach Daten einer bestimmten Objektklasse (&amp;ldquo;mime-type: audio/mp3&amp;rdquo;, &amp;ldquo;mime-type: message/rfc822&amp;rdquo;), bei denen der Fragesteller dann Annahmen über das Vorhandensein bestimmter Attribute machen kann und bei denen die weitere Anfrage dann das Vorhandensein und die Bedeutung bestimmter Attributnamen voraussetzen kann. Das Characteristische an der Anfrage &amp;ldquo;[subject/[illegal strike] to/elves from/santa document-type/RFC822 ultimatum]&amp;rdquo; ist ja die Typabsicherung &amp;ldquo;document-type/&amp;hellip;&amp;rdquo;. Nur deswegen kann sich der Fragesteller sicher sein, daß Attribute wie &amp;ldquo;subject&amp;rdquo;, &amp;ldquo;from&amp;rdquo; und &amp;ldquo;to&amp;rdquo; existieren und daß ihre Werte die gesuchten Bedeutungen haben.&lt;/p&gt;
&lt;p&gt;Die zweite Sorte Frage ist die Googlesuche nach dem Vorkommen von bestimmten Schlüsselworten &lt;em&gt;irgendwo&lt;/em&gt; in der Datei, ohne Rücksicht auf die Strukturen, in die sich die Datei einpassen läßt. Das ist quasi die klassische Volltextsuche, mit einem Klassifikator-Plugin, daß für alle möglichen (auch multimedialen) Datentypen so viele sinnvolle Worte wie möglich extrahiert.&lt;/p&gt;
&lt;p&gt;Man beachte, daß der erste Fall nicht wirklich ein hierarchisches, denormalisiertes Datenbankschema benötigt wie etwa LDAP es bereitstellt. Es ist dagegen vollkommen ausreichend, ein objektrelationales System zu haben, in dem eine Entity von einer anderen Entity erben kann und für dieses System ein Schema bereitzustellen, das Attribute für alle gängigen und interessanten Mime-Types bereitstellt.&lt;/p&gt;
&lt;p&gt;Neben der Arbeit der Implementierung eines solches Dateisystems ist also weitere Arbeit notwendig, nämlich die Normierung von Datenstrukturen, also die Erarbeitung einer Taxonomie von Attributen und Objektklassen für die in einem modernen System vorkommenden Mime-Typen.&lt;/p&gt;
&lt;p&gt;Auch an der Abfragesprache ist noch Arbeit zu leisten - die von Reiser vorgeschlagene Syntax ist einfach, aber unvollständig und definiert keine vollständige Navigation. Die Abfragesprache von LDAP ist ebenfalls unvollständig und hat eine ganze Reihe von Einschränkungen, nicht nur vom relationalen Standpunkt aus, sondern schon auf einer sehr viel niedrigeren Ebene - hier möchte man sich eher bei den Gedankenmodellen von XPath und XQuery bedienen, die eine reichere Syntax und mehr Möglichkeiten liefern. Auf diese Weise wäre auch die Adressierung von Elementen &lt;em&gt;in&lt;/em&gt; Dateien möglich.&lt;/p&gt;
&lt;h2 id=&#34;fazit&#34;&gt;
    &lt;a href=&#34;#fazit&#34;&gt;
	Fazit
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Wir nehmen abschließend mit:&lt;/p&gt;
&lt;p&gt;Ein Dateisystem ist keine Datenbank im herkömmlichen Sinne, sondern ein Speicher, der auch mit unstrukturierten oder partiell strukturierten Daten klarkommen muß. Ein wenig mehr Struktur und ein wenig bessere Rechnerchemöglichkeiten täten heutigen Dateisystemen jedoch sehr gut. Dabei ist jedoch der Strukturlosigkeit Rechnung zu tragen und es sind neben strukturierten Anfragen im SQL-Stil auch strukturlose Anfragen im Stile einer Volltextsuche über alle Attribute zu ermöglichen.&lt;/p&gt;
&lt;p&gt;Diese Anfragen dienen weiterhin nur zur Bestimmung der Inodenummer/ID/UUID einer Datei, also zur Lokalisierung der Datei. Der weitere Zugriff kann dann ganz normal über die Blockverwaltung erfolgen, und muß nicht langsamer als heutzutage sein.&lt;/p&gt;
&lt;p&gt;Der Wert einer solchen Datenbank als Dateisystem ergibt sich wie bei LDAP aus der Normierung der Schemata. Es sind also rechtzeitig sinnvolle Schemata für gängige Systemdatentypen bereitzustellen und es ist ein offener, herstellerunabhängiger Prozeß für die Normierung und Entwicklung einer Hierarchie von Objektklassen und Attributtypen bereitzustellen.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

