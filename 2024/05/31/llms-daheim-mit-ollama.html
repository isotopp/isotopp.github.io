<!doctype html>
<html lang="en">
    <head>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<title>LLMs daheim mit Ollama | Die wunderbare Welt von Isotopp</title>
<meta name="description" content="Kris Köhntopp&#39;s blog (Fedi: @isotoppinfosec.exchange)">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="apple-touch-icon" href="icon.png">
<link rel="icon" href="favicon.ico" type="image/ico">
<link rel="shortcut icon" href="favicon.ico">
<link rel="canonical" href="https://blog.koehntopp.info/2024/05/31/llms-daheim-mit-ollama.html">

<link rel="alternate" type="application/rss+xml" href='https://blog.koehntopp.info/feed.xml' title="Die wunderbare Welt von Isotopp">
<link rel="alternate" type="application/rss+xml" href='https://blog.koehntopp.info/tags/mysql/feed.xml' title="Feed: mysql Articles for Die wunderbare Welt von Isotopp">
<link rel="alternate" type="application/rss+xml" href='https://blog.koehntopp.info/tags/review/feed.xml' title="Feed: review Articles for Die wunderbare Welt von Isotopp">


<meta name="generator" content="Hugo 0.145.0">
<meta property="og:title" content='LLMs daheim mit Ollama | Die wunderbare Welt von Isotopp' />
<meta property="og:site_name" content='Die wunderbare Welt von Isotopp' />
<meta property="og:locale" content="de_DE" />
<meta name="description" content='Kris Köhntopp&#39;s blog (Fedi: @isotoppinfosec.exchange)' />
<meta property="og:description" content='Kris Köhntopp&#39;s blog (Fedi: @isotoppinfosec.exchange)' />
<meta property="og:url" content="https://blog.koehntopp.info/2024/05/31/llms-daheim-mit-ollama.html" />
<meta property="og:type" content="article" />
<meta name="article:published_time" content='2024-05-31T05:06:07Z' />
<meta name="fediverse:creator" content="@isotopp@infosec.exchange">
<meta property="og:image" content='https://blog.koehntopp.info/assets/img/background/rijksmuseum.jpg' />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:creator" content='@isotopp@infosec.exchange' />
<meta property="twitter:title" content='LLMs daheim mit Ollama' />
<meta property="twitter:description" content='Kris Köhntopp&#39;s blog (Fedi: @isotoppinfosec.exchange)' />
<meta property="twitter:image" content='https://blog.koehntopp.info/assets/img/background/rijksmuseum.jpg' />


    



<link rel="stylesheet" href="https://blog.koehntopp.info/style.min.dd9d518fe6cac55191b6af874e18327dff051055754ffc7e106131baf826e6d3.css">


    </head>
    <body>
        

        <nav class="navbar navbar-expand-lg navbar-light px-lg-4 pt-lg-4">
    <div class="container-fluid">
        <a class="navbar-brand justify-content-center" href="https://blog.koehntopp.info/" rel="home" title="Die wunderbare Welt von Isotopp">
            <img alt="Kris" height='48' width='48' src='/assets/img/avatars/isotopp.jpg' class='p-0 me-3 d-block d-lg-inline'>
            <span class='h4 d-block d-lg-inline'>Die wunderbare Welt von Isotopp</span>
        </a>

        <button class="navbar-toggler ms-auto" type="button" data-bs-toggle="collapse"
                data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false"
                aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse text-center" id="navbarSupportedContent">
            <ul class="navbar-nav ms-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="/about/">
                        <span class="">About</span>
                        
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="/contribute/">
                        <span class="">Contribute</span>
                        
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="/search/">
                        <span class=""><svg class='bi' height='1.5rem' width='1.5rem' fill='currentColor'><use xlink:href='/bootstrap-icons.svg#search'/></svg></span>
                        
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="/tags/">
                        <span class=""><svg class='bi' width='1.5rem' height='1.5rem' fill='currentColor'><use xlink:href='/bootstrap-icons.svg#tags-fill'/></svg></span>
                        
                    </a>
                </li>
                
            </ul>
            
        </div>

    </div>
</nav>


        <main role="main" class="container-fluid p-0">

            









<div class="page">
	<article class="llms-daheim-mit-ollama-page">
		<div class='row  justify-content-center text-center my-4 mx-0'>
			<header class="headerbanner">
				<div class='col'>
					<h1 class="title mb-lg-4 text-white">
						LLMs daheim mit Ollama
					</h1>
					<div class='text-uppercase text-light' style='letter-spacing: 0.1rem;'>
						<img alt='@isotopp@infosec.exchange image' src='/assets/img/avatars/isotopp.jpg' class='me-1 p-0'
						     style='width: 1.9rem; border-radius: 1rem;'>
						<a href="https://infosec.exchange/@isotopp" class='text-light text-decoration-none'>
							Kristian Köhntopp
						</a>
						
						<span class='d-none d-lg-inline'>-</span>
						<div class='d-block d-lg-inline'>May 31, 2024</div>
						
					</div>
				</div>
				
				<img alt='a featured image' src="/assets/img/background/rijksmuseum.jpg">
				
			</header>
		</div>

		<div class='row justify-content-center mx-0'>
			<div class='col-lg-4 text-center text-lg-start'>
				
				<div>
					<div class='letter-spacing-01 text-uppercase text-secondary'>
						Previous Post
					</div>
					<a class='text-decoration-none' href="https://blog.koehntopp.info/2024/04/21/mysql-latency-and-iops.html">MySQL: Latency and IOPS</a>
				</div>
				
			</div>

			<div class='col-lg-4 text-lg-end mt-5 mt-lg-0 text-center'>
				
				<div>
					<div class='letter-spacing-01 text-uppercase text-secondary'>
						Next Post
					</div>
					<a class='text-decoration-none' href="https://blog.koehntopp.info/2024/06/07/beyond-rwx.html">Beyond rwx: Linux permissions</a>
				</div>
				
			</div>
		</div>

		<div class='row justify-content-center mx-0 mt-3 mb-5'>
			<div class="col-lg-8 mb-3">
				
				<header class="toc bg-light">
					<nav id="TableOfContents">
  <ul>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#ollama-serve-ausführen"><code>ollama serve</code> ausführen</a></li>
    <li><a href="#modelle-aus-dem-internet-laden">Modelle aus dem Internet laden</a></li>
    <li><a href="#modelle-ausführen">Modelle ausführen</a></li>
    <li><a href="#arbeiten-mit-ollama-run-texte-zusammenfassen">Arbeiten mit <code>ollama run</code>: Texte zusammenfassen</a></li>
    <li><a href="#selbst-coden">Selbst coden</a></li>
  </ul>
</nav>
				</header>
				
			</div>
			<div class='col-lg-8'>
				<p>Dieser Text ist eine Art Follow-up für <a href="/2024/02/06/wie-chatgpt-funktioniert.html">Wie ChatGPT funktioniert</a>

.
Es geht darum, ein LLM lokal auszuführen und damit zu experimentieren.</p>
<p>Wer mit LLMs daheim experimentieren möchte, der steht vor dem Problem,
einen Haufen furchtbar empfindlicher und schlecht zu aktualisierender Abhängigkeiten in Python zu installieren,
monströse Downloads zu verwalten und die Ausführung für die eigene Maschine zu optimieren.
Das wird ein bisschen verbessert mit <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener">llama.cpp</a>

,
einer C++/C Bibliothek, die einem einen Teil der Arbeit abnimmt,
aber man hat immer noch kein Paket, mit dem man einfach Ergebnisse bekommt.</p>
<p>Abhilfe schafft hier <a href="https://github.com/ollama/ollama" target="_blank" rel="noopener">Ollama</a>

,
eine in Golang geschriebene Anwendung,
die llama.cpp integriert und den Download, die Verwaltung und die Ausführung von LLMs automatisiert.
Die <a href="https://ollama.com/" target="_blank" rel="noopener">Website</a>

 verweist auf das Blog,
den Discord und das Github von Ollama, und hat eine Suche,
mit der man auf Ollama angepasste LLms finden und herunterladen kann.</p>
<h1 id="installation">
    <a href="#installation">
	Installation
    </a>
</h1>
<p>LLMs sind rechenintensive Monster, die eine große Maschine zur Ausführung brauchen.
Aber eingedampfte Spezialversionen kann man schon auf einem großen Raspi ausführen.
Größe ist jedoch ein qualitativer Unterschied:
Ein LLM mit 0.3 Mrd. Parameters verhält sich ganz anders als eines mit 8 Mrd. Parametern,
und dieses wiederum wird von einem LLM mit 70 Mrd. Parameters vollkommen deklassiert.
Ersteres kann man auf einem Raspi ausführen,
die 8 Mrd. Parameter-Modelle oft auf einem Mac oder Windows-Rechner mit 32 GB RAM oder 12 GB VRAM in der Grafikkarte,
und die 70 Mrd. Parameter-Modelle brauchen in der Regel Spezialrechner mit 128 GB RAM und zwei 24 GB Nvidia-Karten kombiniert.</p>
<p>In meinem Fall steht mir</p>
<ul>
<li>ein Mac mini M2pro mit 32 GB RAM und 8+4 Cores</li>
<li>ein Windows 11 Rechner mit 64 GB RAM und Nvidia 4070Ti (12 GB VRAM)</li>
</ul>
<p>zur Verfügung. Beide Maschinen haben ausreichend Plattenplatz auf NVME.</p>
<p>Man kann sich Ollama von der Website herunterladen und installieren,
oder das Golang Projekt selbst clonen und compilieren,
aber die einfachste Weise der Installation sind Paketmanager für MacOS und Windows.</p>
<p>Auf dem Mac habe ich Homebrew, und <code>brew install ollama</code> ist die schnellste und einfachste Methode,
Ollama zu installieren und aktuell zu halten.
Auf Windows geht dasselbe mit <a href="https://scoop.sh/" target="_blank" rel="noopener">Scoop</a>

, <code>scoop install ollama</code> regelt die Details.</p>
<h1 id="ollama-serve-ausführen">
    <a href="#ollama-serve-ausf%c3%bchren">
	<code>ollama serve</code> ausführen
    </a>
</h1>
<p>Ollama wird als Server ausgeführt.
Dazu muss man <code>ollama serve</code> starten.</p>
<p>Alle weiteren Operationen setzen voraus, daß in einem Fenster irgendwo <code>ollama serve</code> aktiv ist,
denn alle weiteren Operationen tun nichts anderes als REST-Requests an <code>ollama serve</code> zu senden.
Dies kann mit der <code>ollama</code>-Kommandozeile passieren, oder man schreibt sich eigene Python-Programme, die das tun</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> ollama serve
</span></span><span class="line"><span class="cl"><span class="go">kk:~ kris$ ollama serve
</span></span></span><span class="line"><span class="cl"><span class="go">2024/05/31 09:52:25 routes.go:1008: INFO server config 
</span></span></span><span class="line"><span class="cl"><span class="go">  env=&#34;map[OLLAMA_DEBUG:false OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512
</span></span></span><span class="line"><span class="cl"><span class="go">   OLLAMA_MAX_VRAM:0 OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 
</span></span></span><span class="line"><span class="cl"><span class="go">   OLLAMA_ORIGINS:[http://localhost ... https://0.0.0.0:*] 
</span></span></span><span class="line"><span class="cl"><span class="go">   OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&#34;
</span></span></span><span class="line"><span class="cl"><span class="go">time=2024-05-31T09:52:25.124+02:00 level=INFO source=images.go:704 msg=&#34;total blobs: 28&#34;
</span></span></span><span class="line"><span class="cl"><span class="go">time=2024-05-31T09:52:25.131+02:00 level=INFO source=images.go:711 msg=&#34;total unused blobs removed: 0&#34;
</span></span></span><span class="line"><span class="cl"><span class="go">[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.
</span></span></span><span class="line"><span class="cl"><span class="go">...
</span></span></span><span class="line"><span class="cl"><span class="go">time=2024-05-31T10:02:15.032+02:00 level=INFO source=payload.go:44 msg=&#34;Dynamic LLM libraries [metal]&#34;
</span></span></span><span class="line"><span class="cl"><span class="go">time=2024-05-31T10:02:15.090+02:00 level=INFO source=types.go:71 msg=&#34;inference compute&#34; 
</span></span></span><span class="line"><span class="cl"><span class="go">  id=0 library=metal compute=&#34;&#34; driver=0.0 name=&#34;&#34; total=&#34;21.3 GiB&#34; available=&#34;21.3 GiB&#34;
</span></span></span></code></pre></div><p>Ollama lädt später Parameterdaten für die LLMs herunter. Diese nehmen einige Gigabyte Plattenplatz ein.
Per Default legt das MacOS Ollama dies im Home des Benutzers ab, in <code>$HOME/.ollama/models</code>.
Man kann dies mit einer Umgebungsvariable umstellen.</p>
<p>In diesem Fall muss man <code>OLLAMA_MODELS</code> setzen:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">kk:~ kris$ ollama serve --help
</span></span></span><span class="line"><span class="cl"><span class="go">Start ollama
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Usage:
</span></span></span><span class="line"><span class="cl"><span class="go">  ollama serve [flags]
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Aliases:
</span></span></span><span class="line"><span class="cl"><span class="go">  serve, start
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Flags:
</span></span></span><span class="line"><span class="cl"><span class="go">  -h, --help   help for serve
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Environment Variables:
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">    OLLAMA_HOST         The host:port to bind to (default &#34;127.0.0.1:11434&#34;)
</span></span></span><span class="line"><span class="cl"><span class="go">    OLLAMA_ORIGINS      A comma separated list of allowed origins
</span></span></span><span class="line"><span class="cl"><span class="go">    OLLAMA_MODELS       The path to the models directory (default &#34;~/.ollama/models&#34;)
</span></span></span><span class="line"><span class="cl"><span class="go">    OLLAMA_KEEP_ALIVE   The duration that models stay loaded in memory (default &#34;5m&#34;)
</span></span></span><span class="line"><span class="cl"><span class="go">    OLLAMA_DEBUG        Set to 1 to enable additional debug logging
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gp">$</span> kk:~ kris$ mkdir /Volumes/Ablage/Torch/ollama-models/
</span></span><span class="line"><span class="cl"><span class="gp">$</span> <span class="nv">OLLAMA_MODELS</span><span class="o">=</span>/Volumes/Ablage/Torch/ollama-models/ ollama serve
</span></span></code></pre></div><p>Der Speicher dort sollte schnell sein und ausreichend Raum bereitstellen:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">kk:~ kris$ df -h /Volumes/Ablage
</span></span></span><span class="line"><span class="cl"><span class="go">Filesystem      Size    Used   Avail Capacity iused ifree %iused  Mounted on
</span></span></span><span class="line"><span class="cl"><span class="go">/dev/disk9s1   3.6Ti   804Gi   2.9Ti    22%    360k   31G    0%   /Volumes/Ablage
</span></span></span><span class="line"><span class="cl"><span class="go">kk:~ kris$ du -sh /Volumes/Ablage/Torch/ollama-models
</span></span></span><span class="line"><span class="cl"><span class="go"> 54G	/Volumes/Ablage/Torch/ollama-models
</span></span></span><span class="line"><span class="cl"><span class="go">kk:~ kris$ ollama list
</span></span></span><span class="line"><span class="cl"><span class="go">NAME                    	ID          	SIZE  	MODIFIED
</span></span></span><span class="line"><span class="cl"><span class="go">llama3:70b-instruct-q2_K	693db6efd8f9	26 GB 	6 days ago
</span></span></span><span class="line"><span class="cl"><span class="go">llama3:instruct         	365c0bd3c000	4.7 GB	6 days ago
</span></span></span><span class="line"><span class="cl"><span class="go">mistral:instruct        	2ae6f6dd7a3d	4.1 GB	6 days ago
</span></span></span><span class="line"><span class="cl"><span class="go">gemma:instruct          	a72c7f4d0a15	5.0 GB	6 days ago
</span></span></span><span class="line"><span class="cl"><span class="go">llama3-gradient:instruct	5d1398df5b8b	4.7 GB	6 days ago
</span></span></span><span class="line"><span class="cl"><span class="go">phi3:14b                	1e67dff39209	7.9 GB	7 days ago
</span></span></span><span class="line"><span class="cl"><span class="go">llava-llama3:latest     	44c161b1f465	5.5 GB	7 days ago
</span></span></span><span class="line"><span class="cl"><span class="go">llama3-gradient:latest  	5d1398df5b8b	4.7 GB	7 days ago
</span></span></span><span class="line"><span class="cl"><span class="go">phi3:14b-instruct       	1e67dff39209	7.9 GB	7 days ago
</span></span></span></code></pre></div><p>Der Ollama-Server bindet sich an localhost, Port 11434 und lauscht nun auf eingehende REST-Requests.
Diese können dazu führen, daß ein Model geladen und ausgeführt wird.
Das kann beim ersten Mal einige Zeit dauern, da viele Gigabyte Daten geladen werden müssen.</p>
<p>Der Server hält das Model einige Zeit im Speicher, <code>OLLAMA_KEEP_ALIVE</code> viele Minuten.
Der Default ist 5 m.
Danach wird der Speicher wieder freigegeben.</p>
<h1 id="modelle-aus-dem-internet-laden">
    <a href="#modelle-aus-dem-internet-laden">
	Modelle aus dem Internet laden
    </a>
</h1>
<p>LLMs sind recht komplizierte Konstrukte mit der eigentlichen Parameter-Datei, die den Großteil der Daten aus macht,
einem Systemprompt und weiteren Layers, die meist eher nicht so groß sind.
Auf einem handelsüblichen Desktop-Rechner kann man erwarten, ein &ldquo;7B&rdquo; oder &ldquo;8B&rdquo;-Modell auszuführen.
Dies belegt zwischen 5 GB und 9 GB an Speicher, im RAM oder auf der Grafikkarte.</p>
<p>Richtig große Modelle, 70B-Modelle, brauchen wesentlich mehr Speicher und eine Hardware,
die für die Ausführung von LLMs auf mehreren Grafikkarten vorbereitet ist.</p>
<p>Ein guter Startpunkt ist das Mistral AI-Model <a href="https://ollama.com/library/mistral" target="_blank" rel="noopener">Mistral</a>

.
Man lädt es mit <code>ollama pull mistral:instruct</code> herunter.
Das wird Zeit lang dauern und das Model-Directory entsprechend voll machen.</p>
<p><strong>HINWEIS:</strong> Noch einmal die Erklärung, daß alle Arbeit von <code>ollama serve</code> erledigt wird.
Wenn dieses Kommando nicht aktiv ist, funktioniert keines der anderen Kommandos.
<code>ollama pull</code>, <code>ollama run</code> und auch <code>ollama rm</code> senden lediglich REST-Requests an den Server,
der dann die Arbeit macht.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">kk:~ kris$ ollama pull mistral:instruct
</span></span></span><span class="line"><span class="cl"><span class="go">pulling manifest
</span></span></span><span class="line"><span class="cl"><span class="go">pulling ff82381e2bea... 100% ▕████████████████▏ 4.1 GB
</span></span></span><span class="line"><span class="cl"><span class="go">pulling 43070e2d4e53... 100% ▕████████████████▏  11 KB
</span></span></span><span class="line"><span class="cl"><span class="go">pulling c43332387573... 100% ▕████████████████▏   67 B
</span></span></span><span class="line"><span class="cl"><span class="go">pulling ed11eda7790d... 100% ▕████████████████▏   30 B
</span></span></span><span class="line"><span class="cl"><span class="go">pulling 42347cd80dc8... 100% ▕████████████████▏  485 B
</span></span></span><span class="line"><span class="cl"><span class="go">verifying sha256 digest
</span></span></span><span class="line"><span class="cl"><span class="go">writing manifest
</span></span></span><span class="line"><span class="cl"><span class="go">removing any unused layers
</span></span></span><span class="line"><span class="cl"><span class="go">success
</span></span></span></code></pre></div><h1 id="modelle-ausführen">
    <a href="#modelle-ausf%c3%bchren">
	Modelle ausführen
    </a>
</h1>
<p>Das Modell kann man mit <code>ollama run mistral:instruct</code> testen.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">&gt;</span>&gt;&gt; /set verbose
</span></span><span class="line"><span class="cl"><span class="go">Set &#39;verbose&#39; mode.
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="gp">&gt;</span>&gt;&gt; Explain in a single sentence <span class="s2">&#34;Why is the sky blue?&#34;</span>
</span></span><span class="line"><span class="cl"><span class="go"> The sky appears blue because molecules in the Earth&#39;s atmosphere scatter
</span></span></span><span class="line"><span class="cl"><span class="go">sunlight in all directions, and blue light is scattered more because it
</span></span></span><span class="line"><span class="cl"><span class="go">travels in shorter, smaller waves.
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">total duration:       1.559908875s
</span></span></span><span class="line"><span class="cl"><span class="go">load duration:        2.719417ms
</span></span></span><span class="line"><span class="cl"><span class="go">prompt eval count:    23 token(s)
</span></span></span><span class="line"><span class="cl"><span class="go">prompt eval duration: 303.421ms
</span></span></span><span class="line"><span class="cl"><span class="go">prompt eval rate:     75.80 tokens/s
</span></span></span><span class="line"><span class="cl"><span class="go">eval count:           41 token(s)
</span></span></span><span class="line"><span class="cl"><span class="go">eval duration:        1.245361s
</span></span></span><span class="line"><span class="cl"><span class="go">eval rate:            32.92 tokens/s
</span></span></span></code></pre></div><p>Wir sehen hier, daß mein M2 pro mini 32 GB 1.56 Sekunden gebraucht hat, um diese Antwort zu generieren.
Die Frage hat &ldquo;23 Token&rdquo; belegt, die mit 75 Token/s analysiert worden sind.</p>
<p>Die Generierung der Ausgabe hat 41 Token erzeugt, 1.25s gedauert und ist mit 33 Token/s gelaufen.
Der Rechner hat dabei ca. 50 Watt gezogen. Normal verbraucht er circa 7 Watt, also werden 43 Watt mehr verbraucht.</p>
<p><p class="md__image">
  <img src="/uploads/2024/05/ollama-3.png" alt=""  />
</p>

</p>
<p><em>Ausgabe der MacOS App Stats (<code>brew install stats</code>), der Rechner verbraucht normal ca. 7 Watt, mit dem LLM aktiv 50 Watt.</em></p>
<p><p class="md__image">
  <img src="/uploads/2024/05/ollama-1.png" alt=""  />
</p>

</p>
<p><em>Ausgabe der MacOS App Stats, hier: GPU Auslastung. Dies ist die einzige Anwendung, die die Mac mini GPU an den Anschlag bringt.</em></p>
<p>Die Nvidia 4070Ti kommt auf mehr als 70 Token/s, zieht aber über 400 W.</p>
<p>Der <code>ollama run</code>-Prompt kann mit <code>/bye</code> verlassen werden.</p>
<p>Die vorhandenen Modelle können mit <code>ollama list</code> aufgelistet werden.
Mit <code>ollama rm</code> löscht man ein lokales Modell und gibt den Plattenplatz frei.
Es muss dann neu heruntergeladen werden.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">kk:~ kris$ ollama rm llama3:instruct
</span></span></span><span class="line"><span class="cl"><span class="go">deleted &#39;llama3:instruct&#39;
</span></span></span></code></pre></div><h1 id="arbeiten-mit-ollama-run-texte-zusammenfassen">
    <a href="#arbeiten-mit-ollama-run-texte-zusammenfassen">
	Arbeiten mit <code>ollama run</code>: Texte zusammenfassen
    </a>
</h1>
<p>Für das folgende Beispiel ist ein laufender Server <code>ollama serve</code> erforderlich,
das Modell <code>mistral:instruct</code> muss bereitstehen (d.h. <code>ollama list</code> zeigt es an),
und wir verwenden den Text <a href="https://www.gutenberg.org/files/61213/61213.txt" target="_blank" rel="noopener">The 64-Square Madhouse</a>

 von Fritz Leiber.</p>
<p>Das Ziel ist es, Ollama diesen Text mithilfe von Llama 3 zusammenfassen zu lassen.</p>
<ol>
<li>Den Text von Fritz Leiber vom Projekt Gutenberg herunterladen.
Es ist hilfreich, allen Copyright-Text am Anfang und am Ende wegzuschneiden,
um dem Modell nur die Story selbst zu füttern.</li>
<li>Server starten: <code>ollama serve</code>.
Dabei sind ggf. die notwendigen Parameter als Umgebungsvariablen zu setzen, damit die Modelle gefunden werden.</li>
<li>Modell prüfen: <code>ollama list</code> zeigt das Modell <code>mistral:instruct</code> als vorhanden an.
Ansonsten muss es mit <code>ollama pull mistral:instruct</code> heruntergeladen werden.</li>
<li>Den Prompt mit <code>ollama run mistral:instruct</code> starten.</li>
</ol>
<p>Wir verwenden hier das Modell <code>mistral:instruct</code> von <a href="https://en.wikipedia.org/wiki/Mistral_AI" target="_blank" rel="noopener">Mistral AI</a>

.
Das ist eine Firma von ehemaligen Google und Meta/Facebook Mitarbeitern, die vor einem Jahr in Paris gegründet wurde.</p>
<p>Auf dem Prompt erfolgt die Kommunikation mit dem Modell in Englisch.
Manche Modelle sind ausdrücklich mehrsprachig trainiert (zum Beispiel <a href="https://ollama.com/library/mixtral" target="_blank" rel="noopener">Mixtral</a>

),
aber wenn dies in der Anleitung zum Modell nicht angegeben ist, ist Englisch zu verwenden.</p>
<p>Wir können dem Modell eine Anfrage in einer Zeile stellen.
Mehrzeilige Anweisungen werden mit <code>&quot;&quot;&quot;</code> eingeleitet und genau so beendet.</p>
<p>Es ist ein <code>instruct</code>-Modell, es wird also keine Annahmen machen, sondern wir müssen Arbeitsanwendungen geben.</p>
<p>Wir können dem Client Parameter setzen.
Dies erfolgt mit <code>/</code>-Kommandos, <code>/help</code> für eine Übersicht.</p>
<p>Modelle haben Parameter.
Ein wichtiger Parameter ist <code>num_ctx</code> (manchmal auch n_ctx).
Er bestimmt die Anzahl der Token, die das Modell als Kontextspeicher für die Antwortgenerierung verwendet.</p>
<p>Ein Embedding ist ein vieldimensionaler Vektor, der Bedeutung von Worten codiert.
Es belegt etwa 1 Kilobyte an Speicher.
Ein Token ist ein Zeiger auf ein Embedding, aber im Kontext der Antwortgenerierung wird das Token kopiert.
Ein Kontext von 65536 belegt also in etwa 64 MB Speicher.
In englischer Sprache entspricht ein Wort in etwa 1.3 Tokens.</p>
<p>Unser Beispieltext hat, nachdem wir vorne und hinten alles das wegschneiden,
was Projekt Gutenberg als Disclaimer hinzugefügt hat,
eine Größe von circa 14.000 Worten:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">kk:~ kris$ wc leiber-64-square.txt
</span></span></span><span class="line"><span class="cl"><span class="go">    1824   13853   84236 leiber-64-square.txt
</span></span></span></code></pre></div><p>Wenn wir dem Modell einen Kontext von 65536 Token geben, dann sollte es mit der Geschichte gut zurechtkommen.
<strong>HINWEIS:</strong>* Es heißt <code>/set parameter num_ctx 65536</code>, nicht <code>/set num_ctx 65536</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">kk:~ kris$ ollama run mistral:instruct
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="gp">&gt;</span>&gt;&gt; /set verbose
</span></span><span class="line"><span class="cl"><span class="go">Set &#39;verbose&#39; mode.
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="gp">&gt;</span>&gt;&gt; /set parameter num_ctx <span class="m">65536</span>
</span></span><span class="line"><span class="cl"><span class="go">Set parameter &#39;num_ctx&#39; to &#39;65536&#39;
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="gp">&gt;</span>&gt;&gt; <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2"></span><span class="go">... You are an english teacher.
</span></span></span><span class="line"><span class="cl"><span class="go">... Summarize the following story,
</span></span></span><span class="line"><span class="cl"><span class="go">... providing an overview over the main characters of the story,
</span></span></span><span class="line"><span class="cl"><span class="go">... and a paragraph for each scene or major event that happens in the course of
</span></span></span><span class="line"><span class="cl"><span class="go">... the story.
</span></span></span><span class="line"><span class="cl"><span class="go">... As a conclusion, provide context: What is the story about?
</span></span></span><span class="line"><span class="cl"><span class="go">... 
</span></span></span><span class="line"><span class="cl"><span class="go">... The story:
</span></span></span></code></pre></div><p>Wir setzen also <code>/set verbose</code>, um Statistiken zu bekommen.
Wir setzen dann den Parameter <code>/set parameter num_ctx 65536</code>, um dem Modell genug Arbeitskontext zu geben.
Danach leiten wir mit <code>&quot;&quot;&quot;</code> einen mehrzeiligen Input ein.
Dieser wird durch <code>&quot;&quot;&quot;</code> auf einer neuen Zeile genau so abgeschlossen.</p>
<p>In einem anderen Fenster laden wir nun die Story in den Paste-Buffer:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">(venv) kk:keks kris$ cat leiber-64-square.txt | pbcopy
</span></span></span><span class="line"><span class="cl"><span class="go">(venv) kk:keks kris$
</span></span></span></code></pre></div><p>Diese kann nun in den Ollama-Prompt gepasted werden.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">...
</span></span></span><span class="line"><span class="cl"><span class="go">... Sandra said, &#34;Well, there&#39;s something you can&#39;t build into a
</span></span></span><span class="line"><span class="cl"><span class="go">... machine--ego.&#34;
</span></span></span><span class="line"><span class="cl"><span class="go">...
</span></span></span><span class="line"><span class="cl"><span class="go">... &#34;Oh, I don&#39;t know about that,&#34; said Simon Great.
</span></span></span><span class="line"><span class="cl"><span class="go">...
</span></span></span><span class="line"><span class="cl"><span class="go">... &#34;&#34;&#34;
</span></span></span></code></pre></div><p>Die generierte Antwort des Modells lautet:</p>
<blockquote>
<p>This story is about a chess tournament featuring an electronic computing
machine named the Machine. The Machine is entering its first international
grandmaster chess tournament, and the large hall where the event takes
place is filled with energetic dark-suited men who are either playing
chess or discussing it. There are signs featuring various combinations of
letters such as FIDE, WBM, USCF, USSF, USSR, and UNESCO, and many clocks
bedside table size with red flags and wheels on their faces are present to
keep track of each player&rsquo;s moves. The Machine is programmed by a
psychologist named Simon Great, who has been out of tournament chess for
twenty years but is now making a comeback.</p>
<p>The main characters in the story are Sandra Lea Grayling, a journalist for
the <em>Chicago Space Mirror</em>, and Dr. Krakatower, a former grandmaster of
chess who is also a special correspondent for the London Times. The
Machine is also a character, as it plays against various human opponents
throughout the tournament.</p>
<p>The story unfolds over the course of several days, with each scene or
major event taking place during one of the nine rounds of the tournament.
In the first round, the Machine plays against Bela Grabo and wins the
game. This causes a stir among the players and spectators, as many believe
that the Machine is too good to be true. In the second round, the Machine
faces off against Votbinnik and again emerges victorious. This pattern
continues throughout the tournament, with the Machine consistently
outplaying its human opponents.</p>
<p>Dr. Krakatower struggles throughout the tournament, losing several games
and finding himself in last place. However, he manages to draw his game
against Sherevsky in the next to last round, which gives him a small
glimmer of hope. In the final round, the Machine plays against Willie
Angler, who has been consistently strong throughout the tournament. The
outcome of this match will determine the winner of the tournament.</p>
<p>As the story reaches its conclusion, it is revealed that Angler had
intentionally played a flawed move in his game against the Machine, as he
had discovered an error in the programming of the Machine&rsquo;s opening moves.
This tactic pays off, and Angler wins the game and the tournament. The
final standing shows that Angler and Votbinnik are tied for first place,
with the Machine in second.</p>
<p>The story concludes with a discussion between Simon Great, Willie Angler,
and Dr. Krakatower about the future of chess and computer programming. It
is suggested that grandmaster chess without computers will become less
popular, as players will increasingly rely on computers to aid them in
their games. The story also touches on the idea of man-machine symbiosis,
with the implication that future grandmasters may be a combination of
human and machine. The story serves as a commentary on the changing nature
of chess and the role of technology in society.</p></blockquote>
<p>und</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">total duration:       3m33.240299458s
</span></span></span><span class="line"><span class="cl"><span class="go">load duration:        20.467748958s
</span></span></span><span class="line"><span class="cl"><span class="go">prompt eval count:    22580 token(s)
</span></span></span><span class="line"><span class="cl"><span class="go">prompt eval duration: 1m58.450477s
</span></span></span><span class="line"><span class="cl"><span class="go">prompt eval rate:     190.63 tokens/s
</span></span></span><span class="line"><span class="cl"><span class="go">eval count:           634 token(s)
</span></span></span><span class="line"><span class="cl"><span class="go">eval duration:        1m14.263155s
</span></span></span><span class="line"><span class="cl"><span class="go">eval rate:            8.54 tokens/s
</span></span></span></code></pre></div><p>Wir haben eine recht gute Evaluation Rate: 190 Token/s.
Die Generierung der Antwort ist langsamer, auch wegen des großen Kontexts, den wir gesetzt haben: 8.5 Token/s.</p>
<p>Eine Laufzeit von 3 Minuten bei einem Mehrverbrauch von 43 Watt entspricht einem Energieaufwand von 2.15 Wh.
Bei einem Preis von 32 Cent pro 1000 Wh (1 kWh) sind das 2.15/1000 * 32 = .0672 Cent zur Generierung der Ausgabe.</p>
<h1 id="selbst-coden">
    <a href="#selbst-coden">
	Selbst coden
    </a>
</h1>
<p>Es gibt eine Ollama Python Bibliothek.
Diese tut nicht mehr, als Requests an <code>ollama serve</code> zu senden:</p>
<ul>
<li><a href="https://github.com/ollama/ollama-python" target="_blank" rel="noopener">ollama-python</a>

</li>
<li><a href="https://github.com/ollama/ollama/tree/main/docs" target="_blank" rel="noopener">ollama API Dokumentation</a>

, api.md</li>
<li><a href="https://docs.mistral.ai/" target="_blank" rel="noopener">Mistral Model Documentation</a>

, die Code-Beispiele dort gehen davon aus,
daß man deren Server verwendet. Das tun wir nicht, wir verwenden Ollama und eine lokale Instanz.</li>
</ul>
<p>Die REST API ist in <code>api.md</code> im Ollama Quelltext-Tree auf Github dokumentiert.</p>
<p>Die <code>ollama.chat()</code>-Funktion (und auch die gleichnamige Funktion der Mistral API, die gegen deren Instanz arbeitet),
hat einen Parameter <code>messages</code> (Plural, eine Liste).
Das LLM ist stateless.
Es kann sich nicht an vorhergehende Nachrichten erinnern.
Daher muss man dem Modell bei jedem Request alle vorhergehenden Fragen und Antworten und auch den Systemprompt mitsenden.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">o</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s2">&#34;127.0.0.1&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">response</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">=</span><span class="s2">&#34;mistral:instruct&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;system&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="n">instructions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;user&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="n">old_message</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;assistant&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="n">old_answer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;user&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="n">message</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span></code></pre></div><p>Genauso muss man <code>options={}</code> mitsenden, wenn man zum Beispiel mehr Kontext braucht.</p>
<p>Die Anleitung zu Mistral gibt bessere
<a href="https://docs.mistral.ai/guides/prompting_capabilities/#summarization" target="_blank" rel="noopener">Summarization Prompts</a>


als Beispiel und erklärt im hinteren Teil mehr zu Tokens, Tokenization und zum direkten Umgang mit dem LLM.</p>

			</div>
		</div>

		
		
		<div class='row justify-content-center py-3 mb-3 mx-0'>
			<div class='col-lg-8 text-center'>
				<span class='letter-spacing-01 text-uppercase text-secondary me-2'>Tags</span>
				
				<a href="/tags/#lang_de" class='btn btn-sm btn-outline-primary mb-1'>
				<svg class="bi" width="1rem" height="1rem" fill="currentColor">
					<use xlink:href="/bootstrap-icons.svg#tag-fill"></use>
				</svg>
				lang_de
				</a>
				
				<a href="/tags/#computer" class='btn btn-sm btn-outline-primary mb-1'>
				<svg class="bi" width="1rem" height="1rem" fill="currentColor">
					<use xlink:href="/bootstrap-icons.svg#tag-fill"></use>
				</svg>
				computer
				</a>
				
				<a href="/tags/#erklaerbaer" class='btn btn-sm btn-outline-primary mb-1'>
				<svg class="bi" width="1rem" height="1rem" fill="currentColor">
					<use xlink:href="/bootstrap-icons.svg#tag-fill"></use>
				</svg>
				erklaerbaer
				</a>
				
				<a href="/tags/#ai" class='btn btn-sm btn-outline-primary mb-1'>
				<svg class="bi" width="1rem" height="1rem" fill="currentColor">
					<use xlink:href="/bootstrap-icons.svg#tag-fill"></use>
				</svg>
				ai
				</a>
				
			</div>
		</div>

		
		
		
		
		<div class='row justify-content-center py-3 mb-3 mx-0'>
			<div class='col-lg-8 text-center'>
				<a href="https://github.com/isotopp/isotopp.github.io/edit/main/content/posts/2024-05-31-llms-daheim-mit-ollama.md"
				   rel="noopener noreferrer" target="_blank">
					<span class='letter-spacing-01 text-uppercase text-secondary me-2'>Suggest Changes</span>
				</a>
			</div>
		</div>
		

		<div class='row justify-content-center mx-0'>
			<div class='col-lg-4 text-center text-lg-start'>
				
				<div>
					<div class='letter-spacing-01 text-uppercase text-secondary'>
						Previous Post
					</div>
					<a class='text-decoration-none' href="https://blog.koehntopp.info/2024/04/21/mysql-latency-and-iops.html">MySQL: Latency and IOPS</a>
				</div>
				
			</div>

			<div class='col-lg-4 text-lg-end mt-5 mt-lg-0 text-center'>
				
				<div>
					<div class='letter-spacing-01 text-uppercase text-secondary'>
						Next Post
					</div>
					<a class='text-decoration-none' href="https://blog.koehntopp.info/2024/06/07/beyond-rwx.html">Beyond rwx: Linux permissions</a>
				</div>
				
			</div>
		</div>


		
	</article>
</div>



            <footer class='row justify-content-center mb-4 pb-4 mx-0'>
  <div class='col-8 text-center'>
    <div>
      A collection of old stuff, new stuff and random stuff.
    </div>
    <div class='row text-primary mt-4'>
      <div class='col'>

        <a href='/feed.xml' title='Follow RSS feed' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#rss-fill'/>
          </svg>
        </a>

        <a href='mailto:kristian.koehntopp@gmail.com' title='Email' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#envelope'/>
          </svg>
        </a>

        <a href='https://github.com/isotopp' title='Follow on GitHub' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#github'/>
          </svg>
        </a>

        <a rel="me" href="https://infosec.exchange/@isotopp" title='Follow on Mastodon' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#mastodon'/>
          </svg>
        </a>

        
        <a href='http://steamcommunity.com/id/ixotopp' title='Follow on Steam' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#steam'/>
          </svg>
        </a>

        

        <a href='https://www.youtube.com/user/isotopp' title='Follow on YouTube' class=''>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#youtube'/>
          </svg>
        </a>
      </div>
    </div>
  </div>
</footer>


        </main>

	





<script src="https://blog.koehntopp.info/js/bootstrap.js"></script>




<script src="https://blog.koehntopp.info/js/lunr.js"></script>





<script src="https://blog.koehntopp.info/js/app.js"></script>


    </body>
</html>
