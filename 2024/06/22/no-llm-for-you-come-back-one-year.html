<!doctype html>
<html lang="en">
    <head>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<title>No LLM for you, come back one year | Die wunderbare Welt von Isotopp</title>
<meta name="description" content="Kris Köhntopp&#39;s blog (Fedi: @isotoppinfosec.exchange)">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="apple-touch-icon" href="icon.png">
<link rel="icon" href="favicon.ico" type="image/ico">
<link rel="shortcut icon" href="favicon.ico">
<link rel="canonical" href="https://blog.koehntopp.info/2024/06/22/no-llm-for-you-come-back-one-year.html">

<link rel="alternate" type="application/rss+xml" href='https://blog.koehntopp.info/feed.xml' title="Die wunderbare Welt von Isotopp">
<link rel="alternate" type="application/rss+xml" href='https://blog.koehntopp.info/tags/mysql/feed.xml' title="Feed: mysql Articles for Die wunderbare Welt von Isotopp">
<link rel="alternate" type="application/rss+xml" href='https://blog.koehntopp.info/tags/review/feed.xml' title="Feed: review Articles for Die wunderbare Welt von Isotopp">


<meta name="generator" content="Hugo 0.145.0">
<meta property="og:title" content='No LLM for you, come back one year | Die wunderbare Welt von Isotopp' />
<meta property="og:site_name" content='Die wunderbare Welt von Isotopp' />
<meta property="og:locale" content="de_DE" />
<meta name="description" content='Kris Köhntopp&#39;s blog (Fedi: @isotoppinfosec.exchange)' />
<meta property="og:description" content='Kris Köhntopp&#39;s blog (Fedi: @isotoppinfosec.exchange)' />
<meta property="og:url" content="https://blog.koehntopp.info/2024/06/22/no-llm-for-you-come-back-one-year.html" />
<meta property="og:type" content="article" />
<meta name="article:published_time" content='2024-06-22T05:06:07Z' />
<meta name="fediverse:creator" content="@isotopp@infosec.exchange">
<meta property="og:image" content='https://blog.koehntopp.info/assets/img/background/rijksmuseum.jpg' />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:creator" content='@isotopp@infosec.exchange' />
<meta property="twitter:title" content='No LLM for you, come back one year' />
<meta property="twitter:description" content='Kris Köhntopp&#39;s blog (Fedi: @isotoppinfosec.exchange)' />
<meta property="twitter:image" content='https://blog.koehntopp.info/assets/img/background/rijksmuseum.jpg' />


    



<link rel="stylesheet" href="https://blog.koehntopp.info/style.min.dd9d518fe6cac55191b6af874e18327dff051055754ffc7e106131baf826e6d3.css">


    </head>
    <body>
        

        <nav class="navbar navbar-expand-lg navbar-light px-lg-4 pt-lg-4">
    <div class="container-fluid">
        <a class="navbar-brand justify-content-center" href="https://blog.koehntopp.info/" rel="home" title="Die wunderbare Welt von Isotopp">
            <img alt="Kris" height='48' width='48' src='/assets/img/avatars/isotopp.jpg' class='p-0 me-3 d-block d-lg-inline'>
            <span class='h4 d-block d-lg-inline'>Die wunderbare Welt von Isotopp</span>
        </a>

        <button class="navbar-toggler ms-auto" type="button" data-bs-toggle="collapse"
                data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false"
                aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse text-center" id="navbarSupportedContent">
            <ul class="navbar-nav ms-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="/about/">
                        <span class="">About</span>
                        
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="/contribute/">
                        <span class="">Contribute</span>
                        
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="/search/">
                        <span class=""><svg class='bi' height='1.5rem' width='1.5rem' fill='currentColor'><use xlink:href='/bootstrap-icons.svg#search'/></svg></span>
                        
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="/tags/">
                        <span class=""><svg class='bi' width='1.5rem' height='1.5rem' fill='currentColor'><use xlink:href='/bootstrap-icons.svg#tags-fill'/></svg></span>
                        
                    </a>
                </li>
                
            </ul>
            
        </div>

    </div>
</nav>


        <main role="main" class="container-fluid p-0">

            









<div class="page">
	<article class="no-llm-for-you-come-back-one-year-page">
		<div class='row  justify-content-center text-center my-4 mx-0'>
			<header class="headerbanner">
				<div class='col'>
					<h1 class="title mb-lg-4 text-white">
						No LLM for you, come back one year
					</h1>
					<div class='text-uppercase text-light' style='letter-spacing: 0.1rem;'>
						<img alt='@isotopp@infosec.exchange image' src='/assets/img/avatars/isotopp.jpg' class='me-1 p-0'
						     style='width: 1.9rem; border-radius: 1rem;'>
						<a href="https://infosec.exchange/@isotopp" class='text-light text-decoration-none'>
							Kristian Köhntopp
						</a>
						
						<span class='d-none d-lg-inline'>-</span>
						<div class='d-block d-lg-inline'>June 22, 2024</div>
						
					</div>
				</div>
				
				<img alt='a featured image' src="/assets/img/background/rijksmuseum.jpg">
				
			</header>
		</div>

		<div class='row justify-content-center mx-0'>
			<div class='col-lg-4 text-center text-lg-start'>
				
				<div>
					<div class='letter-spacing-01 text-uppercase text-secondary'>
						Previous Post
					</div>
					<a class='text-decoration-none' href="https://blog.koehntopp.info/2024/06/12/mysql-add-missing-ids.html">MySQL: Add missing IDs</a>
				</div>
				
			</div>

			<div class='col-lg-4 text-lg-end mt-5 mt-lg-0 text-center'>
				
				<div>
					<div class='letter-spacing-01 text-uppercase text-secondary'>
						Next Post
					</div>
					<a class='text-decoration-none' href="https://blog.koehntopp.info/2024/07/01/practice.html">Practice</a>
				</div>
				
			</div>
		</div>

		<div class='row justify-content-center mx-0 mt-3 mb-5'>
			<div class="col-lg-8 mb-3">
				
				<header class="toc bg-light">
					<nav id="TableOfContents">
  <ul>
    <li><a href="#keine-apple-intelligence-in-der-eu">Keine Apple Intelligence in der EU</a></li>
    <li><a href="#warum-die-hardware-und-warum-so-viel-speicher">Warum die Hardware und warum so viel Speicher?</a></li>
    <li><a href="#ki-und-grafikkarten-laufen-auseinander">KI und Grafikkarten laufen auseinander</a></li>
    <li><a href="#qualität-von-kleinen-llms">Qualität von kleinen LLMs</a></li>
    <li><a href="#apple-mit-mehr-ram">Apple mit mehr RAM</a></li>
    <li><a href="#apple-mit-ohne-intelligenz">Apple mit ohne Intelligenz</a></li>
    <li><a href="#nicht-nvidia-sondern-google">Nicht Nvidia, sondern Google</a></li>
  </ul>
</nav>
				</header>
				
			</div>
			<div class='col-lg-8'>
				<p>Es war Samstag, ich las die Nachrichten und ich mußte dazu was im Fedi sagen.
Hier ist der Aufschrieb davon:</p>
<p><a href="https://www.youtube.com/watch?v=RqlQYBcsq54" target="_blank" rel="noopener">No Soup for you!</a>

 ist, was Apple der EU sagt:
<a href="https://www.heise.de/news/Apple-verweist-auf-neue-Regeln-vorerst-keine-Apple-Intelligence-fuer-EU-iPhones-9774039.html" target="_blank" rel="noopener">Apple vs. EU: Apple Intelligence soll vorerst nicht nach Deutschland kommen</a>

</p>
<blockquote>
<p>Aufgrund &ldquo;regulatorischer Unsicherheiten&rdquo; durch den Digital Markets Act (DMA) gehe das Unternehmen davon aus,
bestimmte Funktionen in diesem Jahr nicht mehr für Kunden in Deutschland
und anderen EU-Mitgliedsstaaten einführen zu können,
wie Apple am Freitagabend überraschend in einer Stellungnahme mitteilte.</p></blockquote>
<p>Dieser Seitenhieb auf den Marktregulator ist amüsant,
aber Apple könnte &ldquo;Apple Intelligence&rdquo; nicht weltweit zeitgleich einführen, selbst wenn Apple das wollte.</p>
<h1 id="keine-apple-intelligence-in-der-eu">
    <a href="#keine-apple-intelligence-in-der-eu">
	Keine Apple Intelligence in der EU
    </a>
</h1>
<p>Der wahre Grund ist: Die Features sind unfertig.
Normalerweise liefern iOS Betaversionen relativ vollständige Blicke auf das Release,
alle Features sind vorhanden und funktionieren modulo noch existierende Fehler auf eine Weise,
wie man sie im Produkt-Release später erwarten können wird.
<a href="https://www.lifewire.com/apple-betas-missing-cool-features-8662016" target="_blank" rel="noopener">In der iOS 18 Beta ist das nicht der Fall.</a>

</p>
<blockquote>
<p>But this year, there&rsquo;s less incentive to hop on early because none of the AI tools are there,
and those are surely the features people most want to try out.
The Apple Intelligence features, like better Siri, won&rsquo;t be arriving until later,
perhaps not until after the OS updates have actually launched in the fall.</p></blockquote>
<p>Das ist einerseits so, weil diese Features nicht auf eine Weise funktionieren können,
dass sie Apples Ansprüchen an Qualität genügen.
<a href="https://techinformed.com/mcdonalds-ditches-ai-order-system-after-bacon-ice-cream-mix-up/" target="_blank" rel="noopener">McDonalds hat das schon mal ausprobiert.</a>

</p>
<blockquote>
<p>In one TikTok video –
posted with the caption “Fighting with McDonald’s robot” –
a woman is seen struggling to order vanilla ice cream and a bottle of water,
and instead ends up with multiple sundaes, ketchup sachets, and two portions of butter.</p></blockquote>
<p>Nun war das keine Apple-KI, aber diese Sorte Problem ist systematisch die Sorte Fehler, die LLMs machen,
und es gibt keinen Grund, warum es bei Apple besser sein sollte.</p>
<p>Und das ist nur die oberste Lage Probleme.
Ein wichtiger Werbepunkt von Apple ist, dass die KI-Features &ldquo;on device&rdquo; laufen, statt Daten in eine Cloud zu pumpen.
Das geht auch nicht, jedenfalls nicht so, wie der Kunde es erwartet.</p>
<p>Der aktuelle KI-Hype basiert auf LLMs,
einer besonderen Art neuronaler Netze, die unter anderem Milliarden von &ldquo;Parametern&rdquo; brauchen.
Das sind Haufen von Zahlen, die als Vektoren mit tausenden Dimensionen angeordnet &ldquo;Wissen codieren&rdquo;
und in der Berechnung von Antworten auf Benutzeranfragen verwendet werden.</p>
<p>Nun, jeder dieser Vektoren braucht eine gewisse Menge Speicherplatz und wenn man &ldquo;Milliarden von Parametern&rdquo; haben will,
dann braucht man schnell auch einmal zweistellige Mengen an Gigabyte RAM zum Ausrechnen der Antwort.</p>
<p><p class="md__image">
  <img src="/uploads/2024/06/apple-01.png" alt=""  />
</p>

</p>
<p><em>Eine Auswahl von Größenangaben beliebter öffentlicher LLMs.</em></p>
<p><p class="md__image">
  <img src="/uploads/2024/06/apple-02.png" alt=""  />
</p>

</p>
<p><em>Standardversion von Metas llama3 mit 70 Milliarden Parametern und 40 GB Größe.</em></p>
<p>Ein Modell mit 8 Milliarden Parametern braucht ca. 5 GB RAM, ein Modell mit 14 Milliarden Parametern ca. 8 GB,
und ein Modell mit 70 Milliarden Parametern je nach Quantisierung zwischen 26 GB und 40 GB.</p>
<p>Dazu kommt noch einmal Speicher für den Kontext,
der bei großen Kontext-Definitionen schnell noch einmal die Größenordnung des Modells erreicht.</p>
<p>Apple selbst dokumentiert in
<a href="https://machinelearning.apple.com/research/introducing-apple-foundation-models" target="_blank" rel="noopener">Introducing Apple’s On-Device and Server Foundation Models</a>


wann sie Modelle lokal rechnen:</p>
<blockquote>
<p>In the following overview, we will detail how two of these models — a ~3 billion parameter on-device language model,
and a larger server-based language model available with Private Cloud Compute and running on Apple Silicon servers —
have been built and adapted to perform specialized tasks efficiently, accurately, and responsibly.</p></blockquote>
<p>Ein Modell mit 3 Milliarden Parametern wie <a href="https://ollama.com/library/phi3" target="_blank" rel="noopener">Phi3</a>

 belegt circa 2.5 GB Speicher und
das ist in der Tat eine Größe, die auf einem iPhone mit 6 GB RAM ausgeführt werden kann.
Größere Modelle passen schlicht nicht in den Speicher und werden daher in der Cloud berechnet.</p>
<p>Die Qualität und Komplexität der Antworten eines LLM ist aber sehr stark von der Anzahl der Parameter des Modells abhängig.
Ein 3b-Modell kann schlicht nicht mit einem 70b- oder gar noch größeren Modellen mithalten.
Das ist aber das, was Benutzer von Online-GPTs wie ChatGPT gewohnt sind.</p>
<p>Es ist also eher davon auszugehen,
dass wenig Inferenz (das Ausrechnen von Antworten mit einem austrainierten LLM) &ldquo;on device&rdquo; geht,
und viel in die Cloud ausgelagert wird.
Die Rechenzentren dazu gibt es noch nicht, und das Bauen von Rechenzentren dauert in der Regel einige Zeit.
Wir können davon ausgehen, dass die Vorlaufzeit circa 2 Jahre beträgt:
Das Rechenzentrum wird vermutlich sehr uniform bestückt sein, was die Planung vereinfacht.
Die Energiedichte wird sehr hoch sein, und die Hardware wird schwer zu beschaffen sein, was die Planung verkompliziert.</p>
<p>Die Hardware, die in dieses Rechenzentrum herein kann, wird derzeit meist von Nvidia hergestellt.
Wer Apple kennt, der weiß, wie sehr es die Firma schmerzt, einer Firma wie Nvidia Geld zu geben.
Viel lieber würde Apple den Kram auf eigenen, selbst entworfenen, auf Inferenz spezialisierten Chip laufen lassen.
Auch diese existieren nicht und es würde jedermann überraschen,
wenn es solche Chips vor 2026 gäbe (oder nach 2028 noch nicht gäbe).</p>
<p>Wir können an dieser Stelle vermutlich glaubhaft herleiten,
dass &ldquo;Apple Intelligence&rdquo; zu diesem Zeitpunkt lediglich lanciert wird, um den Kurs von AAPL zu stabilisieren.
Aber natürlich ist es billig, da noch einmal FOMO gegenüber Kunden in Europa zu pflegen und die EU zu dissen.</p>
<h1 id="warum-die-hardware-und-warum-so-viel-speicher">
    <a href="#warum-die-hardware-und-warum-so-viel-speicher">
	Warum die Hardware und warum so viel Speicher?
    </a>
</h1>
<p>Die lineare Algebra in Computerspielen und die Mathematik in LLMs sind zum Großteil Matrix-Multiplikationen.
Daher werden Grafikkarten, superschnelle Matrix-Multiplizierer, vielfach für LLMs &ldquo;mißbraucht&rdquo;.
Sie implementieren die benötigte Operation sehr flink, in Hardware und parallel ausführbar.</p>
<p><a href="https://www.youtube.com/watch?v=-P28LKWTzrI" target="_blank" rel="noopener"><p class="md__image">
  <img src="/uploads/2024/06/apple-03.jpg" alt=""  />
</p>

</a>

</p>
<p><em><a href="https://www.youtube.com/watch?v=-P28LKWTzrI" target="_blank" rel="noopener">Nvidia Werbevideo mit den Mythbusters</a>

,
das die Parallelität von Grafikkarten korrekt veranschaulicht.</em></p>
<p>Die Mythbusters haben die Wirkmächtigkeit der parallelen Ausführung für ein Nvidia Werbevideo einmal korrekt visualisiert.</p>
<p>Zur Geschichte und dem Aufbau von Grafikkarten habe ich vor sieben Jahren einen Link-Artikel geschrieben:
<a href="/2017/11/25/d-abc-at-scale.html">d = a*b+c at scale</a>

,
der die Entwicklung von Grafik-Hardware in den 20 Jahren vor 2017 beschreibt,
weil ich beruflich Anlass hatte, das herauszufinden.
Es ist ein Link-Artikel, ihr müßt die blauen Dinger auch anklicken.</p>
<p>Wikipedia erklärt
<a href="https://de.wikipedia.org/wiki/Matrizenmultiplikation#Beispiel" target="_blank" rel="noopener">Matrix-Multiplikation als superlange Summe von Produkten</a>

.</p>
<p><p class="md__image">
  <img src="/uploads/2024/06/apple-04.png" alt=""  />
</p>

</p>
<p><em>Relevanter Auszug aus der Wikipedia, der Matrix-Multiplikation erläutert.</em></p>
<p>Wir berechnen die Position Zeile 1, Spalte 2 der Ergebnis-Matrix
also durch Multiplikation des Zeilenvektors 1 der Matrix A
mit dem Spaltenvektor 2 der Matrix B,
sodass sich im Beispiel</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="go">(3, 2, 1) * (2, 1, 0) = (3*2 + 2*1 + 1*0) = 8
</span></span></span></code></pre></div><p>ergibt. In <code>d = a*b + c</code> ist die Zwischensumme <code>c</code> zunächst 0, und es wird <code>3*2</code> berechnet.
Das Ergebnis, <code>d = 6</code> wird im nächsten Schritt als <code>c</code> verwendet, sodass wir <code>d = 2*1 + 6</code> erhalten.
Im letzten Schritt wird dies wieder als <code>c</code> verwendet, sodass wir das Endergebnis <code>d = 1*0 + 8</code> bekommen.</p>
<p>Grafikkarten haben spezielle Hardware, die mit einer bestimmten Hardware-Darstellung von Zahlen solche Berechnungen
auf rechteckigen Zahlenfeldern beliebige Größe parallel durchführen können.
&ldquo;NPUs&rdquo;, Neuralprozessoren, tun genau dasselbe.</p>
<p>In dem Unterrichtsabschnitt &ldquo;Vektor- und Matrizenrechnung&rdquo; in der Schule,
oder gar in &ldquo;Lineare Algebra und Vektorräume&rdquo; an der Uni aufgepasst zu haben zahlt sich also jetzt,
30 Jahre später, groß aus.
Aber keine Angst, es hat sich nichts getan, die Mathematik ist immer noch dieselbe wie damals in der Schule,
und es ist nichts Fieseres dabei als Summen von Produkten: Plus und Malnehmen, komplizierter wird&rsquo;s nicht.</p>
<h1 id="ki-und-grafikkarten-laufen-auseinander">
    <a href="#ki-und-grafikkarten-laufen-auseinander">
	KI und Grafikkarten laufen auseinander
    </a>
</h1>
<p>Aber die Matrix-Multiplikation in einer NPU und in einer Grafikkarte laufen langsam auseinander.
Das hat aber nichts mit der Mathematik zu tun, sondern mit der Darstellung von Zahlen.</p>
<p>Wir können uns einmal Fließkommazahlen im Computer angucken:</p>
<p><a href="https://float.exposed/0x400921fb4d12d84a" target="_blank" rel="noopener">3.1415926 als Double (64 Bit)</a>

:
Stellt man eine beliebte Näherung der Zahl Pi als <code>double</code> dar, bekommt man dies:</p>
<ul>
<li>1 Bit Vorzeichen</li>
<li>10 Bit Exponenten</li>
<li>53 Bit &ldquo;Mantisse&rdquo; oder &ldquo;Significand&rdquo;</li>
</ul>
<p>und die Zahl ist nicht der gewünschte Wert, sondern der dichteste Wert, der dargestellt werden kann:
3.14159260000000006841.</p>
<p>Das ist einen Tick zu groß.
Klickt man den &ldquo;Significand&rdquo; jetzt ein Bit kleiner, bekommt man
<a href="https://float.exposed/0x400921fb4d12d849" target="_blank" rel="noopener">3.14159259999999962432</a>

.</p>
<p>Das ist das Kreuz mit <em>diskreter Mathematik</em>: Zwischen den Zahlen,
die wir darstellen können liegen unendlich viele Zahlen, die existieren,
aber nicht diskret in der gewählten Auflösung darstellbar sind.</p>
<p>In der diskreten Mathematik gibt es eine Schrittweite zwischen zwei darstellbaren Fließkommazahlen.
Diese nennt man &ldquo;eps&rdquo;.</p>
<p>&ldquo;epsilon&rdquo; ist in der Analysis (eine Disziplin der Mathematik) ein unendlich kleiner Schritt.
In der diskreten Mathematik, also der Mathematik, die sich mit Zahlen in Computern befasst,
gibt es keine unendlich kleinen Schritte.</p>
<p>&ldquo;eps&rdquo; ist der Schritt, den ein Bit macht.
Bei Fließkommazahlen ist &ldquo;eps&rdquo; variabel, denn es wird mit dem anderen Teil der Fließkommazahl, dem Exponenten skaliert.
Zahlen um 0 herum können sehr genau dargestellt werden,
aber je weiter man von 0 weg kommt, umso größer wird der Exponent und umso größer wird eps.</p>
<p>In KI-Mathematik normalisieren wir Zahlen oft auf das Intervall von 0 bis 1
oder von -1 bis 1.
Wir brauchen also nicht so große Exponenten und können mit weniger Bits auskommen.</p>
<p>Das ist wichtig, denn wenn wir von <code>double</code> auf <code>float</code> heruntergehen,
dann gehen wir von 64 Bit pro Zahl auf 32 Bit runter – doppelt so viel Modell im selben Speicher!
Ein 32-Bit <code>float</code> hat bei Werten um Zehntausend herum schon ein eps von 0.01 (bei 10k Euro ein eps von 1 cent).</p>
<p><a href="https://float.exposed/0x461c400a" target="_blank" rel="noopener">10000.009765625</a>

 als <code>float</code>.</p>
<p>Aber für Werte zwischen 0 und 1 sieht es gut aus:
<a href="https://float.exposed/0x3f000000" target="_blank" rel="noopener">0.5</a>

 und am Significand spielen.</p>
<p>Wir können auch noch weiter heruntergehen:
<a href="https://float.exposed/b0x3f00" target="_blank" rel="noopener">0.5</a>

 als <code>bfloat16</code>.
Das belegt noch einmal halb so viel Speicher, 16 Bit, verbrennt aber zu viele Bits auf den Exponenten:</p>
<ul>
<li>1 Bit Vorzeichen</li>
<li>8 Bit Exponent</li>
<li>7 Bit auf die &ldquo;Mantisse&rdquo;, den &ldquo;Significand&rdquo;</li>
</ul>
<p>Wir können auch das 16-Bit <code>half</code> nehmen:
<a href="https://float.exposed/0x3800" target="_blank" rel="noopener">0.5</a>

.</p>
<p>Hier werden</p>
<ul>
<li>1 Bit Vorzeichen</li>
<li>5 Bit Exponent</li>
<li>10 Bit Significand</li>
</ul>
<p>verwendet. Je nach Anwendungsfall kann das vorteilhaft sein.</p>
<p>Ist es schlimm, ein LLM mit weniger Auflösung zu trainieren oder zu benutzen?
Artikel über &ldquo;Quantisierung&rdquo; von LLMs diskutieren das, und auch <em>wann</em> im Herstellungsprozeß das Modell eingekürzt wird.</p>
<ul>
<li><a href="https://medium.com/@techresearchspace/what-is-quantization-in-llm-01ba61968a51" target="_blank" rel="noopener">What is Quantization in LLM</a>


diskutiert ganz allgemein was Quantisierung bei LLMs ist – also mit welchen Zahlen beim Training gerechnet wird
und wann auf Auslieferungs-Zahlenformat umgerechnet wird.</li>
<li><a href="https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/" target="_blank" rel="noopener">A Guide to Quanitization in LLMs</a>


erklärt dasselbe, aber mit mehr Details betreffend die Integer-Formate, also <code>Qx_y</code>, die man oft sieht.</li>
</ul>
<p>Quantisierung hat mitunter Auswirkungen auf die Qualität der Antworten, die das LLM generiert,
aber sie spart Speicher und ermöglicht oft erst die Ausführung von LLMs auf Rechnern,
die man im Haus oder gar in der Hand hat.</p>
<p>Wenn man zig Milliarden Parameter in den verfügbaren Speicher einer Nvidia 4070 Ti quetschen will,
dann braucht man nun einmal kürzere Zahlendarstellungen.
Und wenn es im iPhone passieren soll, dann noch kürzer.</p>
<p>Damit man von der Beschleunigung einer Nvidia profitiert kann man aber nur Darstellungen nehmen,
die die Nvidia kennt, also in Hardware beschleunigt parallel rechnen kann.
Sonst kann man auch &ldquo;zu Fuß&rdquo;, also mit der CPU, rechnen.</p>
<p>Das ist das Problem mit dem M1 Apple Silicon, das <code>bfloat16</code> nicht in Hardware kann, und das ab M2 korrigiert ist.
Ich würde erwarten, dass neuere Apple CPUs auch andere, noch härtere Quantisierungen parallel in Hardware rechnen können,
aber ich habe nicht nachgesehen, was ab wann das wie realisiert wird.</p>
<h1 id="qualität-von-kleinen-llms">
    <a href="#qualit%c3%a4t-von-kleinen-llms">
	Qualität von kleinen LLMs
    </a>
</h1>
<p>Apple behauptet, dass ihre 3B Modelle in Benchmarks besser sind als die Konkurrenz:
<a href="https://machinelearning.apple.com/research/introducing-apple-foundation-models" target="_blank" rel="noopener">Introducing Apple’s On-Device and Server Foundation Models</a>

,
die Benchmarks am Ende.</p>
<p>Aber 3 Milliarden Parameter sind sehr klein.
Das ist jedoch das, was Apple lokal rechnen kann.
Größere Modelle werden &ldquo;in der Cloud&rdquo; ausgeführt, wo mehr Speicher verfügbar ist.</p>
<p>Benchmarks bei KI-Anwendungen sind jedoch wenig aussagekräftig, damit ein Endkunde Qualität beurteilen kann.
Sie vergleichen Antworten verschiedener Modelle miteinander, aber die Ergebnisse sind nicht so,
wie ein Mensch Qualität erfahren würde.
Das liegt daran, dass Menschen Antworten von Computern nehmen, um ihre eigenen Entscheidungen zu treffen,
oder um sie mit ihren eigenen Entscheidungen zu vergleichen.</p>
<p>Menschen haben aber ein Weltmodell.</p>
<p>Das heißt im Wesentlichen, dass an jeder Entscheidung eine Gewichtung für Fehlschlag und Erfolg hängt,
eine Risikobewertung.
Eine falsche Antwort und eine andere, ebenfalls falsche Antwort sind für einen Menschen also nicht gleich:
Bei einer würde er eine kleine Summe Geld verlieren, bei der anderen sterben.
Im Benchmark aber sind beide gleich &ldquo;falsch&rdquo;.</p>
<p>Der Effekt ist, dass ein schlechtes Modell im Benchmark in der Realität eventuell als besser empfunden werden kann,
weil es nie mörderische Antworten liefert.</p>
<h1 id="apple-mit-mehr-ram">
    <a href="#apple-mit-mehr-ram">
	Apple mit mehr RAM
    </a>
</h1>
<p>Wenig überraschend beginnt Apple nun Rechner mit mehr RAM zu bauen,
oder für LLM-Features Geräte mit mehr RAM zu verlangen:</p>
<p><a href="https://www.golem.de/news/apple-8-gbyte-ram-im-macbook-reichen-nicht-mehr-2406-186433.html" target="_blank" rel="noopener">Golem: 8 GByte RAM im Macbook reichen nicht mehr</a>

</p>
<blockquote>
<p>Das Unternehmen stellte die Beta der Entwicklungsumgebung XCode 16 vor.
In ihr enthalten ist ein neues Feature: Predictive Code Completion:
Diese nutzt KI, um Codeschnipsel zu analysieren und passende Vorschläge in Echtzeit auszugeben.
Allerdings wird dafür laut Apple ein Mac mit mindestens 16 GByte Arbeitsspeicher benötigt.</p></blockquote>
<h1 id="apple-mit-ohne-intelligenz">
    <a href="#apple-mit-ohne-intelligenz">
	Apple mit ohne Intelligenz
    </a>
</h1>
<p>Stellt sich raus, auch der Rest der Welt bekommt zunächst einmal keine Apple Intelligence:</p>
<p><a href="https://www.heise.de/news/Apple-Intelligence-Kommt-die-KI-Siri-erst-mit-iOS-18-4-9793852.html" target="_blank" rel="noopener">Apple Intelligence: Kommt die KI-Siri erst mit iOS 18.4?</a>

</p>
<blockquote>
<p>Neuen Informationen zufolge soll etwa der mit Künstlicher Intelligenz runderneuerte Sprachassistent Siri
deshalb erst im nächsten Frühjahr in den USA sein Debüt feiern.</p></blockquote>
<h1 id="nicht-nvidia-sondern-google">
    <a href="#nicht-nvidia-sondern-google">
	Nicht Nvidia, sondern Google
    </a>
</h1>
<p>Sierk Bornemann weist mich darauf hin, daß Apple in der Tat nicht sehr an Nvidia interessiert ist,
aber augenscheinlich sehr eng mit Google verbandelt ist:</p>
<ul>
<li>Business Insider: <a href="https://www.businessinsider.com/apple-ai-google-data-centers-2024-6?international=true&amp;r=US&amp;IR=T" target="_blank" rel="noopener">Apple&rsquo;s new AI is made in Google data centers</a>

.
Der Artikel behauptet, daß Apple Google&rsquo;s Rechenzentrum-Hardware und TPUs nutzt, um Modelle zu trainieren.
Dabei scheint der Konsum von TPU-Kapazität Google vor einige technische Probleme gestellt zu haben.</li>
<li>Ein <a href="https://x.com/WholeMarsBlog/status/1800317499222921235" target="_blank" rel="noopener">Tweet</a>

 benennt dabei das &ldquo;axlearn&rdquo; Framework,
und zitiert dabei <a href="https://machinelearning.apple.com/research/introducing-apple-foundation-models" target="_blank" rel="noopener">Apple&rsquo;s eigene Dokumentation</a>

.</li>
<li>Und <a href="https://github.com/apple/axlearn/blob/main/docs/04-infrastructure.md" target="_blank" rel="noopener">axlearn</a>

 ist Open Source
und scheint sehr auf Google TPUs hin optimiert zu sein.</li>
<li>Auch <a href="https://blog.trailofbits.com/2024/06/14/understanding-apples-on-device-and-server-foundations-model-release/" target="_blank" rel="noopener">Trail of Bits</a>


stellt heraus, daß Apple ohne &ldquo;Nvidia/CUDA Tax&rdquo; käme, weil das Training mit axlearn auf Google TPUs liefe.</li>
</ul>
<p>Inferenz ist einfacher und verwendet Apple Silicon (also M1-M4).</p>

			</div>
		</div>

		
		
		<div class='row justify-content-center py-3 mb-3 mx-0'>
			<div class='col-lg-8 text-center'>
				<span class='letter-spacing-01 text-uppercase text-secondary me-2'>Tags</span>
				
				<a href="/tags/#lang_de" class='btn btn-sm btn-outline-primary mb-1'>
				<svg class="bi" width="1rem" height="1rem" fill="currentColor">
					<use xlink:href="/bootstrap-icons.svg#tag-fill"></use>
				</svg>
				lang_de
				</a>
				
				<a href="/tags/#erklaerbaer" class='btn btn-sm btn-outline-primary mb-1'>
				<svg class="bi" width="1rem" height="1rem" fill="currentColor">
					<use xlink:href="/bootstrap-icons.svg#tag-fill"></use>
				</svg>
				erklaerbaer
				</a>
				
				<a href="/tags/#apple" class='btn btn-sm btn-outline-primary mb-1'>
				<svg class="bi" width="1rem" height="1rem" fill="currentColor">
					<use xlink:href="/bootstrap-icons.svg#tag-fill"></use>
				</svg>
				apple
				</a>
				
				<a href="/tags/#computer" class='btn btn-sm btn-outline-primary mb-1'>
				<svg class="bi" width="1rem" height="1rem" fill="currentColor">
					<use xlink:href="/bootstrap-icons.svg#tag-fill"></use>
				</svg>
				computer
				</a>
				
			</div>
		</div>

		
		
		
		
		<div class='row justify-content-center py-3 mb-3 mx-0'>
			<div class='col-lg-8 text-center'>
				<a href="https://github.com/isotopp/isotopp.github.io/edit/main/content/posts/2024-06-22-no-llm-for-you-come-back-one-year.md"
				   rel="noopener noreferrer" target="_blank">
					<span class='letter-spacing-01 text-uppercase text-secondary me-2'>Suggest Changes</span>
				</a>
			</div>
		</div>
		

		<div class='row justify-content-center mx-0'>
			<div class='col-lg-4 text-center text-lg-start'>
				
				<div>
					<div class='letter-spacing-01 text-uppercase text-secondary'>
						Previous Post
					</div>
					<a class='text-decoration-none' href="https://blog.koehntopp.info/2024/06/12/mysql-add-missing-ids.html">MySQL: Add missing IDs</a>
				</div>
				
			</div>

			<div class='col-lg-4 text-lg-end mt-5 mt-lg-0 text-center'>
				
				<div>
					<div class='letter-spacing-01 text-uppercase text-secondary'>
						Next Post
					</div>
					<a class='text-decoration-none' href="https://blog.koehntopp.info/2024/07/01/practice.html">Practice</a>
				</div>
				
			</div>
		</div>


		
	</article>
</div>



            <footer class='row justify-content-center mb-4 pb-4 mx-0'>
  <div class='col-8 text-center'>
    <div>
      A collection of old stuff, new stuff and random stuff.
    </div>
    <div class='row text-primary mt-4'>
      <div class='col'>

        <a href='/feed.xml' title='Follow RSS feed' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#rss-fill'/>
          </svg>
        </a>

        <a href='mailto:kristian.koehntopp@gmail.com' title='Email' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#envelope'/>
          </svg>
        </a>

        <a href='https://github.com/isotopp' title='Follow on GitHub' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#github'/>
          </svg>
        </a>

        <a rel="me" href="https://infosec.exchange/@isotopp" title='Follow on Mastodon' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#mastodon'/>
          </svg>
        </a>

        
        <a href='http://steamcommunity.com/id/ixotopp' title='Follow on Steam' class='me-3 text-decoration-none'>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#steam'/>
          </svg>
        </a>

        

        <a href='https://www.youtube.com/user/isotopp' title='Follow on YouTube' class=''>
          <svg class='bi' height='2.5rem' width='2.5rem' fill='currentColor'>
            <use xlink:href='/bootstrap-icons.svg#youtube'/>
          </svg>
        </a>
      </div>
    </div>
  </div>
</footer>


        </main>

	





<script src="https://blog.koehntopp.info/js/bootstrap.js"></script>




<script src="https://blog.koehntopp.info/js/lunr.js"></script>





<script src="https://blog.koehntopp.info/js/app.js"></script>


    </body>
</html>
